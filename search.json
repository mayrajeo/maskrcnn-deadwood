[
  {
    "objectID": "0_index.html",
    "href": "0_index.html",
    "title": "Deadwood detection from RGB UAV imagery using Mask R-CNN",
    "section": "",
    "text": "Abstract\nDeadwood and decaying wood are the most important components for the biodiversity of boreal forests, and it has been estimated that around a quarter of all flora and fauna in Finnish forests depend on that. However, there is a severe lack of stand-level deadwood data in Finland, as the operational inventories either focus on the large-scale estimates or omit deadwood altogether. Unmanned Aerial Vehicles (UAVs) are the only method for remotely mapping small objects, such as fallen deadwood, as even the most spatially accurate commercial satellites provide 30cm ground sampling distance, compared to less than 5 cm that is easily achievable with UAVs.\nIn this work, we utilized Mask R-CNN to detect individual standing and fallen deadwood instances from RGB UAV imagery. We manually annotated over 14 000 deadwood instances from two separate study sites to use as the training and validation data, and also compared these data to field-measured deadwood data. Our models achieved test set Average Precision (AP) of 0.284 for the same geographical area the models were trained on, and AP of 0.220 for geographically distinct area used only for testing.\nIn addition to instance-level deadwood maps, we also estimated stand-level characteristics for the numbers of deadwood. In addition, we estimated the approximate total volume of fallen deadwood based on the annotated polygons. These stand-level features clearly show the borders of the conserved forest, and the volume estimations are able to distinguish between naturally formed deadwood hotspot and areas with logging remnants. The proposed method enables deadwood mapping for larger areas, complementing the traditional field work.\n\n\nGetting started\nMuch of the work relies heavily on https://github.com/jaeeolma/drone_detector, and instructions for its installation work here also.\n\n\nData used\nExamples are using UAV RGB Orthomosaics from either Hiidenportti, Kuhmo, Eastern-Finland or Sudenpesänkangas, Evo, Southern-Finland. Hiidenportti dataset has a spatial resolution of around 4cm, and Sudenpesänkangas dataset has a spatial resolution of 4.85cm. Hiidenportti data contains 9 different UAV mosaics, and Sudenpesänkangas data is one single orthomosaic. From these data, we constructed rectangular virtual plots (hereafter referred as scenes) to use as a training and validation data for the models. From Hiidenportti, we constructed 33 scenes of varying sizes in such way that all 9m circular field plots present in the area were covered, and each field plot center had at least 45 meter distance to the edge of the scene. For Sudenpesänkangas, due to the area and orthomosaic being larger, we extracted 100x100m plots in such way that each scene contains only one circular field plot. In total, Hiidenportti data contained 33 scenes that cover 71 field plots, and Sudenpesänkangas data contaied 71 scenes.\nDeadwood data that was used for training the models was manually annotated using QGIS software. We annotated all visible fallen deadwood trunks and standing deadwood canopies present in the scenes, and saved the results as geojson files. These data were then tiled and converted to COCO-format using functions from drone_detector. Sudenpesäkangas dataset consists of 5334 annotated deadwood instances, and Hiidenportti contains 8479 annotations.\n\n\nAuthors\n\nJanne Mäyrä, Finnish Environment Intitute SYKE\nTopi Tanhuanpää, University of Eastern Finland\nAnton Kuzmin, University of Eastern Finland\nEinari Heinaro, University of Helsinki\nTimo Kumpula, University of Eastern Finland\nPetteri Vihervaara, University of Eastern Finland"
  },
  {
    "objectID": "1_dataset_description_and_generation.html",
    "href": "1_dataset_description_and_generation.html",
    "title": "Dataset description and generation",
    "section": "",
    "text": "Code\nfrom pathlib import Path\nimport os, sys\nimport geopandas as gpd\nimport pandas as pd\nimport math\nimport numpy as np\nfrom shapely.geometry import Point\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\n\nfrom drone_detector.utils import *\nfrom drone_detector.processing.tiling import *\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nsys.path.append('..')\nfrom src.tree_functions import *"
  },
  {
    "objectID": "1_dataset_description_and_generation.html#study-areas",
    "href": "1_dataset_description_and_generation.html#study-areas",
    "title": "Dataset description and generation",
    "section": "1.1 Study areas",
    "text": "1.1 Study areas\nThe first study area is located in the eastern side of Hiidenportti National Park, Sotkamo, Eastern-Finland, and the field data contains areas both inside and outside the conserved areas (Hiidenportti National Park in the west and Teerisuo - Lososuo area in the east). The most common trees in Hiidenportti area are Scots pine and Norway spruce, with different deciduous trees (e.g. Silver birch, Downy birch and European aspen). The UAV data from this study area consist of nine partially overlapping RGB orthoimages which cover approximately 10 km² area, with spatial resolution ranging between 3.9cm and 4.4cm. These scenes cover both conserved and managed forests, as well as some logging openings. The UAV data were collected on 16. and 17.7.2019 using DJI Phantom 4 RTK UAV equipped with 20 megapixel CMOS sensor.\nOur other study area is located in Sudenpesänkangas, Evo, Southern-Finland. The data was acquired on 11.7.2018, using eBee Plus RTK fixed-wing platform, equipped with a 20 megapixel global shutter S.O.D.A camera with a field of view of 64\\(^{\\circ}\\). The UAV data from Evo consist of a single RGB orthomosaic which has a spatial resolution of 4.9cm. The area covers both managed and conserved areas, and the canopy is mostly dominated by Scots pine and Norway spruce, with a mixture of the two birch species. Other species (e.g. aspen and larch) are rather scarce in dominant canopy layer. Data from Evo was used for testing the methods in different geographical location\nIn addition to UAV data, we also have high-resolution LiDAR data from both of our study areas. The point density for Hiidenportti LiDAR data was approximately 15 points/m\\(^2\\), and 11 points/m\\(^2\\) for Evo LiDAR data. These LiDAR data from Hiidenportti were the same as used in Heinaro et al (2021), while the LiDAR data from Evo were used in both Viinikka et al (2020 and Mäyrä et al (2021). We used these data to create canopy height models (CHMs) with 0.5m spatial resoution for both study areas, which were then used to compute the canopy densities for the field plots."
  },
  {
    "objectID": "1_dataset_description_and_generation.html#virtual-deadwood-data",
    "href": "1_dataset_description_and_generation.html#virtual-deadwood-data",
    "title": "Dataset description and generation",
    "section": "1.2 Virtual deadwood data",
    "text": "1.2 Virtual deadwood data\nHaving accurate and good quality ground reference data is not only important for training the models, but also for evaluating the obtained results. Even though we had extensive field data from both of our study areas, using only these data would not provide enough training data for our methods. For both of our study areas, we constructed rectangular virtual plots (scenes) in such way that all usable field plots were covered. Because the sizes of the UAV mosaics were different between the study areas, these plots were constructed slightly differently. For Hiidenportti dataset, we constructed 33 rectangular scenes in such way that all usable field plots were covered and each plot had at least 45 meter distance to the border of the scene. As the total area of Sudenpesänkangas UAV-mosaic is larger than Hiidenportti, we constructed the scenes for Evo area as 100x100 meter non-overlapping squares in such way that each virtual plot contained only one circular field plot.\nWe manually annotated all visible deadwood from the scenes to use our reference data. Both deadwood types were annotated as a separate classes. The standing deadwood instances were annotated so that the full canopy were inside the polygon, but for fallen deadwood we annotated the trunks and left the branches and twigs unannotated. We annotated even the shortest deadwood instances visible, as it was possible that in reality they belonged of a larger trunk, but were partially obscured by canopy.\n\n1.2.1 Hiidenportti dataset\nOut of the 178 circular field plots with 9m radius present in the Hiidenportti area, our UAV data covered 75 in such way that the image quality was sufficient for our study. Out of these 75, we omitted four plots as they had been cut down between the time of field work and UAV flights, bringing the total number of field plots to 71. 23 of these field plots contained accurately measured and located deadwood data, collected by Heinaro et al (2021). In Evo area, the UAV data covered 79 circular field plots with 9 meter radius, from which 71 had suitable image quality for our use cases.\n\n\nCode\nfull_data = gpd.read_file('../../data/raw/hiidenportti/virtual_plots/all_deadwood_hiidenportti.geojson')\nconservation_areas = gpd.read_file('../data/common/LsAlueValtio.shp')\ncons_hp = conservation_areas[conservation_areas.geometry.intersects(box(*full_data.total_bounds))]\n\nfull_data['conservation'] = full_data.progress_apply(lambda row: 1 if row.geometry.intersects(cons_hp.geometry.unary_union)\n                                                                 else 0, axis=1)\nfull_data.to_file('../../data/raw/hiidenportti/virtual_plots/all_deadwood_hiidenportti.geojson')\n\n\n\n\n\nIn total, the final deadwood data contained 7396 annotated fallen deadwood and 1083 standing deadwood instances. The total numbers of deadwood instances present in a single virtual plot varied between 47 and 1051, the number of fallen deadwood between 31 and 990, and for the number of standing deadwood between 1 and 159. Relatively, standing deadwood was much more common in conserved areas than in managed forests, as 59% (639 instances) of standing deadwood were located in conservation areas, compared to 30.6% (2260 instances) of fallen deadwood.\n\n\nCode\npd.crosstab(full_data.conservation, full_data.layer, margins=True)\n\n\n\n\n\n\n  \n    \n      layer\n      groundwood\n      uprightwood\n      All\n    \n    \n      conservation\n      \n      \n      \n    \n  \n  \n    \n      0\n      5909\n      683\n      6592\n    \n    \n      1\n      1487\n      400\n      1887\n    \n    \n      All\n      7396\n      1083\n      8479\n    \n  \n\n\n\n\nIn addition to the total number of deadwood instances, these data tells us also the total area covered in m² as well as pixels for each deadwood type, both for the polygons and their bounding boxes. Based on the annotated polygons, we estimated the trunk length of the fallen deadwood and the maximum canopy diameter of the standing deadwood to be the longer side of the minimum rotated rectangle for the corresponding polygon. Because the spatial resolution of our data is around 4cm, we did not estimate fallen deadwood diameter or volume, as even an error of one pixel effects these calculations too much.\nOverall, the total area covered by fallen deadwood was 3459.81m² and 3565.37m² by standing deadwood. The mean area covered by a single fallen deadwood polygon was around 0.47m² and 3.29m² for standing deadwood.\n\n\nCode\nfull_data['area'] = full_data.geometry.area\nfull_data['pixel_area'] = full_data.area / 0.04**2\nfull_data_dis = full_data.dissolve(by='layer')\nfull_data_dis.area\n\n\nlayer\ngroundwood     3459.805448\nuprightwood    3565.369704\ndtype: float64\n\n\n\n\nCode\npd.pivot_table(full_data, index=['layer'], values='area', aggfunc=['min', 'max', 'mean', 'median'], margins=True)\n\n\n\n\n\n\n  \n    \n      \n      min\n      max\n      mean\n      median\n    \n    \n      \n      area\n      area\n      area\n      area\n    \n    \n      layer\n      \n      \n      \n      \n    \n  \n  \n    \n      groundwood\n      0.024711\n      5.789380\n      0.469012\n      0.348866\n    \n    \n      uprightwood\n      0.103820\n      24.334151\n      3.292461\n      2.416391\n    \n    \n      All\n      0.024711\n      24.334151\n      0.829644\n      0.395097\n    \n  \n\n\n\n\n\n\n<seaborn.axisgrid.FacetGrid>\n\n\n\n\n\nBased on the annotated polygons, we also estimated the trunk length of the fallen deadwood and the maximum canopy diameter of the standing deadwood to be the longest side of the minimum rotated rectangle for the corresponding polygon. For diameter and volume estimation for the fallen deadwood, we constructed three lines that are perpendicular with the minimum rotated rectangle of the polygon that intersect in the polygon at the 10%, 50% and 90% of the length. This process is visualized below. Based on these lines, we used the mean length of these intersecting lines to estimate the polygon diameter. For the volume estimation, we assumed that the fallen deadwood polygons are constructed from two truncated right circular cones and used the following equations to estimate the volume:\n\\[\n    V_{1} = \\frac{\\pi h(r_{10}^2 + r_{10}r_{50} + r_{50}^2)}{3} \\\\\n    V_{2} = \\frac{\\pi h(r_{90}^2 + r_{90}r_{50} + r_{50}^2)}{3} \\\\\n    V = V_1 + V_2\n\\]\nwhere r\\(_{x}\\) are the radii at the length percentile of x, and h is half the estimated polygon length.\n\n\n\n\n\nThe average fallen deadwood trunk length was around 2.48 meters, with the shortest annotated tree being 0.21m and longest 15.38m. On average, both deadwood types were larger in the conservation areas compared to the managed forests. It is worth mentioning that 1655 of the annotated fallen deadwood were shorter than 1.3m (the minimum length for the tree to be measured in the Finnish National Forest Inventory), but due to the possibility of them being partially obscured by the canopy, they were included in the deadwood data.\n\n\nCode\nfull_data['tree_length'] = full_data.apply(lambda row: get_len(row.geometry), axis=1)\nlen(full_data[(full_data.layer == 'groundwood') & (full_data.tree_length < 1.3)])\n\n\n1655\n\n\nStatistics for estimated tree length and canopy diameter.\n\n\nCode\npd.pivot_table(full_data, index=['layer'], \n               values=['tree_length'], aggfunc=['min', 'max', 'mean', 'median', 'count'], margins=True)\n\n\n\n\n\n\n  \n    \n      \n      min\n      max\n      mean\n      median\n      count\n    \n    \n      \n      tree_length\n      tree_length\n      tree_length\n      tree_length\n      tree_length\n    \n    \n      layer\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      groundwood\n      0.206041\n      15.383351\n      2.404708\n      2.002028\n      7396\n    \n    \n      uprightwood\n      0.385542\n      8.313442\n      2.323786\n      2.154042\n      1083\n    \n    \n      All\n      0.206041\n      15.383351\n      2.394372\n      2.024371\n      8479\n    \n  \n\n\n\n\n\n\nCode\npd.pivot_table(full_data, index=['conservation', 'layer'], \n               values=['tree_length'], aggfunc=['min', 'max', 'mean', 'median', 'count'], margins=True)\n\n\n\n\n\n\n  \n    \n      \n      \n      min\n      max\n      mean\n      median\n      count\n    \n    \n      \n      \n      tree_length\n      tree_length\n      tree_length\n      tree_length\n      tree_length\n    \n    \n      conservation\n      layer\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      groundwood\n      0.206041\n      15.383351\n      2.284923\n      1.915742\n      5909\n    \n    \n      uprightwood\n      0.385542\n      6.977358\n      2.248640\n      2.075352\n      683\n    \n    \n      1\n      groundwood\n      0.346741\n      12.954119\n      2.880703\n      2.388131\n      1487\n    \n    \n      uprightwood\n      0.633733\n      8.313442\n      2.452099\n      2.294426\n      400\n    \n    \n      All\n      \n      0.206041\n      15.383351\n      2.394372\n      2.024371\n      8479\n    \n  \n\n\n\n\n\n\n\n\n\nStatistics for estimated diameter for groundwood.\n\n\nCode\nfull_data['diam'] = full_data.geometry.apply(lambda row: np.mean(get_three_point_diams(row))) * 1000\npd.pivot_table(full_data[full_data.layer=='groundwood'], index=['conservation'], \n               values=['diam'], aggfunc=['min', 'max', 'mean', 'median', 'count'], margins=True)\n\n\n\n\n\n\n  \n    \n      \n      min\n      max\n      mean\n      median\n      count\n    \n    \n      \n      diam\n      diam\n      diam\n      diam\n      diam\n    \n    \n      conservation\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      55.336382\n      626.065954\n      180.397765\n      172.583156\n      5909\n    \n    \n      1\n      68.693180\n      677.660650\n      208.541880\n      190.436852\n      1487\n    \n    \n      All\n      55.336382\n      677.660650\n      186.056270\n      175.094096\n      7396\n    \n  \n\n\n\n\n\n\n\n\n\nLook at the largest polygons.\n\n\nCode\n# echo: false\n\nfig, axs = plt.subplots(1,2, figsize=(10,4))\nfor ix, ax in enumerate(axs.flatten()):\n    full_data[(full_data.layer == 'groundwood')&(full_data.diam>650)].iloc[ix:ix+1].exterior.plot(ax=ax, color=\"#e41a1c\")\n    temp = full_data[(full_data.layer == 'groundwood')&(full_data.diam>650)].iloc[ix:ix+1].copy()\n    geom = temp.iloc[0].geometry\n    temp['geometry'] = temp.apply(lambda row: row.geometry.minimum_rotated_rectangle, axis=1)\n    temp.exterior.plot(ax=ax, color='#377eb8', linestyle='--')\n    temp['geometry'] = temp.apply(lambda row: LineString([(row.geometry.exterior.xy[0][0], row.geometry.exterior.xy[1][0]),\n                                                         (row.geometry.exterior.xy[0][1], row.geometry.exterior.xy[1][1])]),\n                                  axis=1)\n    temp.plot(ax=ax, color='#377eb8')\n    temp2 = full_data[(full_data.layer == 'groundwood')&(full_data.diam>650)].iloc[ix:ix+1].copy()\n    lines = get_three_point_diam_lines(temp2.iloc[0].geometry)\n    for l in lines:\n        temp2['geometry'] = l\n        temp2.plot(ax=ax, edgecolor='#4daf4a', linestyle='--')\n    for l in lines:\n        temp2['geometry'] = l.intersection(geom)\n        temp2.plot(ax=ax, edgecolor='#984ea3')\nplt.show()\n\n\n\n\n\nEstimations for volume of fallen deadwood.\n\n\nCode\nfull_data['v'] = full_data.geometry.apply(cut_cone_volume)\n\npd.pivot_table(full_data[full_data.layer=='groundwood'], index=['conservation'], \n               values=['v'], aggfunc=['min', 'max', 'mean', 'median', 'count', 'sum'], margins=True)\n\n\n\n\n\n\n  \n    \n      \n      min\n      max\n      mean\n      median\n      count\n      sum\n    \n    \n      \n      v\n      v\n      v\n      v\n      v\n      v\n    \n    \n      conservation\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      0.001237\n      2.399251\n      0.072439\n      0.046403\n      5909\n      428.041718\n    \n    \n      1\n      0.002850\n      2.130253\n      0.128083\n      0.070668\n      1487\n      190.459568\n    \n    \n      All\n      0.001237\n      2.399251\n      0.083626\n      0.050058\n      7396\n      618.501287\n    \n  \n\n\n\n\n\n\n\n\n\nFinally, get estimations of total volume of groundwood m³/ha.\n\n\nCode\nfield_data_path = Path('../data/hiidenportti')\nvirtual_plot_grid = gpd.read_file(field_data_path/'envelopes_with_trees.geojson')\ntot_vplot_area = virtual_plot_grid.area.sum()\ncons_areas = gpd.clip(cons_hp, virtual_plot_grid)\ntot_cons_area = cons_areas.area.sum()\ntot_man_area = tot_vplot_area - tot_cons_area\ntot_man_ha = tot_man_area / 10000\ntot_cons_ha = tot_cons_area / 10000\n\nest_v_man = full_data[(full_data.layer=='groundwood')&(full_data.conservation==0)].v.sum()/tot_man_ha\nest_v_cons = full_data[(full_data.layer=='groundwood')&(full_data.conservation==1)].v.sum()/tot_cons_ha\nest_v_tot = full_data[(full_data.layer=='groundwood')].v.sum()/(tot_vplot_area/10000)\n\nprint(f'Estimated groundwood volume in managed forests: {est_v_man:.2f} ha/m³')\nprint(f'Estimated groundwood volume in conserved forests: {est_v_cons:.2f} ha/m³')\nprint(f'Estimated groundwood volume in both types: {est_v_tot:.2f} ha/m³')\n\n\nEstimated groundwood volume in managed forests: 10.76 ha/m³\nEstimated groundwood volume in conserved forests: 11.59 ha/m³\nEstimated groundwood volume in both types: 11.00 ha/m³\n\n\n\n\n1.2.2 Evo dataset\nWe constructed the scenes from Evo area as 100x100 meter non-overlapping squares in such way that each virtual plot contained only one circular field plot. This dataset contained 3915 fallen deadwood and 1419 standing deadwood instances, and the total number of deadwood instances in a single field plot varied between 3 and 570, the number of fallen deadwood between 1 and 570, and the number of standing deadwood between 1 and 162.\n\n\nCode\nevo_data = gpd.read_file('../../data/raw/sudenpesankangas/virtual_plots/sudenpesankangas_deadwood.geojson')\ncons_evo = conservation_areas[conservation_areas.geometry.intersects(box(*evo_data.total_bounds))]\nevo_data['conservation'] = evo_data.progress_apply(lambda row: 1 if row.geometry.intersects(cons_evo.geometry.unary_union)\n                                                               else 0, axis=1)\nevo_data.to_file('../../data/raw/sudenpesankangas/virtual_plots/sudenpesankangas_deadwood.geojson')\n\n\n\n\n\n\n\nCode\npd.crosstab(evo_data.conservation, evo_data.label, margins=True)\n\n\n\n\n\n\n  \n    \n      label\n      groundwood\n      uprightwood\n      All\n    \n    \n      conservation\n      \n      \n      \n    \n  \n  \n    \n      0\n      2369\n      386\n      2755\n    \n    \n      1\n      1546\n      1033\n      2579\n    \n    \n      All\n      3915\n      1419\n      5334\n    \n  \n\n\n\n\nThe total area covered by fallen deadwood was 3117.1m² and the total canopy area for standing deadwood was 7196.4m². On average, a single deadwood instance covered more area compared to Hiidenportti dataset, as the average area for fallen deadwood in Evo was 0.81m² and 5.08m² for standing deadwood.\n\n\nCode\nevo_data['area'] = evo_data.geometry.area\nevo_data['pixel_area'] = evo_data.area / 0.0485**2\nevo_data_dis = evo_data.dissolve(by='label')\nevo_data_dis.area\n\n\nlabel\ngroundwood     3117.087445\nuprightwood    7196.352919\ndtype: float64\n\n\n\n\nCode\npd.pivot_table(evo_data, index=['conservation', 'label'], values='area', \n               aggfunc=['min', 'max', 'mean', 'median'], margins=True)\n\n\n\n\n\n\n  \n    \n      \n      \n      min\n      max\n      mean\n      median\n    \n    \n      \n      \n      area\n      area\n      area\n      area\n    \n    \n      conservation\n      label\n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      groundwood\n      0.064145\n      8.456464\n      0.646843\n      0.489316\n    \n    \n      uprightwood\n      0.364184\n      41.339628\n      3.142462\n      2.017817\n    \n    \n      1\n      groundwood\n      0.092866\n      8.173691\n      1.049027\n      0.735591\n    \n    \n      uprightwood\n      0.319265\n      35.944833\n      5.811285\n      4.102591\n    \n    \n      All\n      \n      0.064145\n      41.339628\n      1.944172\n      0.758211\n    \n  \n\n\n\n\n\n\n<seaborn.axisgrid.FacetGrid>\n\n\n\n\n\nSimilar observations were found from the average tree length and canopy diameter, as the average trunk length of the fallen trees was around 1 meter more in Evo, and canopy diameter of standing trees was around 60cm larger. However, this is likely due to the vast majority of the annotated trees in Evo dataset being in conserved areas.\n\n\nCode\nevo_data['tree_length'] = evo_data.apply(lambda row: get_len(row.geometry), axis=1)\npd.pivot_table(evo_data, index=['label'], \n               values=['tree_length'], aggfunc=['min', 'max', 'mean', 'median', 'count'], margins=True)\n\n\n\n\n\n\n  \n    \n      \n      min\n      max\n      mean\n      median\n      count\n    \n    \n      \n      tree_length\n      tree_length\n      tree_length\n      tree_length\n      tree_length\n    \n    \n      label\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      groundwood\n      0.575351\n      24.479043\n      3.429642\n      2.790052\n      3915\n    \n    \n      uprightwood\n      0.674329\n      8.319080\n      2.958530\n      2.745163\n      1419\n    \n    \n      All\n      0.575351\n      24.479043\n      3.304312\n      2.779696\n      5334\n    \n  \n\n\n\n\n\n\nCode\npd.pivot_table(evo_data, index=['conservation','label'], \n               values=['tree_length'], aggfunc=['min', 'max', 'mean', 'median', 'count'], margins=True)\n\n\n\n\n\n\n  \n    \n      \n      \n      min\n      max\n      mean\n      median\n      count\n    \n    \n      \n      \n      tree_length\n      tree_length\n      tree_length\n      tree_length\n      tree_length\n    \n    \n      conservation\n      label\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      groundwood\n      0.605847\n      24.479043\n      3.113435\n      2.609120\n      2369\n    \n    \n      uprightwood\n      0.674329\n      8.319080\n      2.310101\n      2.080163\n      386\n    \n    \n      1\n      groundwood\n      0.575351\n      22.625422\n      3.914179\n      3.145306\n      1546\n    \n    \n      uprightwood\n      0.779996\n      7.974969\n      3.200828\n      2.981814\n      1033\n    \n    \n      All\n      \n      0.575351\n      24.479043\n      3.304312\n      2.779696\n      5334\n    \n  \n\n\n\n\n\n\n\n\n\nIn addition of annotated fallen trees being longer in Evo than Hiidenportti, they were also thicker, as the average diameter around 20mm more in both forest types, and only 7 annotations had diameter less than 100mm.\n\n\nCode\nevo_data['diam'] = evo_data.geometry.apply(lambda row: np.mean(get_three_point_diams(row))) * 1000\npd.pivot_table(evo_data[evo_data.label=='groundwood'], index=['conservation'], \n               values=['diam'], aggfunc=['min', 'max', 'mean', 'median', 'count'], margins=True)\n\n\n\n\n\n\n  \n    \n      \n      min\n      max\n      mean\n      median\n      count\n    \n    \n      \n      diam\n      diam\n      diam\n      diam\n      diam\n    \n    \n      conservation\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      85.977991\n      760.781151\n      200.474161\n      188.603269\n      2369\n    \n    \n      1\n      101.096058\n      709.170305\n      254.250350\n      233.417789\n      1546\n    \n    \n      All\n      85.977991\n      760.781151\n      221.709917\n      203.275219\n      3915\n    \n  \n\n\n\n\n\n\n\n\n\nLook at the largest polygons.\n\n\n\n\n\n\n\nCode\nevo_data['v'] = evo_data.geometry.apply(cut_cone_volume)\npd.pivot_table(evo_data[evo_data.label=='groundwood'], index=['conservation'], \n               values=['v'], aggfunc=['min', 'max', 'mean', 'median', 'count', 'sum'], margins=True)\n\n\n\n\n\n\n  \n    \n      \n      min\n      max\n      mean\n      median\n      count\n      sum\n    \n    \n      \n      v\n      v\n      v\n      v\n      v\n      v\n    \n    \n      conservation\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      0.004749\n      2.765285\n      0.125592\n      0.076470\n      2369\n      297.527245\n    \n    \n      1\n      0.011879\n      4.192773\n      0.262005\n      0.141118\n      1546\n      405.059473\n    \n    \n      All\n      0.004749\n      4.192773\n      0.179460\n      0.092882\n      3915\n      702.586718\n    \n  \n\n\n\n\n\n\n\n\n\nThe estimated total volume of fallen deadwood was 702.59 m3, of which 297.52m3 was in managed forests and 405.06m3 in conservation areas, corresponding to 7.08m3/ha of deadwood in managed forests and 13.97 m3/ha in conserved forests.\n\n\nCode\nevo_fd_path = Path('../data/sudenpesankangas/')\nevo_grid = gpd.read_file(evo_fd_path/'vplots.geojson')\nevo_grid = evo_grid.to_crs('epsg:3067')\nevo_tot_vplot_area = evo_grid.area.sum()\nevo_cons_area = cons_evo.overlay(evo_grid).area.sum()\nevo_man_area = evo_tot_vplot_area - evo_cons_area\nevo_man_ha = evo_man_area / 10000\nevo_cons_ha = evo_cons_area / 10000\nevo_tot_ha = evo_man_ha + evo_cons_ha\n\n\n\n\nCode\nevo_est_v_man = evo_data[(evo_data.label=='groundwood')&(evo_data.conservation==0)].v.sum()/evo_man_ha\nevo_est_v_cons = evo_data[(evo_data.label=='groundwood')&(evo_data.conservation==1)].v.sum()/evo_cons_ha\nevo_est_v_tot = evo_data[(evo_data.label=='groundwood')].v.sum()/evo_tot_ha\n\n\n\n\nCode\nprint(f'Estimated groundwood volume in managed forests: {evo_est_v_man:.2f} ha/m³')\nprint(f'Estimated groundwood volume in conserved forests: {evo_est_v_cons:.2f} ha/m³')\nprint(f'Estimated groundwood volume in both types: {evo_est_v_tot:.2f} ha/m³')\n\n\nEstimated groundwood volume in managed forests: 7.08 ha/m³\nEstimated groundwood volume in conserved forests: 13.97 ha/m³\nEstimated groundwood volume in both types: 9.90 ha/m³\n\n\n\n\n1.2.3 Training data generation\nIn order to ensure that no training data leaked into the test set, which would have given our models overly optimistic results, we selected 5 of the virtual plots as the holdout set to test our models. These virtual plots contained both managed and conserved areas, and were only used for testing the models. Furthermore, the remaining 28 plots were spatially split to training and validation sets with 85:15 ratio to ensure that the models were validated during training with data not used for training.\nAs our virtual plots had too large dimensions to be efficiently used as the training data for the deep learning models, the final preprocessing step for UAV plot images was to split each of them into 512 times 512 pixel image patches. We also split the the polygon data into separate files so that each file only covers this smaller area. Finally, we converted the polygon data into COCO format, in which the polygon coordinates are converted into pixel coordinates related to the corresponding image. Because mosaicking images this way meant that some of the annotated polygons were only partially present in image patches, we removed all partial annotations whose area was less than 25% of the original annotation. Also, all polygons that had bounding box smaller than 16 pixels were discarded, as that small targets can not be accurately found from the data. For test images, as the model outputs were in COCO format, the results were transformed into georeferenced polygon data.\n\n\nCode\ntile_folder = Path('../../data/raw/hiidenportti/virtual_plots/train/images/')\nvector_folder = Path('../../data/raw/hiidenportti/virtual_plots/train/vectors/')\n\noutpath = Path('../../data/processed/hiidenportti/train_512')\n\ntiles = os.listdir(tile_folder)\nvectors = [f for f in os.listdir(vector_folder) if f.endswith('geojson')]\nassert len(tiles) == len(vectors)\n\n\n\n\nCode\nfor t in tiles:\n    if not os.path.exists(outpath/t[:-4]): os.makedirs(outpath/t[:-4])\n    shp_fname = t.replace('tif', 'geojson')\n    tilesize = 512\n    tiler = Tiler(outpath=outpath/t[:-4], gridsize_x=tilesize, gridsize_y=tilesize, overlap=(0,0))\n    tiler.tile_raster(str(tile_folder/t))\n    tiler.tile_vector(vector_folder/shp_fname, min_area_pct=0.25)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport shapely\nfrom shapely.geometry import box\n#Fix labelling, todo fix it in COCOProcessor\n\nfor p in os.listdir(outpath):\n    files = [outpath/p/'vector_tiles'/f for f in os.listdir(outpath/p/'vector_tiles') if f.endswith('geojson')]\n    for f in files: \n        gdf = gpd.read_file(f)\n        bbox = box(*gdf.total_bounds)\n        gdf['geometry'] = gdf.geometry.buffer(0) # fix faulty geometries\n        gdf['geometry'] = gdf.apply(lambda row: fix_multipolys(row.geometry) if row.geometry.type == 'MultiPolygon' \n                                    else shapely.geometry.Polygon(row.geometry.exterior), axis=1)\n        gdf.rename(columns={'groundwood':'label'}, inplace=True)\n        gdf = gpd.clip(gdf, bbox, keep_geom_type=True)\n        gdf['label'] = gdf.apply(lambda row: 'Standing' if row.label == 1 else 'Fallen', axis=1)\n        gdf.to_file(f, driver='GeoJSON')\n\n\n\n\nCode\n# Convert to COCO format\n\nfrom drone_detector.processing.coco import *\n\ndeadwood_categories = [\n        {'supercategory':'deadwood', 'id':1, 'name': 'uprightwood'},\n        {'supercategory':'deadwood', 'id':2, 'name': 'groundwood'},\n    ]\n\nfrom datetime import date\n\ncoco_info = {'description': 'Train dataset for deadwood detection in Hiidenportti',\n             'version': 0.1,\n             'year': 2022,\n             'contributor': 'Janne Mäyrä',\n             'date_created': date.today().strftime(\"%Y/%m/%d\")\n}\n\ncoco_licenses = {}\n\nfor p in os.listdir(outpath):\n    coco_processor = COCOProcessor(outpath/p, outpath/p, coco_info=coco_info, coco_licenses=coco_licenses,\n                                  coco_categories=deadwood_categories)\n    coco_processor.shp_to_coco('layer')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Combine several coco-annotation .json files into one\n\nimport json\n\nfull_coco = None\nimage_id_modifier = 0\nann_id_modifier = 0\nfor p in os.listdir(outpath):\n    with open(outpath/p/'coco.json') as f:\n        coco = json.load(f)\n    \n    # update filename\n    for i in coco['images']:\n        i['file_name'] = f\"{p}/raster_tiles/{i['file_name']}\"\n        \n    if full_coco is None: \n        full_coco = coco\n        image_id_modifier = full_coco['images'][-1]['id']\n        ann_id_modifier = full_coco['annotations'][-1]['id']\n    else:\n        for i in coco['images']:\n            i['id'] += image_id_modifier\n        for a in coco['annotations']:\n            a['image_id'] += image_id_modifier\n            a['id'] += ann_id_modifier\n        \n        full_coco['images'].extend(coco['images'])\n        full_coco['annotations'].extend(coco['annotations'])\n        image_id_modifier = full_coco['images'][-1]['id'] + 1\n        ann_id_modifier = full_coco['annotations'][-1]['id'] + 1\n        \nwith open(outpath.parents[0]/'hiidenportti_train.json', 'w') as outfile: json.dump(full_coco, outfile)\n\n\n\n\nCode\ntile_folder = Path('../../data/raw/hiidenportti/virtual_plots/valid/images/')\nvector_folder = Path('../../data/raw/hiidenportti/virtual_plots/valid/vectors/')\n\noutpath = Path('../../data/processed/hiidenportti/valid_512')\n\ntiles = os.listdir(tile_folder)\nvectors = [f for f in os.listdir(vector_folder) if f.endswith('geojson')]\nassert len(tiles) == len(vectors)\n\n\n\n\nCode\nfor t in tiles:\n    if not os.path.exists(outpath/t[:-4]): os.makedirs(outpath/t[:-4])\n    shp_fname = t.replace('tif', 'geojson')\n    tilesize = 512\n    tiler = Tiler(outpath=outpath/t[:-4], gridsize_x=tilesize, gridsize_y=tilesize, overlap=(0,0))\n    tiler.tile_raster(str(tile_folder/t))\n    tiler.tile_vector(vector_folder/shp_fname, min_area_pct=0.25)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n#Fix labelling, todo fix it in COCOProcessor\n\nfor p in os.listdir(outpath):\n    files = [outpath/p/'vector_tiles'/f for f in os.listdir(outpath/p/'vector_tiles') if f.endswith('geojson')]\n    for f in files: \n        gdf = gpd.read_file(f)\n        bbox = box(*gdf.total_bounds)\n        gdf['geometry'] = gdf.geometry.buffer(0) # fix faulty geometries\n        gdf['geometry'] = gdf.apply(lambda row: fix_multipolys(row.geometry) if row.geometry.type == 'MultiPolygon' \n                                    else shapely.geometry.Polygon(row.geometry.exterior), axis=1)\n        gdf.rename(columns={'groundwood':'label'}, inplace=True)\n        gdf = gpd.clip(gdf, bbox, keep_geom_type=True)\n        gdf['label'] = gdf.apply(lambda row: 'Standing' if row.label == 1 else 'Fallen', axis=1)\n        gdf.to_file(f, driver='GeoJSON')\n\n\n\n\nCode\n# Convert to COCO format\n\nfrom drone_detector.processing.coco import *\n\ndeadwood_categories = [\n        {'supercategory':'deadwood', 'id':1, 'name': 'uprightwood'},\n        {'supercategory':'deadwood', 'id':2, 'name': 'groundwood'},\n    ]\n\nfrom datetime import date\n\ncoco_info = {'description': 'Validation dataset for deadwood detection in Hiidenportti',\n             'version': 0.1,\n             'year': 2022,\n             'contributor': 'Janne Mäyrä',\n             'date_created': date.today().strftime(\"%Y/%m/%d\")\n}\n\ncoco_licenses = {}\n\nfor p in os.listdir(outpath):\n    coco_processor = COCOProcessor(outpath/p, outpath/p, coco_info=coco_info, coco_licenses=coco_licenses,\n                                  coco_categories=deadwood_categories)\n    coco_processor.shp_to_coco('layer')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Combine several coco-annotation .json files into one\n\nimport json\n\nfull_coco = None\nimage_id_modifier = 0\nann_id_modifier = 0\nfor p in os.listdir(outpath):\n    with open(outpath/p/'coco.json') as f:\n        coco = json.load(f)\n    \n    # update filename\n    for i in coco['images']:\n        i['file_name'] = f\"{p}/raster_tiles/{i['file_name']}\"\n        \n    if full_coco is None: \n        full_coco = coco\n        image_id_modifier = full_coco['images'][-1]['id']\n        ann_id_modifier = full_coco['annotations'][-1]['id']\n    else:\n        for i in coco['images']:\n            i['id'] += image_id_modifier\n        for a in coco['annotations']:\n            a['image_id'] += image_id_modifier\n            a['id'] += ann_id_modifier\n        \n        full_coco['images'].extend(coco['images'])\n        full_coco['annotations'].extend(coco['annotations'])\n        image_id_modifier = full_coco['images'][-1]['id'] + 1\n        ann_id_modifier = full_coco['annotations'][-1]['id'] + 1\n        \nwith open(outpath.parents[0]/'hiidenportti_valid.json', 'w') as outfile: json.dump(full_coco, outfile)\n\n\n\n\nCode\ntile_folder = Path('../../data/raw/hiidenportti/virtual_plots/test/images/')\nvector_folder = Path('../../data/raw/hiidenportti/virtual_plots/test/vectors/')\n\noutpath = Path('../../data/processed/hiidenportti/test_512')\n\ntiles = os.listdir(tile_folder)\nvectors = [f for f in os.listdir(vector_folder) if f.endswith('geojson')]\nassert len(tiles) == len(vectors)\n\n\n\n\nCode\nfor t in tiles:\n    if not os.path.exists(outpath/t[:-4]): os.makedirs(outpath/t[:-4])\n    shp_fname = t.replace('tif', 'geojson')\n    tilesize = 512\n    tiler = Tiler(outpath=outpath/t[:-4], gridsize_x=tilesize, gridsize_y=tilesize, overlap=(0,0))\n    tiler.tile_raster(str(tile_folder/t))\n    tiler.tile_vector(vector_folder/shp_fname, min_area_pct=0.25)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n#Fix labelling, todo fix it in COCOProcessor\n\nfor p in os.listdir(outpath):\n    files = [outpath/p/'vector_tiles'/f for f in os.listdir(outpath/p/'vector_tiles') if f.endswith('geojson')]\n    for f in files: \n        gdf = gpd.read_file(f)\n        bbox = box(*gdf.total_bounds)\n        gdf['geometry'] = gdf.geometry.buffer(0) # fix faulty geometries\n        gdf['geometry'] = gdf.apply(lambda row: fix_multipolys(row.geometry) if row.geometry.type == 'MultiPolygon' \n                                    else shapely.geometry.Polygon(row.geometry.exterior), axis=1)\n        gdf.rename(columns={'groundwood':'label'}, inplace=True)\n        gdf = gpd.clip(gdf, bbox, keep_geom_type=True)\n        gdf['label'] = gdf.apply(lambda row: 'Standing' if row.label == 1 else 'Fallen', axis=1)\n        gdf.to_file(f, driver='GeoJSON')\n\n\n\n\nCode\n# Convert to COCO format\n\nfrom drone_detector.processing.coco import *\n\ndeadwood_categories = [\n        {'supercategory':'deadwood', 'id':1, 'name': 'uprightwood'},\n        {'supercategory':'deadwood', 'id':2, 'name': 'groundwood'},\n    ]\n\nfrom datetime import date\n\ncoco_info = {'description': 'Test dataset for deadwood detection in Hiidenportti',\n             'version': 0.1,\n             'year': 2022,\n             'contributor': 'Janne Mäyrä',\n             'date_created': date.today().strftime(\"%Y/%m/%d\")\n}\n\ncoco_licenses = {}\n\nfor p in os.listdir(outpath):\n    coco_processor = COCOProcessor(outpath/p, outpath/p, coco_info=coco_info, coco_licenses=coco_licenses,\n                                  coco_categories=deadwood_categories)\n    coco_processor.shp_to_coco('layer')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Combine several coco-annotation .json files into one\n\nimport json\n\nfull_coco = None\nimage_id_modifier = 0\nann_id_modifier = 0\nfor p in os.listdir(outpath):\n    with open(outpath/p/'coco.json') as f:\n        coco = json.load(f)\n    \n    # update filename\n    for i in coco['images']:\n        i['file_name'] = f\"{p}/raster_tiles/{i['file_name']}\"\n        \n    if full_coco is None: \n        full_coco = coco\n        image_id_modifier = full_coco['images'][-1]['id']\n        ann_id_modifier = full_coco['annotations'][-1]['id']\n    else:\n        for i in coco['images']:\n            i['id'] += image_id_modifier\n        for a in coco['annotations']:\n            a['image_id'] += image_id_modifier\n            a['id'] += ann_id_modifier\n        \n        full_coco['images'].extend(coco['images'])\n        full_coco['annotations'].extend(coco['annotations'])\n        image_id_modifier = full_coco['images'][-1]['id'] + 1\n        ann_id_modifier = full_coco['annotations'][-1]['id'] + 1\n        \nwith open(outpath.parents[0]/'hiidenportti_test.json', 'w') as outfile: json.dump(full_coco, outfile)\n\n\n\n\nCode\ntile_folder = Path('../../data/raw/sudenpesankangas/virtual_plots/train/images/')\nvector_folder = Path('../../data/raw/sudenpesankangas/virtual_plots/train/vectors/')\n\noutpath = Path('../../data/processed/sudenpesankangas/train_512')\n\ntiles = os.listdir(tile_folder)\nvectors = [f for f in os.listdir(vector_folder) if f.endswith('geojson')]\nassert len(tiles) == len(vectors)\n\n\n\n\nCode\nfor t in tiles:\n    if not os.path.exists(outpath/t[:-4]): os.makedirs(outpath/t[:-4])\n    shp_fname = t.replace('tif', 'geojson')\n    tilesize = 512\n    tiler = Tiler(outpath=outpath/t[:-4], gridsize_x=tilesize, gridsize_y=tilesize, overlap=(0,0))\n    tiler.tile_raster(str(tile_folder/t))\n    tiler.tile_vector(vector_folder/shp_fname, min_area_pct=0.25)\n\n\n\n\nCode\n#Fix labelling, todo fix it in COCOProcessor\n\nfor p in os.listdir(outpath):\n    files = [outpath/p/'vector_tiles'/f for f in os.listdir(outpath/p/'vector_tiles') if f.endswith('geojson')]\n    for f in files: \n        gdf = gpd.read_file(f)\n        bbox = box(*gdf.total_bounds)\n        gdf['geometry'] = gdf.geometry.buffer(0) # fix faulty geometries\n        gdf['geometry'] = gdf.apply(lambda row: fix_multipolys(row.geometry) if row.geometry.type == 'MultiPolygon' \n                                    else shapely.geometry.Polygon(row.geometry.exterior), axis=1)\n        gdf = gpd.clip(gdf, bbox, keep_geom_type=True)\n        gdf.to_file(f, driver='GeoJSON')\n\n\n\n\nCode\n# Convert to COCO format\n\nfrom drone_detector.processing.coco import *\n\ndeadwood_categories = [\n        {'supercategory':'deadwood', 'id':1, 'name': 'uprightwood'},\n        {'supercategory':'deadwood', 'id':2, 'name': 'groundwood'},\n    ]\n\nfrom datetime import date\n\ncoco_info = {'description': 'Train dataset for deadwood detection in Sudenpesänkangas',\n             'version': 0.1,\n             'year': 2022,\n             'contributor': 'Janne Mäyrä',\n             'date_created': date.today().strftime(\"%Y/%m/%d\")\n}\n\ncoco_licenses = {}\n\nfor p in os.listdir(outpath):\n    coco_processor = COCOProcessor(outpath/p, outpath/p, coco_info=coco_info, coco_licenses=coco_licenses,\n                                  coco_categories=deadwood_categories)\n    coco_processor.shp_to_coco('label')\n\n\n\n\nCode\n# Combine several coco-annotation .json files into one\n\nimport json\n\nfull_coco = None\nimage_id_modifier = 0\nann_id_modifier = 0\nfor p in os.listdir(outpath):\n    with open(outpath/p/'coco.json') as f:\n        coco = json.load(f)\n    \n    # update filename\n    for i in coco['images']:\n        i['file_name'] = f\"{p}/raster_tiles/{i['file_name']}\"\n        \n    if full_coco is None: \n        full_coco = coco\n        image_id_modifier = full_coco['images'][-1]['id']\n        ann_id_modifier = full_coco['annotations'][-1]['id']\n    else:\n        for i in coco['images']:\n            i['id'] += image_id_modifier\n        for a in coco['annotations']:\n            a['image_id'] += image_id_modifier\n            a['id'] += ann_id_modifier\n        \n        full_coco['images'].extend(coco['images'])\n        full_coco['annotations'].extend(coco['annotations'])\n        image_id_modifier = full_coco['images'][-1]['id'] + 1\n        ann_id_modifier = full_coco['annotations'][-1]['id'] + 1\n        \nwith open(outpath.parents[0]/'sudenpesankangas_train.json', 'w') as outfile: json.dump(full_coco, outfile)\n\n\n\n\nCode\ntile_folder = Path('../../data/raw/sudenpesankangas/virtual_plots/valid/images/')\nvector_folder = Path('../../data/raw/sudenpesankangas/virtual_plots/valid/vectors/')\n\noutpath = Path('../../data/processed/sudenpesankangas/valid_512')\n\ntiles = os.listdir(tile_folder)\nvectors = [f for f in os.listdir(vector_folder) if f.endswith('geojson')]\nassert len(tiles) == len(vectors)\n\n\n\n\nCode\nfor t in tiles:\n    if not os.path.exists(outpath/t[:-4]): os.makedirs(outpath/t[:-4])\n    shp_fname = t.replace('tif', 'geojson')\n    tilesize = 512\n    tiler = Tiler(outpath=outpath/t[:-4], gridsize_x=tilesize, gridsize_y=tilesize, overlap=(0,0))\n    tiler.tile_raster(str(tile_folder/t))\n    tiler.tile_vector(vector_folder/shp_fname, min_area_pct=0.25)\n\n\n\n\nCode\n#Fix labelling, todo fix it in COCOProcessor\n\nfor p in os.listdir(outpath):\n    files = [outpath/p/'vector_tiles'/f for f in os.listdir(outpath/p/'vector_tiles') if f.endswith('geojson')]\n    for f in files: \n        gdf = gpd.read_file(f)\n        bbox = box(*gdf.total_bounds)\n        gdf['geometry'] = gdf.geometry.buffer(0) # fix faulty geometries\n        gdf['geometry'] = gdf.apply(lambda row: fix_multipolys(row.geometry) if row.geometry.type == 'MultiPolygon' \n                                    else shapely.geometry.Polygon(row.geometry.exterior), axis=1)\n        gdf = gpd.clip(gdf, bbox, keep_geom_type=True)\n        gdf.to_file(f, driver='GeoJSON')\n\n\n\n\nCode\n# Convert to COCO format\n\nfrom drone_detector.processing.coco import *\n\ndeadwood_categories = [\n        {'supercategory':'deadwood', 'id':1, 'name': 'uprightwood'},\n        {'supercategory':'deadwood', 'id':2, 'name': 'groundwood'},\n    ]\n\nfrom datetime import date\n\ncoco_info = {'description': 'Train dataset for deadwood detection in Sudenpesänkangas',\n             'version': 0.1,\n             'year': 2022,\n             'contributor': 'Janne Mäyrä',\n             'date_created': date.today().strftime(\"%Y/%m/%d\")\n}\n\ncoco_licenses = {}\n\nfor p in os.listdir(outpath):\n    coco_processor = COCOProcessor(outpath/p, outpath/p, coco_info=coco_info, coco_licenses=coco_licenses,\n                                  coco_categories=deadwood_categories)\n    coco_processor.shp_to_coco('label')\n\n\n\n\nCode\n# Combine several coco-annotation .json files into one\n\nimport json\n\nfull_coco = None\nimage_id_modifier = 0\nann_id_modifier = 0\nfor p in os.listdir(outpath):\n    with open(outpath/p/'coco.json') as f:\n        coco = json.load(f)\n    \n    # update filename\n    for i in coco['images']:\n        i['file_name'] = f\"{p}/raster_tiles/{i['file_name']}\"\n        \n    if full_coco is None: \n        full_coco = coco\n        image_id_modifier = full_coco['images'][-1]['id']\n        ann_id_modifier = full_coco['annotations'][-1]['id']\n    else:\n        for i in coco['images']:\n            i['id'] += image_id_modifier\n        for a in coco['annotations']:\n            a['image_id'] += image_id_modifier\n            a['id'] += ann_id_modifier\n        \n        full_coco['images'].extend(coco['images'])\n        full_coco['annotations'].extend(coco['annotations'])\n        image_id_modifier = full_coco['images'][-1]['id'] + 1\n        ann_id_modifier = full_coco['annotations'][-1]['id'] + 1\n        \nwith open(outpath.parents[0]/'sudenpesankangas_valid.json', 'w') as outfile: json.dump(full_coco, outfile)\n\n\n\n\nCode\ntile_folder = Path('../../data/raw/sudenpesankangas/virtual_plots/test/images/')\nvector_folder = Path('../../data/raw/sudenpesankangas/virtual_plots/test/vectors/')\n\noutpath = Path('../../data/processed/sudenpesankangas/test_512')\n\ntiles = os.listdir(tile_folder)\nvectors = [f for f in os.listdir(vector_folder) if f.endswith('geojson')]\nassert len(tiles) == len(vectors)\n\n\n\n\nCode\nfor t in tiles:\n    if not os.path.exists(outpath/t[:-4]): os.makedirs(outpath/t[:-4])\n    shp_fname = t.replace('tif', 'geojson')\n    tilesize = 512\n    tiler = Tiler(outpath=outpath/t[:-4], gridsize_x=tilesize, gridsize_y=tilesize, overlap=(0,0))\n    tiler.tile_raster(str(tile_folder/t))\n    tiler.tile_vector(vector_folder/shp_fname, min_area_pct=0.25)\n\n\n\n\nCode\n#Fix labelling, todo fix it in COCOProcessor\n\nfor p in os.listdir(outpath):\n    files = [outpath/p/'vector_tiles'/f for f in os.listdir(outpath/p/'vector_tiles') if f.endswith('geojson')]\n    for f in files: \n        gdf = gpd.read_file(f)\n        bbox = box(*gdf.total_bounds)\n        gdf['geometry'] = gdf.geometry.buffer(0) # fix faulty geometries\n        gdf['geometry'] = gdf.apply(lambda row: fix_multipolys(row.geometry) if row.geometry.type == 'MultiPolygon' \n                                    else shapely.geometry.Polygon(row.geometry.exterior), axis=1)\n        gdf = gpd.clip(gdf, bbox, keep_geom_type=True)\n        gdf.to_file(f, driver='GeoJSON')\n\n\n\n\nCode\n# Convert to COCO format\n\nfrom drone_detector.processing.coco import *\n\ndeadwood_categories = [\n        {'supercategory':'deadwood', 'id':1, 'name': 'uprightwood'},\n        {'supercategory':'deadwood', 'id':2, 'name': 'groundwood'},\n    ]\n\nfrom datetime import date\n\ncoco_info = {'description': 'Test dataset for deadwood detection in Sudenpesänkangas',\n             'version': 0.1,\n             'year': 2022,\n             'contributor': 'Janne Mäyrä',\n             'date_created': date.today().strftime(\"%Y/%m/%d\")\n}\n\ncoco_licenses = {}\n\nfor p in os.listdir(outpath):\n    coco_processor = COCOProcessor(outpath/p, outpath/p, coco_info=coco_info, coco_licenses=coco_licenses,\n                                  coco_categories=deadwood_categories)\n    coco_processor.shp_to_coco('label')\n\n\n\n\nCode\n# Combine several coco-annotation .json files into one\n\nimport json\n\nfull_coco = None\nimage_id_modifier = 0\nann_id_modifier = 0\nfor p in os.listdir(outpath):\n    with open(outpath/p/'coco.json') as f:\n        coco = json.load(f)\n    \n    # update filename\n    for i in coco['images']:\n        i['file_name'] = f\"{p}/raster_tiles/{i['file_name']}\"\n        \n    if full_coco is None: \n        full_coco = coco\n        image_id_modifier = full_coco['images'][-1]['id']\n        ann_id_modifier = full_coco['annotations'][-1]['id']\n    else:\n        for i in coco['images']:\n            i['id'] += image_id_modifier\n        for a in coco['annotations']:\n            a['image_id'] += image_id_modifier\n            a['id'] += ann_id_modifier\n        \n        full_coco['images'].extend(coco['images'])\n        full_coco['annotations'].extend(coco['annotations'])\n        image_id_modifier = full_coco['images'][-1]['id'] + 1\n        ann_id_modifier = full_coco['annotations'][-1]['id'] + 1\n        \nwith open(outpath.parents[0]/'sudenpesankangas_test.json', 'w') as outfile: json.dump(full_coco, outfile)"
  },
  {
    "objectID": "2_comparing_annotations_and_field_data.html",
    "href": "2_comparing_annotations_and_field_data.html",
    "title": "Comparison of plot characteristics and digitized deadwood",
    "section": "",
    "text": "Code\nfrom drone_detector.utils import * \nfrom drone_detector.imports import *\nimport rasterio.mask as rio_mask\nimport seaborn as sns\nsns.set_style('whitegrid')\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nsys.path.append('..')\nfrom src.tree_functions import *\nThe following shows how we compared the field-measured data from both study sites and annotations that are located within these plots."
  },
  {
    "objectID": "2_comparing_annotations_and_field_data.html#hiidenportti",
    "href": "2_comparing_annotations_and_field_data.html#hiidenportti",
    "title": "Comparison of plot characteristics and digitized deadwood",
    "section": "1 Hiidenportti",
    "text": "1 Hiidenportti\nRead data and do some wrangling.\n\n\nCode\nfield_data_path = Path('../data/hiidenportti')\nanns = gpd.read_file('../../data/raw/hiidenportti/virtual_plots/all_deadwood_hiidenportti.geojson')\nplot_circles = gpd.read_file(field_data_path/'plot_circles.geojson')\nfield_data = pd.read_csv(field_data_path/'all_plot_data.csv')\nvirtual_plot_grid = gpd.read_file(field_data_path/'envelopes_with_trees.geojson')\nconservation_areas = gpd.read_file('../data/common/LsAlueValtio.shp')\ncons_hp = conservation_areas[conservation_areas.geometry.intersects(box(*anns.total_bounds))]\ncons_hp = gpd.clip(cons_hp, virtual_plot_grid)\n\n\nFilter plot circles so that only those present in scenes remain.\n\n\nCode\nplot_circles['in_vplot'] = plot_circles.apply(lambda row: 1 if any(virtual_plot_grid.geometry.contains(row.geometry)) \n                                              else 0, axis=1)\nplot_circles['id'] = plot_circles['id'].astype(int)\nfield_data = field_data[field_data.id.isin(plot_circles[plot_circles.in_vplot==1].id.unique())]\nfield_data.rename(columns= {c: c.replace('.','_') for c in field_data.columns}, inplace=True)\ndw_cols = ['id'] + [c for c in field_data.columns if 'dw' in c]\nplot_dw_data = field_data[dw_cols].copy()\nplot_circles = plot_circles[plot_circles.in_vplot == 1]\n\n\nRead individual tree measurements.\n\n\nCode\ntree_data = pd.read_csv(field_data_path/'hp_tree_data_fixed_1512.csv')\ntree_data = tree_data[tree_data.plot_id.isin(plot_dw_data.id.unique())]\n\n\nCount the number of deadwood (n_dw), fallen deadwood (n_ddw) and standing deadwood (n_udw) from the individual field data measurements.\n\n\nCode\nplot_dw_data['n_dw_plot'] = plot_dw_data.id.apply(lambda row: len(tree_data[tree_data.plot_id == row]))\nplot_dw_data['n_ddw_plot'] = plot_dw_data.id.apply(lambda row: len(tree_data[(tree_data.plot_id == row) \n                                                                             & (tree_data.tree_class == 4)]))\nplot_dw_data['n_udw_plot'] = plot_dw_data.id.apply(lambda row: len(tree_data[(tree_data.plot_id == row) \n                                                                             & (tree_data.tree_class == 3)]))\n\n\n\n\nCode\ndef match_circular_plot(row, plots):\n    \"Match annotations with field plots\"\n    for p in plots.itertuples():\n        if row.geometry.intersects(p.geometry):\n            return int(p.id)\n\n\n\n\nCode\nanns['plot_id'] = anns.apply(lambda row: match_circular_plot(row, plot_circles), axis=1)\nanns_in_plots = anns.overlay(plot_circles[['geometry']])\nanns_in_plots['plot_id'] = anns_in_plots.plot_id.astype(int)\n\n\nCount the number of annotated deadwood instances in each circular field plot, as well as note which of the circular plots are located in the conserved areas.\n\n\nCode\nplot_dw_data['n_dw_ann'] = plot_dw_data.apply(lambda row: anns_in_plots.plot_id.value_counts()[row.id] \n                                              if row.id in anns_in_plots.plot_id.unique() else 0, axis=1)\nplot_dw_data['n_ddw_ann'] = plot_dw_data.apply(lambda row: anns_in_plots[anns_in_plots.groundwood==2].plot_id.value_counts()[row.id] \n                                              if row.id in anns_in_plots[anns_in_plots.groundwood==2].plot_id.unique() else 0, axis=1)\nplot_dw_data['n_udw_ann'] = plot_dw_data.apply(lambda row: anns_in_plots[anns_in_plots.groundwood==1].plot_id.value_counts()[row.id] \n                                              if row.id in anns_in_plots[anns_in_plots.groundwood==1].plot_id.unique() else 0, axis=1)\nplot_dw_data['geometry'] = plot_dw_data.apply(lambda row: plot_circles[plot_circles.id == row.id].geometry.iloc[0], \n                                              axis=1)\nplot_dw_data = gpd.GeoDataFrame(plot_dw_data, crs=plot_circles.crs)\nplot_dw_data['conservation'] = plot_dw_data.apply(lambda row: 1 if any(cons_hp.geometry.contains(row.geometry))\n                                                  else 0, axis=1)\n\n\nAdd information about conservation area to tree data.\n\n\nCode\ntree_data['conservation'] = tree_data.apply(lambda row: plot_dw_data[plot_dw_data.id == row.plot_id].conservation.unique()[0], axis=1)\n\n\nPlot the numbers. These show that we were able to annotate slighly less deadwood than what was measured in the field. On the other hand, especially in managed forests there were also several cases where there were more fallen deadwood annotations than field measured fallen deadwood. As the resolution of our data makes it impossible to accurately tell whether a fallen trunk had DBH less than 100mm, some of the annotations can be such that were determined too thin in the field. Other likely reason is that canopy obscured the deadwood instances, as the average canopy cover percentage was around 61 % for the field plots.\n\n\nCode\nplot_dw_data.pivot_table(index='conservation', values=['n_ddw_plot', 'n_udw_plot', 'n_ddw_ann', 'n_udw_ann'], \n                         aggfunc='sum', margins=True)\n\n\n\n\n\n\n  \n    \n      \n      n_ddw_ann\n      n_ddw_plot\n      n_udw_ann\n      n_udw_plot\n    \n    \n      conservation\n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      248\n      217\n      49\n      75\n    \n    \n      1\n      48\n      96\n      10\n      24\n    \n    \n      All\n      296\n      313\n      59\n      99\n    \n  \n\n\n\n\nGet plot-wise canopy cover percentage based on LiDAR derived canopy height model as the percentage of plot area with height more than 2 meters.\n\n\nCode\npcts = []\n\nwith rio.open('../../data/raw/hiidenportti/full_mosaics/CHM_Hiidenportti_epsg.tif') as src:\n    crs = src.crs\n    for row in plot_dw_data.itertuples():\n        plot_im, plot_tfm = rio_mask.mask(src, [row.geometry], crop=True)\n        pcts.append(plot_im[plot_im > 2].shape[0] / plot_im[plot_im >= 0].shape[0])\n\nplot_dw_data['canopy_cover_pct'] = pcts\n\npd.pivot_table(data=plot_dw_data, index=['conservation'], values=['canopy_cover_pct'],\n               aggfunc=['min', 'max', 'mean', 'std', 'count'], margins=True)\n\n\n\n\n\n\n  \n    \n      \n      min\n      max\n      mean\n      std\n      count\n    \n    \n      \n      canopy_cover_pct\n      canopy_cover_pct\n      canopy_cover_pct\n      canopy_cover_pct\n      canopy_cover_pct\n    \n    \n      conservation\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      0.142006\n      0.947162\n      0.593817\n      0.187985\n      51\n    \n    \n      1\n      0.083333\n      0.855172\n      0.641549\n      0.176416\n      20\n    \n    \n      All\n      0.083333\n      0.947162\n      0.607263\n      0.184815\n      71\n    \n  \n\n\n\n\nOverall the conserved forests are a bit more dense, but not that much.\nPlot the relationship between annotated deadwood and field-measured deadwood. Each dot is a single field plot.\n\n\n\n\n\nFor annotated groundwood, as the annotations can be twisted due to e.g. orthorectification errors, we use the longest side of the smallest rotated rectangle that can be fit around the polygon as a proxy for the trunk length.\nDiameter can be approximated with the following approach. For each polygon,\n\nGet minimum rotated rectangle\nDraw three lines that are perpendicular to the longer side of the minimum rotated rectangle, starting from the points of 10%, 50% and 90% of the length of the minimum rotated rectangle\nIntersect these lines with the polygon. Diameter is the mean lenght of these intersecting lines.\n\nThis is not the same as DBH, but close enough.\n\n\nCode\nanns_in_plots['tree_length'] = anns_in_plots.apply(lambda row: get_len(row.geometry), axis=1)\n\n\nCompare the distributions of the downed trunk lengths. Both graphs only take the parts within the plots into account. Lengths are binned into 1m bins.\nAs expected, the field measured fallen deadwood were clearly shorter than annotated deadwood. Most of the annotated fallen deadwood were shorter than 2 meters, while field measured trees were typically longer than 5 meters. However, this is most likely due to canopy obscuring the fallen deadwood, and some of the longer trees are annotated as multiple shorter ones. When comparing the difference in total lengths of field-measured trees and annotated trees, typically denser canopy cover meant also larger difference.\n\n\n\n\n\n\n\n\n\n\nAs expected, annotated trunks are clearly on average shorter than field measured.\nCompare the measured DBH for downed trees and estimated diameter of annotated downed deadwood. For annotated deadwood, the diameter is estimated for the whole tree, not only for the part within the field plot. DBHs are binned into 50mm bins.\nWhen comparing the DBH distributions for fallen deadwood, the diameter classes 125-150mm, 150-175mm and 175-200mm were clearly overrepresented in the annotated data. As these DBH classes were also the most common among field data, the reason for this could be the same as with length differences, and the visible trees area partially obscured by the canopy.\n\n\n\n\n\n\n\n\n\n\nAggregate the above, first field data.\n\n\nCode\npd.pivot_table(data=tree_data[(tree_data.tree_class == 4)&(tree_data.DBH>0)],\n               index=['conservation'], values=['l', 'DBH'],\n               aggfunc=['mean', 'min', 'max', 'count'], margins=True)\n\n\n\n\n\n\n  \n    \n      \n      mean\n      min\n      max\n      count\n    \n    \n      \n      DBH\n      l\n      DBH\n      l\n      DBH\n      l\n      DBH\n      l\n    \n    \n      conservation\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      133.786875\n      6.768601\n      15.558197\n      0.518478\n      450.000000\n      16.700000\n      217\n      217\n    \n    \n      1\n      185.538932\n      7.588368\n      39.167120\n      0.214113\n      580.560964\n      18.000666\n      96\n      96\n    \n    \n      All\n      149.659711\n      7.020031\n      15.558197\n      0.214113\n      580.560964\n      18.000666\n      313\n      313\n    \n  \n\n\n\n\nThen annotations.\n\n\nCode\npd.pivot_table(data=anns_in_plots[anns_in_plots.groundwood==2], index=['conservation'], values=['tree_length', 'diam'],\n               aggfunc=['mean', 'min', 'max', 'count'], margins=True)\n\n\n\n\n\n\n  \n    \n      \n      mean\n      min\n      max\n      count\n    \n    \n      \n      diam\n      tree_length\n      diam\n      tree_length\n      diam\n      tree_length\n      diam\n      tree_length\n    \n    \n      conservation\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      175.046987\n      1.818960\n      16.64560\n      0.095362\n      381.292305\n      6.485503\n      236\n      236\n    \n    \n      1\n      208.354703\n      2.083887\n      27.32243\n      0.149797\n      625.927586\n      5.907329\n      60\n      60\n    \n    \n      All\n      181.798551\n      1.872662\n      16.64560\n      0.095362\n      625.927586\n      6.485503\n      296\n      296\n    \n  \n\n\n\n\nCheck the relationship between canopy cover and deadwood types. First standing deadwood.\n\n\nCode\ng = sns.lmplot(data=plot_dw_data, x='n_udw_plot', y='canopy_cover_pct', col='conservation', hue='conservation', ci=1,\n                legend=False)\ng.axes[0,0].set_title('Managed forests')\ng.axes[0,1].set_title('Protected forests')\ng.axes[0,0].set_xticks(range(0,10,2))\ng.axes[0,1].set_xticks(range(0,10,2))\ng.set_ylabels('Canopy cover percentage ffor field plots')\ng.set_xlabels('Number of field-measured standing deadwood within plots')\nplt.show()\n\n\n\n\n\n\n\nCode\npd.pivot_table(data=anns_in_plots[anns_in_plots.groundwood==2], index=['conservation'], values=['tree_length', 'diam'],\n               aggfunc=['mean', 'min', 'max', 'count'], margins=True)\n\n\n\n\n\n\n  \n    \n      \n      mean\n      min\n      max\n      count\n    \n    \n      \n      diam\n      tree_length\n      diam\n      tree_length\n      diam\n      tree_length\n      diam\n      tree_length\n    \n    \n      conservation\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      175.046987\n      1.818960\n      16.64560\n      0.095362\n      381.292305\n      6.485503\n      236\n      236\n    \n    \n      1\n      208.354703\n      2.083887\n      27.32243\n      0.149797\n      625.927586\n      5.907329\n      60\n      60\n    \n    \n      All\n      181.798551\n      1.872662\n      16.64560\n      0.095362\n      625.927586\n      6.485503\n      296\n      296\n    \n  \n\n\n\n\n\n\nCode\ng = sns.lmplot(data=plot_dw_data, x='n_udw_ann', y='canopy_cover_pct', col='conservation', hue='conservation', ci=1,\n                legend=False)\ng.axes[0,0].set_title('Managed forests')\ng.axes[0,1].set_title('Protected forests')\ng.axes[0,0].set_xticks(range(0,10,2))\ng.axes[0,1].set_xticks(range(0,10,2))\ng.set_ylabels('Canopy cover percentage for field plots')\ng.set_xlabels('Number of annotated standing deadwood')\nplt.show()\n\n\n\n\n\nThen fallen deadwood\n\n\nCode\ng = sns.lmplot(data=plot_dw_data, x='n_ddw_plot', y='canopy_cover_pct', col='conservation', hue='conservation', ci=1,\n                legend=False)\ng.axes[0,0].set_title('Managed forests')\ng.axes[0,1].set_title('Protected forests')\ng.axes[0,0].set_xticks(range(0,21,2))\ng.axes[0,1].set_xticks(range(0,21,2))\ng.set_ylabels('Canopy cover percentage for field plots')\ng.set_xlabels('Number of field-measured fallen deadwood within plots')\nplt.show()\n\n\n\n\n\n\n\nCode\ng = sns.lmplot(data=plot_dw_data, x='n_ddw_ann', y='canopy_cover_pct', col='conservation', hue='conservation',\n                legend=False, ci=1)\ng.axes[0,0].set_title('Managed forests')\ng.axes[0,1].set_title('Protected forests')\ng.axes[0,0].set_xticks(range(0,21,2))\ng.axes[0,1].set_xticks(range(0,21,2))\ng.set_ylabels('Total lengths (m) of field-measured groundwood within plots')\ng.set_xlabels('Total lengths (m) of annotated groundwood within plots')\nplt.show()\n\n\n\n\n\nPlot the total lengths of groundwood located within a single field plot.\n\n\nCode\nplot_dw_data['n_dw_stand_field'] = plot_dw_data.apply(lambda row: len(tree_data[(tree_data.plot_id == row.id) &\n                                                                                (tree_data.tree_class == 3)]),\n                                                      axis=1)\nplot_dw_data['n_dw_fall_field'] = plot_dw_data.apply(lambda row: len(tree_data[(tree_data.plot_id == row.id) &\n                                                                               ((tree_data.tree_class == 4)|\n                                                                                (tree_data.tree_class == 5))]),\n                                                     axis=1)\nplot_dw_data['n_dw_tot_field'] = plot_dw_data.n_dw_stand_field + plot_dw_data.n_dw_fall_field\nplot_dw_data['l_gw_field'] = plot_dw_data.apply(lambda row: tree_data[(tree_data.plot_id == row.id) &\n                                                                      (tree_data.tree_class == 4)].l.sum()\n                                              , axis=1)\nplot_dw_data['l_gw_ann'] = plot_dw_data.apply(lambda row: anns_in_plots[(anns_in_plots.plot_id == row.id) &\n                                                                        (anns_in_plots.layer == 'groundwood')\n                                                                       ].tree_length.sum()\n                                              , axis=1)\n\n\n\n\n\n\n\nTotal lengths for annotations are generally much less than field-measured.\nPlot the difference in the numbers of annotated deadwood.\n\n\nCode\nplot_dw_data['n_ddw_plot_diff'] = plot_dw_data.n_ddw_plot - plot_dw_data.n_ddw_ann\nplot_dw_data['n_udw_plot_diff'] = plot_dw_data.n_udw_plot - plot_dw_data.n_udw_ann\n\n\n\n\nText(0.5, 1.0, 'Conserved forests')\n\n\n\n\n\nTypically more fallen deadwood is left unannotated if the canopy cover is high.\n\n\nText(0.5, 1.0, 'Conserved forests')\n\n\n\n\n\nFor standing deadwood the same effect is not present, at least it can’t be seen as clearly.\n\n\nCode\nplot_dw_data['l_dw_plot_diff'] = plot_dw_data.l_ddw - plot_dw_data.l_gw_ann\n\n\nRelation between canopy cover percentage and total lengths of groundwood in field plots.\n\n\n\n\n\nEstimate within-plot volume of downed deadwood from annotations.\n\n\nCode\nanns_in_plots['v_ddw'] = anns_in_plots.geometry.apply(cut_cone_volume)\nplot_dw_data['v_ddw_ann'] = plot_dw_data.apply(lambda row: anns_in_plots[(anns_in_plots.plot_id == row.id) &\n                                                                        (anns_in_plots.layer == 'groundwood')\n                                                                       ].v_ddw.sum()\n                                              , axis=1)\nplot_dw_data['v_dw_plot'] = (plot_dw_data['v_dw']/10000)*np.pi*9**2\nplot_dw_data['v_ddw_plot'] = (plot_dw_data['v_ddw']/10000)*np.pi*9**2\nplot_dw_data['v_udw_plot'] = plot_dw_data.v_dw_plot - plot_dw_data.v_ddw_plot\n\n\n\n\n\n\n\nCompare statistics for the total volume of fallen deadwood. First field data.\n\n\nCode\npd.pivot_table(data=plot_dw_data, index=['conservation'], values=['v_ddw'],\n               aggfunc=['min', 'max', 'mean', 'median','std', 'count'], margins=True)\n\n\n\n\n\n\n  \n    \n      \n      min\n      max\n      mean\n      median\n      std\n      count\n    \n    \n      \n      v_ddw\n      v_ddw\n      v_ddw\n      v_ddw\n      v_ddw\n      v_ddw\n    \n    \n      conservation\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      0.0\n      97.300652\n      22.922567\n      14.343594\n      26.380391\n      51\n    \n    \n      1\n      0.0\n      139.171786\n      42.482418\n      28.313861\n      42.481488\n      20\n    \n    \n      All\n      0.0\n      139.171786\n      28.432385\n      15.575737\n      32.641219\n      71\n    \n  \n\n\n\n\nThen estimation based on annotations.\n\n\nCode\npd.pivot_table(data=plot_dw_data, index=['conservation'], values=['v_ddw_ann_ha'],\n               aggfunc=['min', 'max', 'mean', 'median','std', 'count'], margins=True)\n\n\n\n\n\n\n  \n    \n      \n      min\n      max\n      mean\n      median\n      std\n      count\n    \n    \n      \n      v_ddw_ann_ha\n      v_ddw_ann_ha\n      v_ddw_ann_ha\n      v_ddw_ann_ha\n      v_ddw_ann_ha\n      v_ddw_ann_ha\n    \n    \n      conservation\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      0.0\n      64.201482\n      11.277266\n      6.746180\n      13.582354\n      51\n    \n    \n      1\n      0.0\n      43.360494\n      9.047403\n      5.115679\n      11.749208\n      20\n    \n    \n      All\n      0.0\n      64.201482\n      10.649135\n      5.661624\n      13.048417\n      71\n    \n  \n\n\n\n\nPlot the difference between volume based on field data and estimated volume based on annotations.\n\n\n<seaborn.axisgrid.FacetGrid>\n\n\n\n\n\nAgain, denser canopy means generally more difference."
  },
  {
    "objectID": "2_comparing_annotations_and_field_data.html#sudenpesänkangas",
    "href": "2_comparing_annotations_and_field_data.html#sudenpesänkangas",
    "title": "Comparison of plot characteristics and digitized deadwood",
    "section": "2 Sudenpesänkangas",
    "text": "2 Sudenpesänkangas\nRead data and do some wrangling.\n\n\nCode\nevo_fd_path = Path('../data/sudenpesankangas/')\nevo_anns = gpd.read_file('../../data/raw/sudenpesankangas/virtual_plots/sudenpesankangas_deadwood.geojson')\nevo_anns = evo_anns.to_crs('epsg:3067')\nevo_plot_circles = gpd.read_file(evo_fd_path/'plot_circles.geojson')\nevo_plot_circles['id'] = evo_plot_circles['id'].astype(int)\n\nevo_field_data = pd.read_csv(evo_fd_path/'puutiedot_sudenpesänkangas.csv', sep=';', decimal=',')\nevo_field_data = gpd.GeoDataFrame(evo_field_data, geometry=gpd.points_from_xy(evo_field_data.gx, evo_field_data.gy), \n                                   crs='epsg:3067')\nevo_grid = gpd.read_file(evo_fd_path/'vplots.geojson')\nevo_grid = evo_grid.to_crs('epsg:3067')\nevo_field_data['plotid'] = evo_field_data.kaid + 1000\nevo_field_data = evo_field_data[evo_field_data.puuluo.isin([3,4])]\n\ncons_evo = conservation_areas[conservation_areas.geometry.intersects(box(*evo_anns.total_bounds))]\ncons_evo = gpd.clip(cons_evo, evo_grid)\n\ndef match_plotid_spk(geom, plots):\n    for r in plots.itertuples():\n        if r.geometry.contains(geom):\n            return r.id\n    return None\n\nevo_plots_updated = pd.read_csv(evo_fd_path/'Koealatunnukset_Evo_2018.txt', sep=' ')\nevo_plots_luke = pd.read_csv(evo_fd_path/'Koealatunnukset_Evo_2018_LUKE.txt', sep=' ')\nevo_plots_updated = gpd.GeoDataFrame(evo_plots_updated, geometry=gpd.points_from_xy(evo_plots_updated.x,\n                                                                                    evo_plots_updated.y),\n                                     crs='epsg:3067')\nevo_plots_luke = gpd.GeoDataFrame(evo_plots_luke, geometry=gpd.points_from_xy(evo_plots_luke.x,\n                                                                              evo_plots_luke.y),\n                                     crs='epsg:3067')\nevo_plots = pd.concat([evo_plots_updated, evo_plots_luke])\nevo_plots.rename(columns= {c: c.replace('.','_') for c in evo_plots.columns}, inplace=True)\n\nevo_plots['spk_id'] = evo_plots.geometry.apply(lambda row: match_plotid_spk(row, evo_plot_circles))\n\nevo_plots.dropna(subset='spk_id', inplace=True)\n\nevo_plots['geometry'] = evo_plots.spk_id.apply(lambda row: evo_plot_circles[evo_plot_circles.id == row].geometry.iloc[0])\n\nevo_plots.drop(columns=['id'], inplace=True)\nevo_plots.rename(columns={'spk_id': 'id'}, inplace=True)\n\nevo_plots['conservation'] = evo_plots.geometry.apply(lambda row: 1 if cons_evo.geometry.unary_union.intersects(row)\n                                                     else 0)\n\nevo_anns['plot_id'] = evo_anns.apply(lambda row: int(row.vplot_id.split('_')[1]), axis=1)\n\nevo_plots = evo_plots[evo_plots.id.isin(evo_anns.plot_id.unique())]\n\nevo_anns['plot_id'] = evo_anns.apply(lambda row: match_circular_plot(row, evo_plots), axis=1)\nevo_anns = evo_anns.overlay(evo_plots[['geometry']])\n\n\nAdd canopy density based on LiDAR derived canopy height model. The density is the percentage of field plot area with height above 2 meters.\n\n\nCode\npcts = []\n\nwith rio.open('../../data/raw/sudenpesankangas/full_mosaics/sudenpesankangas_chm.tif') as src:\n    for row in evo_plots.itertuples():\n        plot_im, plot_tfm = rio_mask.mask(src, [row.geometry], crop=True)\n        pcts.append(plot_im[plot_im > 2].shape[0] / plot_im[plot_im >= 0].shape[0])\n        \nevo_plots['canopy_cover_pct'] = pcts\n\npd.pivot_table(data=evo_plots, index=['conservation'], values=['canopy_cover_pct'],\n               aggfunc=['min', 'max', 'mean', 'std', 'count'], margins=True)\n\n\n\n\n\n\n  \n    \n      \n      min\n      max\n      mean\n      std\n      count\n    \n    \n      \n      canopy_cover_pct\n      canopy_cover_pct\n      canopy_cover_pct\n      canopy_cover_pct\n      canopy_cover_pct\n    \n    \n      conservation\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      0.237288\n      0.989960\n      0.782452\n      0.200439\n      42\n    \n    \n      1\n      0.678000\n      0.991992\n      0.869878\n      0.085901\n      29\n    \n    \n      All\n      0.237288\n      0.991992\n      0.818162\n      0.168393\n      71\n    \n  \n\n\n\n\nAs seen here, canopies in Evo are far more dense than in Hiidenportti.\nCount the number of deadwood instances similarly as for Hiidenportti data and plot the relationship between them.\n\n\nCode\nevo_plots['n_dw_ann'] = evo_plots.apply(lambda row: evo_anns.plot_id.value_counts()[int(row.id)]\n                                        if row.id in evo_anns.plot_id.unique() else 0, axis=1)\nevo_plots['n_ddw_ann'] = evo_plots.apply(lambda row: evo_anns[evo_anns.label=='groundwood'].plot_id.value_counts()[int(row.id)]\n                                        if row.id in evo_anns[evo_anns.label=='groundwood'].plot_id.unique() else 0, axis=1)\nevo_plots['n_udw_ann'] = evo_plots.apply(lambda row: evo_anns[evo_anns.label!='groundwood'].plot_id.value_counts()[int(row.id)]\n                                        if row.id in evo_anns[evo_anns.label!='groundwood'].plot_id.unique() else 0, axis=1)\n\nevo_plots['n_dw_plot'] = np.round((evo_plots['n_dw']/10000)*np.pi*9**2).astype(int)\nevo_plots['n_ddw_plot'] = np.round((evo_plots['n_ddw']/10000)*np.pi*9**2).astype(int)\nevo_plots['n_udw_plot'] = evo_plots.n_dw_plot - evo_plots.n_ddw_plot\nevo_plots['conservation'] = evo_plots.apply(lambda row: 1 if any(cons_evo.geometry.contains(row.geometry))\n                                                        else 0, axis=1)\n\nevo_plots.pivot_table(index='conservation', values=['n_ddw_plot', 'n_udw_plot', 'n_ddw_ann', 'n_udw_ann'], \n                         aggfunc='sum', margins=True)\n\n\n\n\n\n\n  \n    \n      \n      n_ddw_ann\n      n_ddw_plot\n      n_udw_ann\n      n_udw_plot\n    \n    \n      conservation\n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      65\n      14\n      14\n      68\n    \n    \n      1\n      22\n      29\n      29\n      92\n    \n    \n      All\n      87\n      43\n      43\n      160\n    \n  \n\n\n\n\nFor Evo dataset, we were able to annotate twice as much groundwood as there are in total in the field data, and only a quarter of the number of standing deadwood. Because Evo dataset is missing the field-measured information for deadwood height, we were not able to check whether the missing standing deadwood were, for instance, only a couple of meters tall and thus hidden below the canopy. Compared to Hiidenportti, the forest canopy in Evo was significantly thicker, as the average canopy cover percentage in the field plots was around 82%.\n\n\n\n\n\nPlot the plot-wise differences between field-measurements and annotations.\n\n\nCode\ng = sns.lmplot(data=evo_plots, x='n_udw_plot', y='canopy_cover_pct', col='conservation',\n               hue='conservation', ci=0)\ng.set_ylabels('Canopy cover percentage in circular plot')\ng.set_xlabels('Field-measured standing deadwood')\nplt.show()\n\n\n\n\n\n\n\nCode\ng = sns.lmplot(data=evo_plots, x='n_udw_ann', y='canopy_cover_pct', col='conservation',\n               hue='conservation', ci=0)\ng.set_ylabels('Canopy cover percentage in circular plot')\ng.set_xlabels('Number of annotated standing deadwood')\nplt.show()\n\n\n\n\n\n\n\nCode\ng = sns.lmplot(data=evo_plots, x='n_ddw_plot', y='canopy_cover_pct', col='conservation',\n               hue='conservation', ci=0)\ng.set_ylabels('Canopy cover percentage in circular plot')\ng.set_xlabels('Number of field-measured fallen deadwood')\nplt.show()\n\n\n\n\n\n\n\nCode\ng = sns.lmplot(data=evo_plots, x='n_ddw_ann', y='canopy_cover_pct', col='conservation',\n               hue='conservation', ci=0)\ng.set_ylabels('Canopy cover percentage in circular plot')\ng.set_xlabels('Number of annotated fallen deadwood')\nplt.show()\n\n\n\n\n\nAgain, denser canopy indicates less annotated fallen data.\nSame plots but for differences in numbers.\n\n\nCode\nevo_plots['n_ddw_diff'] = evo_plots.n_ddw_plot - evo_plots.n_ddw_ann\nevo_plots['n_udw_diff'] = evo_plots.n_udw_plot - evo_plots.n_udw_ann\n\n\nFirst for fallen deadwood.\n\n\n<seaborn.axisgrid.FacetGrid>\n\n\n\n\n\nThen for standing deadwood.\n\n\n<seaborn.axisgrid.FacetGrid>\n\n\n\n\n\nSee whether there is any realotion between the number of annotated fallen deadwood an canopy cover.\n\n\n<seaborn.axisgrid.FacetGrid>\n\n\n\n\n\nOr the difference between the number of annotations (positive means more field data than annotated data) and canopy cover.\n\n\nCode\nevo_plots['n_dw_plot_diff'] = evo_plots.n_dw_plot - evo_plots.n_dw_ann\n\n\n\n\n<seaborn.axisgrid.FacetGrid>\n\n\n\n\n\nAgain, denser canopy indicates less annotated groundwood, and here also deadwood in general. However, Evo has such a low number of fallen deadwood that evaluating that is difficult.\nAs Evo data doesn’t have field-measured deadwood lengths, we can’t plot that relationship. We can, however, plot the DBH distributions, even though Evo dataset only has 43 downed deadwood with dbh measured.\n\n\nCode\nevo_field_data = evo_field_data[evo_field_data.plotid.isin(evo_plots.id.unique())]\nevo_field_data['conservation'] = evo_field_data.apply(lambda row: evo_plots[evo_plots.id == row.plotid].conservation.unique()[0], axis=1)\n\n\n\n\n\n\n\n\n\n\n\n\nSome statistics for field data.\n\n\nCode\npd.pivot_table(data=evo_field_data[(evo_field_data.puuluo == 4)&(evo_field_data.lapimitta_mm>0)],\n               index=['conservation'], values=['est_pituus_m', 'lapimitta_mm'],\n               aggfunc=['mean', 'min', 'max', 'count'], margins=True)\n\n\n\n\n\n\n  \n    \n      \n      mean\n      min\n      max\n      count\n    \n    \n      \n      est_pituus_m\n      lapimitta_mm\n      est_pituus_m\n      lapimitta_mm\n      est_pituus_m\n      lapimitta_mm\n      est_pituus_m\n      lapimitta_mm\n    \n    \n      conservation\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      13.609571\n      169.357143\n      5.934\n      55\n      34.211\n      552\n      14\n      14\n    \n    \n      1\n      11.905690\n      115.896552\n      5.446\n      45\n      28.101\n      445\n      29\n      29\n    \n    \n      All\n      12.460442\n      133.302326\n      5.446\n      45\n      34.211\n      552\n      43\n      43\n    \n  \n\n\n\n\nSame for annotated data.\n\n\nCode\nevo_anns['tree_length'] = evo_anns.geometry.apply(get_len)\npd.pivot_table(data=evo_anns[evo_anns.label=='groundwood'], index=['conservation'], values=['tree_length', 'diam'],\n               aggfunc=['mean', 'min', 'max', 'count'], margins=True)\n\n\n\n\n\n\n  \n    \n      \n      mean\n      min\n      max\n      count\n    \n    \n      \n      diam\n      tree_length\n      diam\n      tree_length\n      diam\n      tree_length\n      diam\n      tree_length\n    \n    \n      conservation\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      200.620326\n      2.520178\n      44.682490\n      0.251763\n      760.781151\n      9.225878\n      63\n      63\n    \n    \n      1\n      237.091999\n      2.592576\n      139.506761\n      0.535882\n      612.397653\n      6.664814\n      24\n      24\n    \n    \n      All\n      210.681477\n      2.540150\n      44.682490\n      0.251763\n      760.781151\n      9.225878\n      87\n      87\n    \n  \n\n\n\n\nVolume estimations for Evo data are really inaccurate, as most field data measurements are zero.\n\n\nCode\nevo_anns['v_ddw'] = evo_anns.geometry.apply(cut_cone_volume)\nevo_plots['v_ddw_ann'] = evo_plots.apply(lambda row: evo_anns[(evo_anns.plot_id == row.id) &\n                                                              (evo_anns.label == 'groundwood')\n                                                              ].v_ddw.sum()\n                                              , axis=1)\n\n\n\n\n\n\n\nStatistics for volume based on annotations.\n\n\nCode\npd.pivot_table(data=evo_plots, index=['conservation'], values=['v_ddw'],\n               aggfunc=['min', 'max', 'mean', 'median','std', 'count'], margins=True)\n\n\n\n\n\n\n  \n    \n      \n      min\n      max\n      mean\n      median\n      std\n      count\n    \n    \n      \n      v_ddw\n      v_ddw\n      v_ddw\n      v_ddw\n      v_ddw\n      v_ddw\n    \n    \n      conservation\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      0.0\n      123.004587\n      5.522673\n      0.0\n      23.497052\n      42\n    \n    \n      1\n      0.0\n      98.258093\n      6.338610\n      0.0\n      18.873961\n      29\n    \n    \n      All\n      0.0\n      123.004587\n      5.855943\n      0.0\n      21.587804\n      71\n    \n  \n\n\n\n\n\n\nCode\npd.pivot_table(data=evo_plots, index=['conservation'], values=['v_ddw_ann_ha'],\n               aggfunc=['min', 'max', 'mean', 'median','std', 'count'], margins=True)\n\n\n\n\n\n\n  \n    \n      \n      min\n      max\n      mean\n      median\n      std\n      count\n    \n    \n      \n      v_ddw_ann_ha\n      v_ddw_ann_ha\n      v_ddw_ann_ha\n      v_ddw_ann_ha\n      v_ddw_ann_ha\n      v_ddw_ann_ha\n    \n    \n      conservation\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      0.0\n      96.060293\n      7.991108\n      0.0\n      19.252499\n      42\n    \n    \n      1\n      0.0\n      71.154407\n      5.065659\n      0.0\n      13.517535\n      29\n    \n    \n      All\n      0.0\n      96.060293\n      6.796206\n      0.0\n      17.096402\n      71"
  },
  {
    "objectID": "3_mask_rcnn_model_training.html",
    "href": "3_mask_rcnn_model_training.html",
    "title": "Mask R-CNN model training",
    "section": "",
    "text": "Code\nimport detectron2\nfrom drone_detector.imports import *\n\nfrom detectron2 import model_zoo\nfrom detectron2.engine import DefaultPredictor\nfrom detectron2.config import get_cfg\nfrom detectron2.utils.visualizer import Visualizer\nfrom detectron2.data import MetadataCatalog, DatasetCatalog, build_detection_test_loader, build_detection_train_loader, DatasetMapper\n\nfrom detectron2.data import transforms as T\nfrom detectron2.data import detection_utils as utils\nimport torch\n\nfrom detectron2.data.datasets import register_coco_instances\nfrom detectron2.engine import DefaultTrainer\n\nfrom detectron2.evaluation import COCOEvaluator, DatasetEvaluators\nimport cv2\nimport matplotlib.pyplot as plt\nimport wandb\n\nfrom drone_detector.engines.detectron2.augmentations import *\n\n\n\n1 Model training\nWe used Mask R-CNN with a several pretrained backbones as our model, and fine-tuned the model with our remote sensing data. Because the convolutional layers of a CNN model extract interesting, useful features from the images, it is possible and advisable to use pretrained weights as a baseline and fine-tune the model with custom data. All models were trained for 3000 iterations with a batch size of 8, and validation metrics were recorded every 100 iterations. We used a base learning rate of 0.001 and linear warmup with cosine annealing as the learning rate scheduler, using 1000 iterations for the warmup phase.\n\n\nCode\noutpath = Path('../../data/processed/hiidenportti/')\nregister_coco_instances('hiidenportti_train', {}, outpath/'hiidenportti_train.json', outpath/'train_512')\nregister_coco_instances('hiidenportti_val', {}, outpath/'hiidenportti_valid.json', outpath/'valid_512')\n\n\n\n\nCode\ncfg = get_cfg()\ncfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\ncfg.DATASETS.TRAIN = (\"hiidenportti_train\",)\ncfg.DATASETS.TEST = (\"hiidenportti_val\",)\ncfg.DATALOADER.NUM_WORKERS = 4\ncfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")  # Let training initialize from model zoo\ncfg.SOLVER.IMS_PER_BATCH = 8\ncfg.TEST.EVAL_PERIOD = 100\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\ncfg.OUTPUT_DIR = 'detectron2_models/mask_rcnn_R_50_FPN_3x_256'\n\ncfg.SOLVER.LR_SCHEDULER_NAME = \"WarmupCosineLR\"\ncfg.SOLVER.BASE_LR = 0.001  # pick a good LR\ncfg.SOLVER.WARMUP_ITERS = 1000\ncfg.SOLVER.MAX_ITER = 3000 \ncfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512   # (default: 512)\ncfg.MODEL.ROI_HEADS.NUM_CLASSES = 2\n\nos.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n\n\n\n\nCode\nclass Trainer(DefaultTrainer):\n    \"\"\"\n    Trainer class for training detectron2 models\n    \"\"\"\n    \n    def __init__(self, cfg):\n        super().__init__(cfg)\n    \n    @classmethod\n    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n        return DatasetEvaluators([COCOEvaluator(dataset_name, output_dir=output_folder)])\n    \n    @classmethod\n    def build_train_loader(cls, cfg):\n        augs = build_aug_transforms(cfg,\n                                    flip_horiz=True,\n                                    flip_vert=True,\n                                    max_rotate=10,\n                                    brightness_limits=(.8,1.4),\n                                    contrast_limits=(.8,1.4),\n                                    saturation_limits=(.8,1.4),\n                                    p_lighting=0.75)\n        return build_detection_train_loader(cfg, mapper=DatasetMapper(cfg, is_train=True, augmentations=augs))\n\n\nIn order to effectively increase the amount of our training data, we applied a set of augmentations to our image patches and masks. First, each image was randomly flipped and randomly rotated up to 90 degrees. These geometric transformations were applied to both masks and image patches. In addition, brightness and contrast of the image were randomly adjusted and images had a chance to be slightly blurred. Each of these individual augmentations had a probability of 0.5 to be applied.\n\n\nCode\nfig, axs = plt.subplots(4,2, figsize=(8,16), dpi=150)\nmetadata = MetadataCatalog.get(cfg.DATASETS.TRAIN[0])\ndls = iter(build_detection_train_loader(cfg, mapper=DatasetMapper(cfg, is_train=True, \n                                                                  augmentations=build_aug_transforms(cfg))))\nbatch = next(dls)\nfor d, ax in zip(batch, axs.flatten()):\n    ax.set_xticks([])\n    ax.set_yticks([])\n    img = d['image'].permute(1,2,0).cpu().detach().numpy()\n    img = utils.convert_image_to_rgb(img, cfg.INPUT.FORMAT)\n    visualizer = Visualizer(img, metadata=metadata, scale=0.5)\n    target_fields = d[\"instances\"].get_fields()\n    labels = [metadata.thing_classes[i] for i in target_fields[\"gt_classes\"]]\n    vis = visualizer.overlay_instances(\n        labels=labels,\n        boxes=target_fields.get(\"gt_boxes\", None),\n        masks=target_fields.get(\"gt_masks\", None),\n        keypoints=target_fields.get(\"gt_keypoints\", None),\n    )\n    ax.imshow(vis.get_image())\n\n\n\n\n\nAll models were trained with Python version 3.9.5 using deep learning stack containing version PyTorch 1.10.1 and Detectron2 detection and segmentation library. Weights & Biases was used to track the model metrics. We used a single NVIDIA V100 GPU with 32GB of memory, hosted on computing nodes of Puhti supercomputer hosted by CSC – IT Center for Science, Finland.\nThe following is for the example, all models were trained as batch jobs. First initialize wandb login.\n\n\nCode\nimport yaml\nwandb.login()\ncfg_wandb = yaml.safe_load(cfg.dump())\nwandb.init(project='hiidenportti-deadwood', \n           name='test',\n           sync_tensorboard=True, \n           config=cfg_wandb)\n\n\nThen make the Trainer.\n\n\nCode\ntrainer = Trainer(cfg) \ntrainer.resume_or_load(resume=False)\n\n\nTrain the model.\n\n\nCode\ntrainer.train()\n\n\nAnd finish wandb.\n\n\nCode\nwith open(os.path.join(cfg.OUTPUT_DIR, 'config.yaml'), 'w') as f:\n    f.write(cfg.dump())\n    \nwandb.finish()"
  },
  {
    "objectID": "4_patch_level_results.html",
    "href": "4_patch_level_results.html",
    "title": "Patch level results",
    "section": "",
    "text": "Code\nfrom drone_detector.imports import *\nfrom drone_detector.engines.detectron2.tta import *\nfrom detectron2.engine import DefaultPredictor\nfrom detectron2.config import get_cfg\nfrom detectron2.utils.visualizer import Visualizer\nfrom detectron2.data.datasets import register_coco_instances\nfrom detectron2.data import build_detection_test_loader, DatasetCatalog, MetadataCatalog\nfrom detectron2.evaluation  import COCOEvaluator, DatasetEvaluators, inference_on_dataset\nimport cv2\nimport matplotlib.pyplot as plt\nimport torch\nFirst, see the results for patch level data. Patch level data means 512x512 pixel image patches, while virtual plot level data means larger scenes."
  },
  {
    "objectID": "4_patch_level_results.html#image-patches",
    "href": "4_patch_level_results.html#image-patches",
    "title": "Patch level results",
    "section": "2.1 Image patches",
    "text": "2.1 Image patches\n\n\nCode\nregister_coco_instances('hiidenportti_test', {}, datapath/'hiidenportti_test.json', datapath/'test_512')\n\n\n\n\nCode\nevaluator = COCOEvaluator('hiidenportti_test', output_dir=cfg.OUTPUT_DIR)\nval_loader = build_detection_test_loader(cfg, \"hiidenportti_test\")\nprint(inference_on_dataset(predictor.model, val_loader, evaluator))\n\n\nLoading and preparing results...\nDONE (t=0.00s)\ncreating index...\nindex created!\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.332\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.621\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.313\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.319\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.243\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.173\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.418\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.437\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.369\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.426\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\nLoading and preparing results...\nDONE (t=0.01s)\ncreating index...\nindex created!\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.333\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.658\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.318\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.258\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.351\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.162\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.407\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.426\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.365\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.407\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\nOrderedDict([('bbox', {'AP': 33.19666685364819, 'AP50': 62.05764557202501, 'AP75': 31.342614304169953, 'APs': 31.880641144927264, 'APm': 24.332151808777574, 'APl': nan, 'AP-uprightwood': 36.60906045016792, 'AP-groundwood': 29.784273257128458}), ('segm', {'AP': 33.32628165118522, 'AP50': 65.83497640740984, 'AP75': 31.756091860397973, 'APs': 25.756281121466383, 'APm': 35.122128446886805, 'APl': nan, 'AP-uprightwood': 36.999960175091246, 'AP-groundwood': 29.652603127279193})])\n\n\nTest set results are slightly better than validation set results.\n\n\n\n\n\nThen check the effect of TTA for this set.\n\n\nCode\nevaluator = COCOEvaluator(\"hiidenportti_test\", output_dir=cfg.OUTPUT_DIR, max_dets_per_image=100, use_fast_impl=False)\nprint(inference_on_dataset(tta_predictor.model, val_loader, evaluator))\n\n\nLoading and preparing results...\nDONE (t=0.00s)\ncreating index...\nindex created!\nRunning per image evaluation...\nEvaluate annotation type *bbox*\nDONE (t=0.57s).\nAccumulating evaluation results...\nDONE (t=0.06s).\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.359\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.674\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.345\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.353\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.272\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.180\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.456\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.491\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.439\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.536\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\nLoading and preparing results...\nDONE (t=0.02s)\ncreating index...\nindex created!\nRunning per image evaluation...\nEvaluate annotation type *segm*\nDONE (t=0.64s).\nAccumulating evaluation results...\nDONE (t=0.06s).\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.366\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.704\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.362\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.285\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.386\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.178\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.449\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.479\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.430\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.454\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\nOrderedDict([('bbox', {'AP': 35.944294534953336, 'AP50': 67.41454378120778, 'AP75': 34.45688601547454, 'APs': 35.341279426398316, 'APm': 27.230624878830568, 'APl': nan, 'AP-uprightwood': 38.583296052344416, 'AP-groundwood': 33.30529301756224}), ('segm', {'AP': 36.61569479126223, 'AP50': 70.40829745412609, 'AP75': 36.19336278436601, 'APs': 28.51581402750356, 'APm': 38.62714646160273, 'APl': nan, 'AP-uprightwood': 40.63335622143643, 'AP-groundwood': 32.598033361088035})])\n\n\nAgain, AP50-score increases when running TTA."
  },
  {
    "objectID": "5_scene_level_results.html",
    "href": "5_scene_level_results.html",
    "title": "Scene level results",
    "section": "",
    "text": "Code\nfrom drone_detector.utils import *\nfrom drone_detector.imports import *\nimport os\nfrom drone_detector.metrics import *\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nsys.path.append('..')\nfrom src.postproc_functions import *\nfrom tqdm.auto import tqdm\ntqdm.pandas()\nAs patch-level data and results are not really useful for our purposes, here we run the predictions for larger scenes. Each plot is tiled to 512x512px patches, possibly with 256px overlap and afterwards the predictions are collated and optionally cleaned so that the amount of overlapping predictions is lower."
  },
  {
    "objectID": "5_scene_level_results.html#no-overlap-no-post-processing",
    "href": "5_scene_level_results.html#no-overlap-no-post-processing",
    "title": "Scene level results",
    "section": "1.1 No overlap, no post-processing",
    "text": "1.1 No overlap, no post-processing\n\n\nCode\nfrom drone_detector.engines.detectron2.predict import predict_instance_masks\nraw_path = Path('../../data/raw/hiidenportti/virtual_plots/buffered_test/images')\ntest_rasters = [raw_path/f for f in os.listdir(raw_path) if f.endswith('tif')]\n\n\nTemplate folder has the following structure:\ntemplate_folder\n|-predicted_vectors\n|-raster_tiles\n|-vector_tiles\n|-raw_preds\nWhere raster_tiles and vector_tiles are symbolic links pointing to corresponding data directories, and predicted_vectors and raw_preds are empty folders for predictions.\n\n\nCode\npred_outpath = Path('../results/hp_unprocessed_new/')\nif not os.path.exists(pred_outpath):\n    shutil.copytree('../results/template_folder/', pred_outpath, symlinks=True)\n\n\n\n\nCode\nfor t in test_rasters:\n    outfile_name = pred_outpath/f'raw_preds/{str(t).split(\"/\")[-1][:-4]}.geojson'\n    predict_instance_masks(path_to_model_files='../models/hiidenportti/mask_rcnn_R_101_FPN_3x/', \n                           path_to_image=str(t),\n                           outfile=str(outfile_name),\n                           processing_dir='temp',\n                           tile_size=512,\n                           tile_overlap=0,\n                           smooth_preds=False,\n                           use_tta=True,\n                           coco_set='../../data/processed/hiidenportti/hiidenportti_valid.json',\n                           postproc_results=False)\n\n\n\n\nCode\nraw_res_path = pred_outpath\ntruth_shps = sorted([raw_res_path/'vector_tiles'/f for f in os.listdir(raw_res_path/'vector_tiles')])\nraw_shps = sorted([raw_res_path/'raw_preds'/f for f in os.listdir(raw_res_path/'raw_preds')])\nrasters = sorted([raw_res_path/'raster_tiles'/f for f in os.listdir(raw_res_path/'raster_tiles')])\n\n\n“Raw” predictions are modified as such: 1. Invalid polygons are fixed to be valid polygons. MultiPolygon masks are replaced with the largest single polygon of the multipoly. 2. Extent is clipped to be same as the corresponding ground truth data 3. Label numbering is adjusted 4. Polygons with area less than 16² pixels are discarded\n\n\nCode\nfor p, t in zip(raw_shps, truth_shps):\n    temp_pred = gpd.read_file(p)\n    temp_truth = gpd.read_file(t)\n    temp_pred = gpd.clip(temp_pred, box(*temp_truth.total_bounds))\n\n    temp_pred['geometry'] = temp_pred.apply(lambda row: fix_multipolys(row.geometry) \n                                            if row.geometry.type == 'MultiPolygon' \n                                            else shapely.geometry.Polygon(row.geometry.exterior), axis=1)\n    temp_pred['label'] += 1\n    temp_pred = temp_pred[temp_pred.geometry.area > 16*0.04**2]\n    temp_pred.to_file(raw_res_path/'predicted_vectors'/p.name)\n\n\n\n\nCode\npred_shps = sorted([raw_res_path/'predicted_vectors'/f for f in os.listdir(raw_res_path/'predicted_vectors')])\n\n\nCollate predictions and annotations so that IoU and such is easy to compute.\n\n\nCode\ntruths = None\npreds = None\n\nfor p, t in zip(pred_shps, truth_shps):\n    temp_pred = gpd.read_file(p)\n    temp_truth = gpd.read_file(t)\n    if truths is None:\n        truths = temp_truth\n        preds = temp_pred\n    else:\n        truths = pd.concat((truths, temp_truth))\n        preds = pd.concat((preds, temp_pred))\n\n\nFix labeling.\n\n\nCode\npreds['layer'] = preds.apply(lambda row: 'groundwood' if row.label == 2 else 'uprightwood', axis=1)\n\n\nCheck the number of predictions. The models have found almost 1500 more deadwood instances at this point.\n\n\nCode\npreds.shape, truths.shape\n\n\n((3444, 4), (1741, 6))\n\n\n\n\nCode\npreds.label.value_counts()\n\n\n2    2790\n1     654\nName: label, dtype: int64\n\n\n\n\nCode\ndis_truths = truths.dissolve(by='layer')\ndis_preds = preds.dissolve(by='layer')\n\n\nCheck IoU-score.\n\n\nCode\npoly_IoU(dis_truths, dis_preds)\n\n\nlayer\ngroundwood     0.472870\nuprightwood    0.460234\ndtype: float64\n\n\nRun GisCOCOeval, which converts georeferenced vector files to COCO-annotations and runs the metrics.\n\n\nCode\ndeadwood_categories = [{'supercategory': 'deadwood', 'id':1, 'name':'uprightwood'},\n                 \n                       {'supercategory': 'deadwood', 'id':2, 'name':'groundwood'}]\n\nraw_coco_eval = GisCOCOeval(raw_res_path, raw_res_path, \n                            None, None, deadwood_categories)\n\n\n\n\nCode\nraw_coco_eval.prepare_data(gt_label_col='layer')\n\n\n\n\n\n\n\n\n\n\nCode\nraw_coco_eval.prepare_eval()\n\n\nloading annotations into memory...\nDone (t=0.03s)\ncreating index...\nindex created!\nLoading and preparing results...\nDONE (t=0.03s)\ncreating index...\nindex created!\n\n\nAs the scenes can contain more than 1000 annotations, set maxDets to larger values than default.\n\n\nCode\nraw_coco_eval.coco_eval.params.maxDets = [1000, 10000]\n\n\n\n\nCode\nraw_coco_eval.evaluate()\n\n\n\nEvaluating for category uprightwood\nRunning per image evaluation...\nEvaluate annotation type *segm*\nDONE (t=0.81s).\nAccumulating evaluation results...\nDONE (t=0.01s).\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=10000 ] = 0.348\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=10000 ] = 0.677\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=10000 ] = 0.325\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=10000 ] = 0.217\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=10000 ] = 0.396\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=10000 ] = 1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.466\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.387\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.497\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 1.000\n\nEvaluating for category groundwood\nRunning per image evaluation...\nEvaluate annotation type *segm*\nDONE (t=11.59s).\nAccumulating evaluation results...\nDONE (t=0.02s).\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=10000 ] = 0.202\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=10000 ] = 0.525\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=10000 ] = 0.104\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=10000 ] = 0.204\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=10000 ] = 0.160\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=10000 ] = -1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.342\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.347\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.200\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = -1.000\n\nEvaluating for full data...\nRunning per image evaluation...\nEvaluate annotation type *segm*\nDONE (t=12.39s).\nAccumulating evaluation results...\nDONE (t=0.03s).\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=10000 ] = 0.275\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=10000 ] = 0.601\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=10000 ] = 0.214\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=10000 ] = 0.210\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=10000 ] = 0.278\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=10000 ] = 1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.404\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.367\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.349\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 1.000\n\n\nCompared to the patch-level results, the AP50 score is around 0.2 lower than patch-level results. However, edges are not handled in any way in this processing level.\nGet the number of false positives (FP), true positives (TP) and false negatives (FN). Object detection has infinite number of true negatives so we are not interested in them.\n\n\nCode\nfp_cols = [f'FP_{np.round(i, 2)}' for i in np.arange(0.5, 1.04, 0.05)]\ntp_cols = [f'TP_{np.round(i, 2)}' for i in np.arange(0.5, 1.03, 0.05)]\ntp_truths = truths.copy()\ntp_truths.rename(columns={'groundwood':'label'}, inplace=True)\ntruth_sindex = tp_truths.sindex\nfp_preds = preds.copy()\npred_sindex = fp_preds.sindex\ntp_truths[tp_cols] = tp_truths.progress_apply(lambda row: is_true_positive(row, fp_preds, pred_sindex), \n                                              axis=1, result_type='expand')\nfp_preds[fp_cols] = fp_preds.progress_apply(lambda row: is_false_positive(row, tp_truths, truth_sindex,\n                                                                            fp_preds, pred_sindex),\n                                            axis=1, result_type='expand')\n\n\n\n\n\n\n\n\n\n\nCode\npd.crosstab(fp_preds.layer, fp_preds['FP_0.5'], margins=True)\n\n\n\n\n\n\n  \n    \n      FP_0.5\n      FP\n      TP\n      All\n    \n    \n      layer\n      \n      \n      \n    \n  \n  \n    \n      groundwood\n      1837\n      953\n      2790\n    \n    \n      uprightwood\n      382\n      272\n      654\n    \n    \n      All\n      2219\n      1225\n      3444\n    \n  \n\n\n\n\n\n\nCode\npd.crosstab(tp_truths.layer, tp_truths['TP_0.5'], margins=True)\n\n\n\n\n\n\n  \n    \n      TP_0.5\n      FN\n      TP\n      All\n    \n    \n      layer\n      \n      \n      \n    \n  \n  \n    \n      groundwood\n      448\n      953\n      1401\n    \n    \n      uprightwood\n      68\n      272\n      340\n    \n    \n      All\n      516\n      1225\n      1741\n    \n  \n\n\n\n\nFrom these we can get both precision and recall: \\(Precision = \\frac{tp}{tp+fp}, Recall = \\frac{tp}{tp+fn}\\)\n\n\nCode\nprint(f'Precision for fallen deadwood with IoU threshold of 0.5 is {(953/2790):.2f}')\nprint(f'Recall for fallen deadwood with IoU threshold of 0.5 is {(953/1401):.2f}')\n\n\nPrecision for fallen deadwood with IoU threshold of 0.5 is 0.34\nRecall for fallen deadwood with IoU threshold of 0.5 is 0.68\n\n\n\n\nCode\nprint(f'Precision for standing deadwood with IoU threshold of 0.5 is {(272/654):.2f}')\nprint(f'Recall for standing deadwood with IoU threshold of 0.5 is {(212/340):.2f}')\n\n\nPrecision for standing deadwood with IoU threshold of 0.5 is 0.42\nRecall for standing deadwood with IoU threshold of 0.5 is 0.62\n\n\n\n\nCode\nprint(f'Overall precision with IoU threshold of 0.5 is {(1225/3444):.2f}')\nprint(f'Overall recall with IoU threshold of 0.5 is {(1225/1741):.2f}')\n\n\nOverall precision with IoU threshold of 0.5 is 0.36\nOverall recall with IoU threshold of 0.5 is 0.70"
  },
  {
    "objectID": "5_scene_level_results.html#half-patch-overlap-and-edge-filtering",
    "href": "5_scene_level_results.html#half-patch-overlap-and-edge-filtering",
    "title": "Scene level results",
    "section": "1.2 Half patch overlap and edge filtering",
    "text": "1.2 Half patch overlap and edge filtering\nFor this postprocessing method, mosaics are tiled so that the sliding window moves half tile lenght. For example, when moving row-wise, the first bottom-left coordinates are (0,0), and next ones (256,0), (512,0)… and same is done column-wise. We discard all predicted polygons whose centroid point is not within the half-overlap area. For instance, for first tile (bottom left (0,0)), the x-coordinate must be between 128 and 384, for second tile (256,0) between 384 and 640, and likewise for y-coordinates. This method discards almost 75% of all predictions in the scenes as they are either overlapping or cut in half in the patch borders.\nThe images used for predictions are buffered so that the whole area is covered, considering the discarding process.\n\n\nCode\npred_outpath = Path('../results/hp_overlap_filter_new/')\nif not os.path.exists(pred_outpath):\n    shutil.copytree('../results/template_folder/', pred_outpath, symlinks=True)\n\n\n\n\nCode\nraw_path = Path('../../data/raw/hiidenportti/virtual_plots/buffered_test/images')\ntest_rasters = [raw_path/f for f in os.listdir(raw_path) if f.endswith('tif')]\n\nfor t in test_rasters:\n    outfile_name = pred_outpath/f'raw_preds/{str(t).split(\"/\")[-1][:-4]}.geojson'\n    predict_instance_masks(path_to_model_files='../models/hiidenportti/mask_rcnn_R_101_FPN_3x/', \n                           path_to_image=str(t),\n                           outfile=str(outfile_name),\n                           processing_dir='temp',\n                           tile_size=512,\n                           tile_overlap=256,\n                           smooth_preds=False,\n                           use_tta=True,\n                           coco_set='../../data/processed/hiidenportti/hiidenportti_valid.json',\n                           postproc_results=True)\n\n\nModify as previously.\n\n\nCode\nhp_res_path = pred_outpath\ntruth_shps = sorted([hp_res_path/'vector_tiles'/f for f in os.listdir(hp_res_path/'vector_tiles')])\nhp_raw_shps = sorted([hp_res_path/'raw_preds'/f for f in os.listdir(hp_res_path/'raw_preds')])\nrasters = sorted([hp_res_path/'raster_tiles'/f for f in os.listdir(hp_res_path/'raster_tiles')])\n\n\n\n\nCode\nfor p, t in zip(hp_raw_shps, truth_shps):\n    temp_pred = gpd.read_file(p)\n    temp_truth = gpd.read_file(t)\n    temp_pred = gpd.clip(temp_pred, box(*temp_truth.total_bounds))\n\n    temp_pred['geometry'] = temp_pred.apply(lambda row: fix_multipolys(row.geometry) \n                                            if row.geometry.type == 'MultiPolygon' \n                                            else shapely.geometry.Polygon(row.geometry.exterior), axis=1)\n    temp_pred['label'] += 1\n    temp_pred = temp_pred[temp_pred.geometry.area > 16*0.04**2]\n    temp_pred.to_file(hp_res_path/'predicted_vectors'/p.name)\n\n\n\n\nCode\npred_shps = sorted([hp_res_path/'predicted_vectors'/f for f in os.listdir(hp_res_path/'predicted_vectors')])\n\n\nCollate all predictions into single dataframes\n\n\nCode\ntruths = None\npreds = None\n\nfor p, t in zip(pred_shps, truth_shps):\n    temp_pred = gpd.read_file(p)\n    temp_truth = gpd.read_file(t)\n    if truths is None:\n        truths = temp_truth\n        preds = temp_pred\n    else:\n        truths = pd.concat((truths, temp_truth))\n        preds = pd.concat((preds, temp_pred))\n\n\n\n\nCode\npreds['layer'] = preds.apply(lambda row: 'groundwood' if row.label == 2 else 'uprightwood', axis=1)\n\n\nThe total amount after cleaning is similar than before.\n\n\nCode\npreds.layer.value_counts()\n\n\ngroundwood     2698\nuprightwood     619\nName: layer, dtype: int64\n\n\n\n\nCode\ndis_truths = truths.dissolve(by='layer')\ndis_preds = preds.dissolve(by='layer')\n\n\nBut IoU, especially for standing deadwood increases.\n\n\nCode\npoly_IoU(dis_truths, dis_preds)\n\n\nlayer\ngroundwood     0.487089\nuprightwood    0.503599\ndtype: float64\n\n\n\n\nCode\ndeadwood_categories = [{'supercategory': 'deadwood', 'id':1, 'name':'uprightwood'},\n                 \n                       {'supercategory': 'deadwood', 'id':2, 'name':'groundwood'}]\n\nhp_coco_eval = GisCOCOeval(hp_res_path, hp_res_path, \n                            None, None, deadwood_categories)\n\n\n\n\nCode\nhp_coco_eval.prepare_data(gt_label_col='layer')\n\n\n\n\n\n\n\n\n\n\nCode\nhp_coco_eval.prepare_eval()\n\n\nloading annotations into memory...\nDone (t=0.03s)\ncreating index...\nindex created!\nLoading and preparing results...\nDONE (t=0.03s)\ncreating index...\nindex created!\n\n\n\n\nCode\nhp_coco_eval.coco_eval.params.maxDets = [1000, 10000]\n\n\n\n\nCode\nhp_coco_eval.evaluate()\n\n\n\nEvaluating for category uprightwood\nRunning per image evaluation...\nEvaluate annotation type *segm*\nDONE (t=0.76s).\nAccumulating evaluation results...\nDONE (t=0.01s).\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=10000 ] = 0.447\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=10000 ] = 0.780\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=10000 ] = 0.479\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=10000 ] = 0.296\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=10000 ] = 0.505\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=10000 ] = 1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.545\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.428\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.591\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 1.000\n\nEvaluating for category groundwood\nRunning per image evaluation...\nEvaluate annotation type *segm*\nDONE (t=12.06s).\nAccumulating evaluation results...\nDONE (t=0.02s).\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=10000 ] = 0.266\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=10000 ] = 0.652\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=10000 ] = 0.145\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=10000 ] = 0.269\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=10000 ] = 0.211\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=10000 ] = -1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.389\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.394\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.280\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = -1.000\n\nEvaluating for full data...\nRunning per image evaluation...\nEvaluate annotation type *segm*\nDONE (t=12.87s).\nAccumulating evaluation results...\nDONE (t=0.03s).\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=10000 ] = 0.356\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=10000 ] = 0.716\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=10000 ] = 0.312\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=10000 ] = 0.282\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=10000 ] = 0.358\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=10000 ] = 1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.467\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.411\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.436\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 1.000\n\n\nOverall AP50 increases by around 0.1 with this kind of post-processing.\n\n\nCode\ntp_truths = truths.copy()\ntp_truths.rename(columns={'groundwood':'label'}, inplace=True)\ntruth_sindex = tp_truths.sindex\nfp_preds = preds.copy()\npred_sindex = fp_preds.sindex\ntp_truths[tp_cols] = tp_truths.progress_apply(lambda row: is_true_positive(row, fp_preds, pred_sindex), \n                                              axis=1, result_type='expand')\nfp_preds[fp_cols] = fp_preds.progress_apply(lambda row: is_false_positive(row, tp_truths, truth_sindex,\n                                                                            fp_preds, pred_sindex),\n                                            axis=1, result_type='expand')\n\n\n\n\n\n\n\n\n\n\nCode\npd.crosstab(fp_preds.layer, fp_preds['FP_0.5'], margins=True)\n\n\n\n\n\n\n  \n    \n      FP_0.5\n      FP\n      TP\n      All\n    \n    \n      layer\n      \n      \n      \n    \n  \n  \n    \n      groundwood\n      1636\n      1062\n      2698\n    \n    \n      uprightwood\n      318\n      301\n      619\n    \n    \n      All\n      1954\n      1363\n      3317\n    \n  \n\n\n\n\n\n\nCode\npd.crosstab(tp_truths.layer, tp_truths['TP_0.5'], margins=True)\n\n\n\n\n\n\n  \n    \n      TP_0.5\n      FN\n      TP\n      All\n    \n    \n      layer\n      \n      \n      \n    \n  \n  \n    \n      groundwood\n      339\n      1062\n      1401\n    \n    \n      uprightwood\n      39\n      301\n      340\n    \n    \n      All\n      378\n      1363\n      1741\n    \n  \n\n\n\n\n\\(Precision = \\frac{tp}{tp+fp}, Recall = \\frac{tp}{tp+fn}\\)\n\n\nCode\nprint(f'Precision for fallen deadwood with IoU threshold of 0.5 is {(1062/2698):.2f}')\nprint(f'Recall for fallen deadwood with IoU threshold of 0.5 is {(1062/1401):.2f}')\n\n\nPrecision for fallen deadwood with IoU threshold of 0.5 is 0.39\nRecall for fallen deadwood with IoU threshold of 0.5 is 0.76\n\n\n\n\nCode\nprint(f'Precision for standing deadwood with IoU threshold of 0.5 is {(301/619):.2f}')\nprint(f'Recall for standing deadwood with IoU threshold of 0.5 is {(301/340):.2f}')\n\n\nPrecision for standing deadwood with IoU threshold of 0.5 is 0.49\nRecall for standing deadwood with IoU threshold of 0.5 is 0.89\n\n\n\n\nCode\nprint(f'Overall precision with IoU threshold of 0.5 is {(1363/3317):.2f}')\nprint(f'Overall recall with IoU threshold of 0.5 is {(1363/1741):.2f}')\n\n\nOverall precision with IoU threshold of 0.5 is 0.41\nOverall recall with IoU threshold of 0.5 is 0.78"
  },
  {
    "objectID": "5_scene_level_results.html#overlap-edge-filtering-and-mask-merging",
    "href": "5_scene_level_results.html#overlap-edge-filtering-and-mask-merging",
    "title": "Scene level results",
    "section": "1.3 Overlap, edge filtering and mask merging",
    "text": "1.3 Overlap, edge filtering and mask merging\nMask merging is built on previous predictions. In this step, for each polygon we check whether the ratio between intersection with any other polygon of the same class and the area of the polygon is more than 0.2. If yes, the polygon is merged to the other polygon with which it had intersection-over-area ratio.\n\n\nCode\nmerge_outpath = Path('../results/hp_merge_new//')\nif not os.path.exists(merge_outpath):\n    shutil.copytree('../results/template_folder/', merge_outpath, symlinks=True)\n\n\nTwo iterations of merging is usually enough.\n\n\nCode\nfor r in pred_shps:\n    gdf_temp = gpd.read_file(r)\n    standing = gdf_temp[gdf_temp.label==1].copy()\n    fallen = gdf_temp[gdf_temp.label==2].copy()\n    standing = merge_polys(standing, 0.2)\n    fallen = merge_polys(fallen, 0.2)\n    standing = merge_polys(standing, 0.2)\n    fallen = merge_polys(fallen, 0.2)\n    gdf_merged = pd.concat((standing, fallen))\n    gdf_merged.to_file(merge_outpath/'predicted_vectors'/r.name, driver='GeoJSON')\n    gdf_merged = None\n    gdf_temp = None\n\n\n\n\nCode\nmerge_outpath\n\n\nPath('../results/hp_merge_new')\n\n\n\n\nCode\nmerged_coco_eval = GisCOCOeval(merge_outpath, merge_outpath, None, None, deadwood_categories)\nmerged_coco_eval.prepare_data(gt_label_col='layer')\nmerged_coco_eval.prepare_eval()\nmerged_coco_eval.evaluate()\n\n\n\n\n\n\n\n\nloading annotations into memory...\nDone (t=0.03s)\ncreating index...\nindex created!\nLoading and preparing results...\nDONE (t=0.02s)\ncreating index...\nindex created!\n\nEvaluating for category uprightwood\nRunning per image evaluation...\nEvaluate annotation type *segm*\nDONE (t=0.68s).\nAccumulating evaluation results...\nDONE (t=0.01s).\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.436\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.761\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.465\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.288\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.495\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.399\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.246\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.460\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 1.000\n\nEvaluating for category groundwood\nRunning per image evaluation...\nEvaluate annotation type *segm*\nDONE (t=9.05s).\nAccumulating evaluation results...\nDONE (t=0.01s).\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.246\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.605\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.130\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.248\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.207\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = -1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.151\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.153\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.082\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n\nEvaluating for full data...\nRunning per image evaluation...\nEvaluate annotation type *segm*\nDONE (t=9.90s).\nAccumulating evaluation results...\nDONE (t=0.02s).\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.341\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.683\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.297\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.268\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.351\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.275\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.200\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.271\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 1.000\n\n\nThis usually worsens the results a bit, but the produced results are better suited for deriving forest charasteristics, as the number of overlapping detected instances decrease significantly.\n\n\nCode\npreds.reset_index(drop=True, inplace=True)\nstanding = merge_polys(preds[preds.label == 1].copy(), 0.2)\nfallen = merge_polys(preds[preds.label == 2].copy(), 0.2)\nstanding = merge_polys(standing, 0.2)\nfallen = merge_polys(fallen, 0.2)\npreds_merged = pd.concat((standing, fallen))\n\n\n619it [00:02, 250.70it/s]\n2698it [00:23, 113.43it/s]\n574it [00:02, 284.39it/s]\n2074it [00:13, 155.47it/s]\n\n\n\n\nCode\npreds_merged['layer'] = preds_merged.apply(lambda row: 'groundwood' if row.label == 2 else 'uprightwood', axis=1)\npreds_merged.layer.value_counts()\n\n\ngroundwood     2003\nuprightwood     564\nName: layer, dtype: int64\n\n\n\n\nCode\ntp_truths = truths.copy()\ntp_truths.rename(columns={'groundwood':'label'}, inplace=True)\ntruth_sindex = tp_truths.sindex\nfp_preds_merged = preds_merged.copy()\npred_sindex = fp_preds_merged.sindex\ntp_truths[tp_cols] = tp_truths.progress_apply(lambda row: is_true_positive(row, fp_preds_merged, pred_sindex), \n                                              axis=1, result_type='expand')\nfp_preds_merged[fp_cols] = fp_preds_merged.progress_apply(lambda row: is_false_positive(row, tp_truths, truth_sindex,\n                                                                            fp_preds_merged, pred_sindex),\n                                            axis=1, result_type='expand')\n\n\n\n\n\n\n\n\n\n\nCode\npd.crosstab(fp_preds_merged.layer, fp_preds_merged['FP_0.5'], margins=True)\n\n\n\n\n\n\n  \n    \n      FP_0.5\n      FP\n      TP\n      All\n    \n    \n      layer\n      \n      \n      \n    \n  \n  \n    \n      groundwood\n      1036\n      967\n      2003\n    \n    \n      uprightwood\n      271\n      293\n      564\n    \n    \n      All\n      1307\n      1260\n      2567\n    \n  \n\n\n\n\n\n\nCode\npd.crosstab(tp_truths.layer, tp_truths['TP_0.5'], margins=True)\n\n\n\n\n\n\n  \n    \n      TP_0.5\n      FN\n      TP\n      All\n    \n    \n      layer\n      \n      \n      \n    \n  \n  \n    \n      groundwood\n      434\n      967\n      1401\n    \n    \n      uprightwood\n      47\n      293\n      340\n    \n    \n      All\n      481\n      1260\n      1741\n    \n  \n\n\n\n\n\\(Precision = \\frac{tp}{tp+fp}, Recall = \\frac{tp}{tp+fn}\\)\n\n\nCode\nprint(f'Precision for fallen deadwood with IoU threshold of 0.5 is {(967/2003):.2f}')\nprint(f'Recall for fallen deadwood with IoU threshold of 0.5 is {(967/1401):.2f}')\n\n\nPrecision for fallen deadwood with IoU threshold of 0.5 is 0.48\nRecall for fallen deadwood with IoU threshold of 0.5 is 0.69\n\n\n\n\nCode\nprint(f'Precision for standing deadwood with IoU threshold of 0.5 is {(293/564):.2f}')\nprint(f'Recall for standing deadwood with IoU threshold of 0.5 is {(293/340):.2f}')\n\n\nPrecision for standing deadwood with IoU threshold of 0.5 is 0.52\nRecall for standing deadwood with IoU threshold of 0.5 is 0.86\n\n\n\n\nCode\nprint(f'Overall precision with IoU threshold of 0.5 is {(1260/2567):.2f}')\nprint(f'Overall recall with IoU threshold of 0.5 is {(1260/1741):.2f}')\n\n\nOverall precision with IoU threshold of 0.5 is 0.49\nOverall recall with IoU threshold of 0.5 is 0.72\n\n\nAs the postprocessing merges data instead of dropping less certain predictions, total area and IoU remain the same as in previous step.\n\n\nCode\npreds_merged.to_file('../results/hiidenportti/merged_all_20220823.geojson')"
  },
  {
    "objectID": "5_scene_level_results.html#no-overlap-no-post-processing-1",
    "href": "5_scene_level_results.html#no-overlap-no-post-processing-1",
    "title": "Scene level results",
    "section": "2.1 No overlap, no post-processing",
    "text": "2.1 No overlap, no post-processing\n\n\nCode\nspk_raw_res_path = Path('../results/spk_benchmark/r101_nobuf/')\ntruth_shps = sorted([spk_raw_res_path/'vector_tiles'/f for f in os.listdir(spk_raw_res_path/'vector_tiles')])\nspk_raw_shps = sorted([spk_raw_res_path/'raw_preds'/f for f in os.listdir(spk_raw_res_path/'raw_preds')])\nrasters = sorted([spk_raw_res_path/'raster_tiles'/f for f in os.listdir(spk_raw_res_path/'raster_tiles')])\n\n\n\n\nCode\nfor p, t in zip(spk_raw_shps, truth_shps):\n    temp_pred = gpd.read_file(p)\n    temp_truth = gpd.read_file(t)\n    temp_pred = gpd.clip(temp_pred, box(*temp_truth.total_bounds))\n    temp_pred['geometry'] = temp_pred.apply(lambda row: fix_multipolys(row.geometry) \n                                            if row.geometry.type == 'MultiPolygon' \n                                            else shapely.geometry.Polygon(row.geometry.exterior), axis=1)\n    temp_pred['label'] += 1\n    temp_pred = temp_pred[temp_pred.geometry.area > 16*0.0485**2]\n    temp_pred.to_file(spk_raw_res_path/'predicted_vectors'/p.name)\n\n\n\n\nCode\npred_shps = sorted([spk_raw_res_path/'predicted_vectors'/f for f in os.listdir(spk_raw_res_path/'predicted_vectors')])\n\n\n\n\nCode\ntruths = None\npreds = None\n\nfor p, t in zip(pred_shps, truth_shps):\n    temp_pred = gpd.read_file(p)\n    temp_truth = gpd.read_file(t)\n    if truths is None:\n        truths = temp_truth\n        preds = temp_pred\n    else:\n        truths = pd.concat((truths, temp_truth))\n        preds = pd.concat((preds, temp_pred))\n\n\n\n\nCode\npreds['layer'] = preds.apply(lambda row: 'groundwood' if row.label == 2 else 'uprightwood', axis=1)\n\n\n\n\nCode\npreds.shape, truths.shape\n\n\n((6893, 4), (5334, 4))\n\n\n\n\nCode\npreds.layer.value_counts()\n\n\ngroundwood     5481\nuprightwood    1412\nName: layer, dtype: int64\n\n\n\n\nCode\ntruths.rename(columns={'label':'layer'}, inplace=True)\n\n\n\n\nCode\ndis_truths = truths.dissolve(by='layer')\ndis_preds = preds.dissolve(by='layer')\n\n\nIoU for standing deadwood is good already for this preprocessing level.\n\n\nCode\npoly_IoU(dis_truths, dis_preds)\n\n\nlayer\ngroundwood     0.467202\nuprightwood    0.605853\ndtype: float64\n\n\n\n\nCode\ndeadwood_categories = [{'supercategory': 'deadwood', 'id':1, 'name':'uprightwood'},\n                 \n                       {'supercategory': 'deadwood', 'id':2, 'name':'groundwood'}]\n\nspk_raw_coco_eval = GisCOCOeval(spk_raw_res_path, spk_raw_res_path, \n                           None, None, deadwood_categories)\nspk_raw_coco_eval.prepare_data(gt_label_col='label')\nspk_raw_coco_eval.prepare_eval()\nspk_raw_coco_eval.coco_eval.params.maxDets = [1000, 10000]\nspk_raw_coco_eval.evaluate()\n\n\n\n\n\n\n\n\nloading annotations into memory...\nDone (t=0.19s)\ncreating index...\nindex created!\nLoading and preparing results...\nDONE (t=0.06s)\ncreating index...\nindex created!\n\nEvaluating for category uprightwood\nRunning per image evaluation...\nEvaluate annotation type *segm*\nDONE (t=0.81s).\nAccumulating evaluation results...\nDONE (t=0.02s).\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=10000 ] = 0.249\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=10000 ] = 0.518\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=10000 ] = 0.224\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=10000 ] = 0.112\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=10000 ] = 0.326\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=10000 ] = 0.279\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.353\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.233\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.428\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.436\n\nEvaluating for category groundwood\nRunning per image evaluation...\nEvaluate annotation type *segm*\nDONE (t=10.17s).\nAccumulating evaluation results...\nDONE (t=0.05s).\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=10000 ] = 0.129\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=10000 ] = 0.382\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=10000 ] = 0.049\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=10000 ] = 0.128\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=10000 ] = 0.174\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=10000 ] = -1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.233\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.234\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.215\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = -1.000\n\nEvaluating for full data...\nRunning per image evaluation...\nEvaluate annotation type *segm*\nDONE (t=10.88s).\nAccumulating evaluation results...\nDONE (t=0.07s).\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=10000 ] = 0.189\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=10000 ] = 0.450\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=10000 ] = 0.136\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=10000 ] = 0.120\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=10000 ] = 0.250\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=10000 ] = 0.279\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.293\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.233\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.321\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.436\n\n\n\n\nCode\ntp_truths = truths.copy()\ntp_truths['label'] = tp_truths.layer.apply(lambda row: 1 if row=='uprightwood' else 2)\ntruth_sindex = tp_truths.sindex\nfp_preds = preds.copy()\npred_sindex = fp_preds.sindex\ntp_truths[tp_cols] = tp_truths.progress_apply(lambda row: is_true_positive(row, fp_preds, pred_sindex), \n                                              axis=1, result_type='expand')\nfp_preds[fp_cols] = fp_preds.progress_apply(lambda row: is_false_positive(row, tp_truths, truth_sindex,\n                                                                            fp_preds, pred_sindex),\n                                            axis=1, result_type='expand')\n\n\n\n\nCode\npd.crosstab(fp_preds.layer, fp_preds['FP_0.5'], margins=True)\n\n\n\n\n\n\n  \n    \n      FP_0.5\n      FP\n      TP\n      All\n    \n    \n      layer\n      \n      \n      \n    \n  \n  \n    \n      groundwood\n      3426\n      2055\n      5481\n    \n    \n      uprightwood\n      529\n      883\n      1412\n    \n    \n      All\n      3955\n      2938\n      6893\n    \n  \n\n\n\n\n\n\nCode\npd.crosstab(tp_truths.layer, tp_truths['TP_0.5'], margins=True)\n\n\n\n\n\n\n  \n    \n      TP_0.5\n      FN\n      TP\n      All\n    \n    \n      layer\n      \n      \n      \n    \n  \n  \n    \n      groundwood\n      1860\n      2055\n      3915\n    \n    \n      uprightwood\n      536\n      883\n      1419\n    \n    \n      All\n      2396\n      2938\n      5334\n    \n  \n\n\n\n\n\\(Precision = \\frac{tp}{tp+fp}, Recall = \\frac{tp}{tp+fn}\\)\n\n\nCode\nprint(f'Precision for fallen deadwood with IoU threshold of 0.5 is {(2055/5481):.2f}')\nprint(f'Recall for fallen deadwood with IoU threshold of 0.5 is {(2055/3915):.2f}')\n\n\nPrecision for fallen deadwood with IoU threshold of 0.5 is 0.37\nRecall for fallen deadwood with IoU threshold of 0.5 is 0.52\n\n\n\n\nCode\nprint(f'Precision for standing deadwood with IoU threshold of 0.5 is {(883/1412):.2f}')\nprint(f'Recall for standing deadwood with IoU threshold of 0.5 is {(883/1419):.2f}')\n\n\nPrecision for standing deadwood with IoU threshold of 0.5 is 0.63\nRecall for standing deadwood with IoU threshold of 0.5 is 0.62\n\n\n\n\nCode\nprint(f'Overall precision with IoU threshold of 0.5 is {(2938/6893):.2f}')\nprint(f'Overall recall with IoU threshold of 0.5 is {(2938/5334):.2f}')\n\n\nOverall precision with IoU threshold of 0.5 is 0.43\nOverall recall with IoU threshold of 0.5 is 0.55"
  },
  {
    "objectID": "5_scene_level_results.html#half-patch-overlap-and-edge-filtering-1",
    "href": "5_scene_level_results.html#half-patch-overlap-and-edge-filtering-1",
    "title": "Scene level results",
    "section": "2.2 Half patch overlap and edge filtering",
    "text": "2.2 Half patch overlap and edge filtering\n\n\nCode\nspk_res_path = Path('../results/spk_benchmark/r101/')\ntruth_shps = sorted([spk_res_path/'vector_tiles'/f for f in os.listdir(spk_res_path/'vector_tiles')])\nspk_buf_raw_shps = sorted([spk_res_path/'raw_preds'/f for f in os.listdir(spk_res_path/'raw_preds')])\nrasters = sorted([spk_res_path/'raster_tiles'/f for f in os.listdir(spk_res_path/'raster_tiles')])\n\n\n\n\nCode\nfor p, t in zip(spk_buf_raw_shps, truth_shps):\n    temp_pred = gpd.read_file(p)\n    temp_truth = gpd.read_file(t)\n    temp_pred = gpd.clip(temp_pred, box(*temp_truth.total_bounds))\n    temp_pred['geometry'] = temp_pred.apply(lambda row: fix_multipolys(row.geometry) \n                                            if row.geometry.type == 'MultiPolygon' \n                                            else shapely.geometry.Polygon(row.geometry.exterior), axis=1)\n    temp_pred['label'] += 1\n    temp_pred = temp_pred[temp_pred.geometry.area > 16*0.0485**2]\n    temp_pred.to_file(spk_res_path/'predicted_vectors'/p.name)\n\n\n\n\nCode\npred_shps = sorted([spk_res_path/'predicted_vectors'/f for f in os.listdir(spk_res_path/'predicted_vectors')])\n\n\n\n\nCode\ntruths = None\npreds = None\n\nfor p, t in zip(pred_shps, truth_shps):\n    temp_pred = gpd.read_file(p)\n    temp_truth = gpd.read_file(t)\n    if truths is None:\n        truths = temp_truth\n        preds = temp_pred\n    else:\n        truths = pd.concat((truths, temp_truth))\n        preds = pd.concat((preds, temp_pred))\n\n\n\n\nCode\npreds['layer'] = preds.apply(lambda row: 'groundwood' if row.label == 2 else 'uprightwood', axis=1)\n\n\n\n\nCode\npreds.shape, truths.shape\n\n\n((6511, 4), (5334, 4))\n\n\n\n\nCode\npreds.layer.value_counts()\n\n\ngroundwood     5204\nuprightwood    1307\nName: layer, dtype: int64\n\n\n\n\nCode\ntruths.rename(columns={'label':'layer'}, inplace=True)\n\n\n\n\nCode\ndis_truths = truths.dissolve(by='layer')\ndis_preds = preds.dissolve(by='layer')\n\n\n\n\nCode\npoly_IoU(dis_truths, dis_preds)\n\n\nlayer\ngroundwood     0.469782\nuprightwood    0.618346\ndtype: float64\n\n\n\n\nCode\ndeadwood_categories = [{'supercategory': 'deadwood', 'id':1, 'name':'uprightwood'},\n                 \n                       {'supercategory': 'deadwood', 'id':2, 'name':'groundwood'}]\n\nspk_coco_eval = GisCOCOeval(spk_res_path, spk_res_path, \n                           None, None, deadwood_categories)\nspk_coco_eval.prepare_data(gt_label_col='label')\nspk_coco_eval.prepare_eval()\nspk_coco_eval.coco_eval.params.maxDets = [1000, 10000]\nspk_coco_eval.evaluate()\n\n\n\n\n\n\n\n\nloading annotations into memory...\nDone (t=0.19s)\ncreating index...\nindex created!\nLoading and preparing results...\nDONE (t=0.05s)\ncreating index...\nindex created!\n\nEvaluating for category uprightwood\nRunning per image evaluation...\nEvaluate annotation type *segm*\nDONE (t=0.73s).\nAccumulating evaluation results...\nDONE (t=0.02s).\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=10000 ] = 0.321\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=10000 ] = 0.591\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=10000 ] = 0.331\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=10000 ] = 0.145\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=10000 ] = 0.422\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=10000 ] = 0.362\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.398\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.237\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.495\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.657\n\nEvaluating for category groundwood\nRunning per image evaluation...\nEvaluate annotation type *segm*\nDONE (t=9.72s).\nAccumulating evaluation results...\nDONE (t=0.05s).\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=10000 ] = 0.158\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=10000 ] = 0.438\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=10000 ] = 0.063\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=10000 ] = 0.155\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=10000 ] = 0.233\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=10000 ] = -1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.252\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.251\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.284\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = -1.000\n\nEvaluating for full data...\nRunning per image evaluation...\nEvaluate annotation type *segm*\nDONE (t=10.63s).\nAccumulating evaluation results...\nDONE (t=0.07s).\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=10000 ] = 0.239\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=10000 ] = 0.514\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=10000 ] = 0.197\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=10000 ] = 0.150\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=10000 ] = 0.327\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=10000 ] = 0.362\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.325\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.244\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.390\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.657\n\n\nThe overall AP50 score is close to the patch-level results here.\n\n\nCode\ntp_truths = truths.copy()\ntp_truths['label'] = tp_truths.layer.apply(lambda row: 1 if row=='uprightwood' else 2)\ntruth_sindex = tp_truths.sindex\nfp_preds = preds.copy()\npred_sindex = fp_preds.sindex\ntp_truths[tp_cols] = tp_truths.progress_apply(lambda row: is_true_positive(row, fp_preds, pred_sindex), \n                                              axis=1, result_type='expand')\nfp_preds[fp_cols] = fp_preds.progress_apply(lambda row: is_false_positive(row, tp_truths, truth_sindex,\n                                                                            fp_preds, pred_sindex),\n                                            axis=1, result_type='expand')\n\n\n\n\n\n\n\n\n\n\nCode\npd.crosstab(fp_preds.layer, fp_preds['FP_0.5'], margins=True)\n\n\n\n\n\n\n  \n    \n      FP_0.5\n      FP\n      TP\n      All\n    \n    \n      layer\n      \n      \n      \n    \n  \n  \n    \n      groundwood\n      3034\n      2170\n      5204\n    \n    \n      uprightwood\n      378\n      929\n      1307\n    \n    \n      All\n      3412\n      3099\n      6511\n    \n  \n\n\n\n\n\n\nCode\npd.crosstab(tp_truths.layer, tp_truths['TP_0.5'], margins=True)\n\n\n\n\n\n\n  \n    \n      TP_0.5\n      FN\n      TP\n      All\n    \n    \n      layer\n      \n      \n      \n    \n  \n  \n    \n      groundwood\n      1745\n      2170\n      3915\n    \n    \n      uprightwood\n      489\n      930\n      1419\n    \n    \n      All\n      2234\n      3100\n      5334\n    \n  \n\n\n\n\n\\(Precision = \\frac{tp}{tp+fp}, Recall = \\frac{tp}{tp+fn}\\)\n\n\nCode\nprint(f'Precision for fallen deadwood with IoU threshold of 0.5 is {(2170/5204):.2f}')\nprint(f'Recall for fallen deadwood with IoU threshold of 0.5 is {(2170/3915):.2f}')\n\n\nPrecision for fallen deadwood with IoU threshold of 0.5 is 0.42\nRecall for fallen deadwood with IoU threshold of 0.5 is 0.55\n\n\n\n\nCode\nprint(f'Precision for standing deadwood with IoU threshold of 0.5 is {(929/1307):.2f}')\nprint(f'Recall for standing deadwood with IoU threshold of 0.5 is {(930/1419):.2f}')\n\n\nPrecision for standing deadwood with IoU threshold of 0.5 is 0.71\nRecall for standing deadwood with IoU threshold of 0.5 is 0.66\n\n\n\n\nCode\nprint(f'Overall precision with IoU threshold of 0.5 is {(3099/6511):.2f}')\nprint(f'Overall recall with IoU threshold of 0.5 is {(3100/5334):.2f}')\n\n\nOverall precision with IoU threshold of 0.5 is 0.48\nOverall recall with IoU threshold of 0.5 is 0.58"
  },
  {
    "objectID": "5_scene_level_results.html#overlap-edge-filtering-and-mask-merging-1",
    "href": "5_scene_level_results.html#overlap-edge-filtering-and-mask-merging-1",
    "title": "Scene level results",
    "section": "2.3 Overlap, edge filtering and mask merging",
    "text": "2.3 Overlap, edge filtering and mask merging\n\n\nCode\nmerge_outpath = Path('../results/spk_benchmark/r101_merge/')\nif not os.path.exists(merge_outpath):\n    shutil.copytree('../results/spk_template/', merge_outpath, symlinks=True)\n\n\nTwo iterations of merging is usually enough.\n\n\nCode\nfor r in pred_shps:\n    gdf_temp = gpd.read_file(r)\n    standing = gdf_temp[gdf_temp.label==1].copy()\n    fallen = gdf_temp[gdf_temp.label==2].copy()\n    if len(standing) > 0:\n        standing = merge_polys(standing, 0.2)\n        standing = merge_polys(standing, 0.2)\n    if len(fallen) > 0:\n        fallen = merge_polys(fallen, 0.2)\n        fallen = merge_polys(fallen, 0.2)\n    gdf_merged = pd.concat((standing, fallen))\n    gdf_merged.to_file(merge_outpath/'predicted_vectors'/r.name, driver='GeoJSON')\n    gdf_merged = None\n    gdf_temp = None\n\n\n\n\nCode\nmerged_coco_eval = GisCOCOeval(merge_outpath, merge_outpath, None, None, deadwood_categories)\nmerged_coco_eval.prepare_data(gt_label_col='label')\nmerged_coco_eval.prepare_eval()\nmerged_coco_eval.evaluate()\n\n\n\n\n\n\n\n\nloading annotations into memory...\nDone (t=0.20s)\ncreating index...\nindex created!\nLoading and preparing results...\nDONE (t=0.31s)\ncreating index...\nindex created!\n\nEvaluating for category uprightwood\nRunning per image evaluation...\nEvaluate annotation type *segm*\nDONE (t=0.68s).\nAccumulating evaluation results...\nDONE (t=0.02s).\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.311\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.572\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.321\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.142\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.408\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.420\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.378\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.228\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.469\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.636\n\nEvaluating for category groundwood\nRunning per image evaluation...\nEvaluate annotation type *segm*\nDONE (t=7.82s).\nAccumulating evaluation results...\nDONE (t=0.04s).\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.160\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.450\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.064\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.157\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.258\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = -1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.205\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.203\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.275\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n\nEvaluating for full data...\nRunning per image evaluation...\nEvaluate annotation type *segm*\nDONE (t=8.47s).\nAccumulating evaluation results...\nDONE (t=0.06s).\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.236\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.511\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.192\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.149\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.333\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.420\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.292\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.215\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.372\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.636\n\n\n\n\nCode\npreds.reset_index(drop=True, inplace=True)\nstanding = merge_polys(preds[preds.label == 1].copy(), 0.2)\nfallen = merge_polys(preds[preds.label == 2].copy(), 0.2)\nstanding = merge_polys(standing, 0.2)\nfallen = merge_polys(fallen, 0.2)\npreds_merged = pd.concat((standing, fallen))\n\n\n\n\nCode\npreds_merged['layer'] = preds_merged.apply(lambda row: 'groundwood' if row.label == 2 else 'uprightwood', axis=1)\npreds_merged.layer.value_counts()\n\n\ngroundwood     4130\nuprightwood    1190\nName: layer, dtype: int64\n\n\n\n\nCode\ntp_truths = truths.copy()\ntp_truths['label'] = tp_truths.layer.apply(lambda row: 1 if row=='uprightwood' else 2)\ntruth_sindex = tp_truths.sindex\nfp_preds_merged = preds_merged.copy()\npred_sindex = fp_preds_merged.sindex\ntp_truths[tp_cols] = tp_truths.progress_apply(lambda row: is_true_positive(row, fp_preds_merged, pred_sindex), \n                                              axis=1, result_type='expand')\nfp_preds_merged[fp_cols] = fp_preds_merged.progress_apply(lambda row: is_false_positive(row, tp_truths, truth_sindex,\n                                                                            fp_preds_merged, pred_sindex),\n                                            axis=1, result_type='expand')\n\n\n\n\n\n\n\n\n\n\nCode\npd.crosstab(fp_preds_merged.layer, fp_preds_merged['FP_0.5'], margins=True)\n\n\n\n\n\n\n  \n    \n      FP_0.5\n      FP\n      TP\n      All\n    \n    \n      layer\n      \n      \n      \n    \n  \n  \n    \n      groundwood\n      2014\n      2116\n      4130\n    \n    \n      uprightwood\n      306\n      884\n      1190\n    \n    \n      All\n      2320\n      3000\n      5320\n    \n  \n\n\n\n\n\n\nCode\npd.crosstab(tp_truths.layer, tp_truths['TP_0.5'], margins=True)\n\n\n\n\n\n\n  \n    \n      TP_0.5\n      FN\n      TP\n      All\n    \n    \n      layer\n      \n      \n      \n    \n  \n  \n    \n      groundwood\n      1800\n      2115\n      3915\n    \n    \n      uprightwood\n      534\n      885\n      1419\n    \n    \n      All\n      2334\n      3000\n      5334\n    \n  \n\n\n\n\n\\(Precision = \\frac{tp}{tp+fp}, Recall = \\frac{tp}{tp+fn}\\)\n\n\nCode\nprint(f'Precision for fallen deadwood with IoU threshold of 0.5 is {(2116/4130):.2f}')\nprint(f'Recall for fallen deadwood with IoU threshold of 0.5 is {(2115/3915):.2f}')\n\n\nPrecision for fallen deadwood with IoU threshold of 0.5 is 0.51\nRecall for fallen deadwood with IoU threshold of 0.5 is 0.54\n\n\n\n\nCode\nprint(f'Precision for standing deadwood with IoU threshold of 0.5 is {(884/1190):.2f}')\nprint(f'Recall for standing deadwood with IoU threshold of 0.5 is {(885/1419):.2f}')\n\n\nPrecision for standing deadwood with IoU threshold of 0.5 is 0.74\nRecall for standing deadwood with IoU threshold of 0.5 is 0.62\n\n\n\n\nCode\nprint(f'Overall precision with IoU threshold of 0.5 is {(3000/5320):.2f}')\nprint(f'Overall recall with IoU threshold of 0.5 is {(3000/5334):.2f}')\n\n\nOverall precision with IoU threshold of 0.5 is 0.56\nOverall recall with IoU threshold of 0.5 is 0.56"
  },
  {
    "objectID": "6_result_comparison_with_field_data.html",
    "href": "6_result_comparison_with_field_data.html",
    "title": "Result comparison with field data",
    "section": "",
    "text": "Code\nfrom drone_detector.utils import * \nfrom drone_detector.imports import *\nimport rasterio.mask as rio_mask\nimport seaborn as sns\nsns.set_style('whitegrid')\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nsys.path.append('..')\nfrom src.tree_functions import *\nHere we first compare the annotations and predictions in field plot level metrics, and then do the same for predictions and field data. Comparison between annotations and predictions is done with all data within the scenes, whereas the comparisons between predictions and field data are done with the predictions intersecting the field plot circle."
  },
  {
    "objectID": "6_result_comparison_with_field_data.html#predictions-vs-annotations-with-all-data-present-in-the-scenes",
    "href": "6_result_comparison_with_field_data.html#predictions-vs-annotations-with-all-data-present-in-the-scenes",
    "title": "Result comparison with field data",
    "section": "1.1 Predictions vs annotations, with all data present in the scenes",
    "text": "1.1 Predictions vs annotations, with all data present in the scenes\nFirst crosstab the numbers of different deadwood types. First annotations.\n\n\nCode\npd.crosstab(anns.conservation, anns.layer, margins=True)\n\n\n\n\n\n\n  \n    \n      layer\n      groundwood\n      uprightwood\n      All\n    \n    \n      conservation\n      \n      \n      \n    \n  \n  \n    \n      0\n      916\n      181\n      1097\n    \n    \n      1\n      485\n      159\n      644\n    \n    \n      All\n      1401\n      340\n      1741\n    \n  \n\n\n\n\nThen predictions.\n\n\nCode\npd.crosstab(preds.conservation, preds.layer, margins=True)\n\n\n\n\n\n\n  \n    \n      layer\n      groundwood\n      uprightwood\n      All\n    \n    \n      conservation\n      \n      \n      \n    \n  \n  \n    \n      0\n      1211\n      373\n      1584\n    \n    \n      1\n      792\n      191\n      983\n    \n    \n      All\n      2003\n      564\n      2567\n    \n  \n\n\n\n\nAdd tree length and diameter estimations and check them for groundwood.\n\n\nCode\nanns['tree_length'] = anns.geometry.apply(get_len)\npreds['tree_length'] = preds.geometry.apply(get_len)\nanns['diam'] = anns.geometry.apply(lambda row: np.mean(get_three_point_diams(row))) * 1000\npreds['diam'] = preds.geometry.apply(lambda row: np.mean(get_three_point_diams(row))) * 1000\n\n\n\n\n\n\n\n\n\n\n\n\nDistributions for length look pretty similar, though there are some clear outliers in the predictions.\n\n\n\n\n\n\n\n\n\n\nDiameter distributions differ mostly in the 150-175mm class, which is overrepresented for predictions in managed forests.\nCheck the statistics for diameter and length.\n\n\nCode\nanns[anns.layer=='groundwood'].pivot_table(index='conservation', values=['diam', 'tree_length'], \n                                           margins=True, aggfunc=['mean', 'min', 'max'])\n\n\n\n\n\n\n  \n    \n      \n      mean\n      min\n      max\n    \n    \n      \n      diam\n      tree_length\n      diam\n      tree_length\n      diam\n      tree_length\n    \n    \n      conservation\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      194.449747\n      2.621222\n      89.906915\n      0.383440\n      558.643918\n      15.087245\n    \n    \n      1\n      220.942312\n      3.208590\n      87.563515\n      0.371857\n      614.556062\n      12.617573\n    \n    \n      All\n      203.620977\n      2.824558\n      87.563515\n      0.371857\n      614.556062\n      15.087245\n    \n  \n\n\n\n\n\n\nCode\npreds[preds.layer=='groundwood'].pivot_table(index='conservation', values=['diam', 'tree_length'], \n                                           margins=True, aggfunc=['mean', 'min', 'max'])\n\n\n\n\n\n\n  \n    \n      \n      mean\n      min\n      max\n    \n    \n      \n      diam\n      tree_length\n      diam\n      tree_length\n      diam\n      tree_length\n    \n    \n      conservation\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      181.405153\n      2.390542\n      73.158266\n      0.234854\n      416.484903\n      20.598724\n    \n    \n      1\n      187.242699\n      2.351327\n      95.175610\n      0.472634\n      483.888458\n      11.152778\n    \n    \n      All\n      183.713359\n      2.375036\n      73.158266\n      0.234854\n      483.888458\n      20.598724\n    \n  \n\n\n\n\nCheck the area covered by standing deadwood canopies. First annotations.\n\n\nCode\nanns['area_m2'] = anns.geometry.area\npreds['area_m2'] = preds.geometry.area\nanns[anns.layer=='uprightwood'].pivot_table(index='conservation', values=['area_m2'], margins=True,\n                                            aggfunc=['mean', 'sum'])\n\n\n\n\n\n\n  \n    \n      \n      mean\n      sum\n    \n    \n      \n      area_m2\n      area_m2\n    \n    \n      conservation\n      \n      \n    \n  \n  \n    \n      0\n      3.066763\n      555.084039\n    \n    \n      1\n      4.061288\n      645.744719\n    \n    \n      All\n      3.531849\n      1200.828758\n    \n  \n\n\n\n\nThen predictions. Predictions seem to slightly underestimate the sizes of standing deadwood canopies.\n\n\nCode\npreds[preds.layer=='uprightwood'].pivot_table(index='conservation', values=['area_m2'], margins=True,\n                                              aggfunc=['mean', 'sum'])\n\n\n\n\n\n\n  \n    \n      \n      mean\n      sum\n    \n    \n      \n      area_m2\n      area_m2\n    \n    \n      conservation\n      \n      \n    \n  \n  \n    \n      0\n      2.574031\n      960.113404\n    \n    \n      1\n      3.667250\n      700.444729\n    \n    \n      All\n      2.944252\n      1660.558133\n    \n  \n\n\n\n\nCheck the volume estimations.\n\n\nCode\nanns['v_ddw'] = anns.geometry.apply(cut_cone_volume)\npreds['v_ddw'] = preds.geometry.apply(cut_cone_volume)\nvirtual_plot_grid['vplot_id'] = virtual_plot_grid.apply(lambda p: f'{p.id}_{p.level_1}', axis=1)\n\ntest_cons_areas = cons_hp.overlay(virtual_plot_grid[virtual_plot_grid.vplot_id.isin(preds.vplot_id.unique())])\n\ntest_vplot_area = virtual_plot_grid[virtual_plot_grid.vplot_id.isin(preds.vplot_id.unique())].area.sum()\ntest_cons_area = test_cons_areas.area.sum()\ntest_man_area = test_vplot_area - test_cons_area\ntest_man_ha = test_man_area / 10000\ntest_cons_ha = test_cons_area / 10000\n\nann_est_v_man = anns[(anns.layer=='groundwood')&(anns.conservation==0)].v_ddw.sum()/test_man_ha\nann_est_v_cons = anns[(anns.layer=='groundwood')&(anns.conservation==1)].v_ddw.sum()/test_cons_ha\nann_est_v_test = anns[(anns.layer=='groundwood')].v_ddw.sum()/(test_vplot_area/10000)\n\npred_est_v_man = preds[(preds.layer=='groundwood')&(preds.conservation==0)].v_ddw.sum()/test_man_ha\npred_est_v_cons = preds[(preds.layer=='groundwood')&(preds.conservation==1)].v_ddw.sum()/test_cons_ha\npred_est_v_test = preds[(preds.layer=='groundwood')].v_ddw.sum()/(test_vplot_area/10000)\n\n\n\n\nCode\nprint(f'Estimated groundwood volume in managed forests, based on annotations: {ann_est_v_man:.2f} ha/m³')\nprint(f'Estimated groundwood volume in conserved forests, based on annotations: {ann_est_v_cons:.2f} ha/m³')\nprint(f'Estimated groundwood volume in both types, based on annotations: {ann_est_v_test:.2f} ha/m³')\n\n\nEstimated groundwood volume in managed forests, based on annotations: 13.05 ha/m³\nEstimated groundwood volume in conserved forests, based on annotations: 14.05 ha/m³\nEstimated groundwood volume in both types, based on annotations: 13.50 ha/m³\n\n\n\n\nCode\nprint(f'Estimated groundwood volume in managed forests, based on predictions: {pred_est_v_man:.2f} ha/m³')\nprint(f'Estimated groundwood volume in conserved forests, based on predictions: {pred_est_v_cons:.2f} ha/m³')\nprint(f'Estimated groundwood volume in both types, based on predictions: {pred_est_v_test:.2f} ha/m³')\n\n\nEstimated groundwood volume in managed forests, based on predictions: 15.02 ha/m³\nEstimated groundwood volume in conserved forests, based on predictions: 13.74 ha/m³\nEstimated groundwood volume in both types, based on predictions: 14.43 ha/m³"
  },
  {
    "objectID": "6_result_comparison_with_field_data.html#predictions-vs-field-data-with-only-predictions-present-in-field-plots",
    "href": "6_result_comparison_with_field_data.html#predictions-vs-field-data-with-only-predictions-present-in-field-plots",
    "title": "Result comparison with field data",
    "section": "1.2 Predictions vs field data, with only predictions present in field plots",
    "text": "1.2 Predictions vs field data, with only predictions present in field plots\nCount the number of annotated deadwood instances in each circular field plot, as well as note which of the circular plots are located in the conserved areas.\n\n\nCode\nplot_dw_data['n_dw_ann'] = plot_dw_data.apply(lambda row: anns_in_plots.plot_id.value_counts()[row.id] \n                                              if row.id in anns_in_plots.plot_id.unique() else 0, axis=1)\nplot_dw_data['n_ddw_ann'] = plot_dw_data.apply(lambda row: anns_in_plots[anns_in_plots.groundwood==2].plot_id.value_counts()[row.id] \n                                              if row.id in anns_in_plots[anns_in_plots.groundwood==2].plot_id.unique() else 0, axis=1)\nplot_dw_data['n_udw_ann'] = plot_dw_data.apply(lambda row: anns_in_plots[anns_in_plots.groundwood==1].plot_id.value_counts()[row.id] \n                                              if row.id in anns_in_plots[anns_in_plots.groundwood==1].plot_id.unique() else 0, axis=1)\nplot_dw_data['geometry'] = plot_dw_data.apply(lambda row: plot_circles[plot_circles.id == row.id].geometry.iloc[0], \n                                              axis=1)\nplot_dw_data = gpd.GeoDataFrame(plot_dw_data, crs=plot_circles.crs)\nplot_dw_data['conservation'] = plot_dw_data.apply(lambda row: 1 if any(cons_hp.geometry.contains(row.geometry))\n                                                  else 0, axis=1)\nplot_dw_data['n_dw_pred'] = plot_dw_data.apply(lambda row: preds_in_plots.plot_id.value_counts()[row.id] \n                                              if row.id in preds_in_plots.plot_id.unique() else 0, axis=1)\nplot_dw_data['n_ddw_pred'] = plot_dw_data.apply(lambda row: preds_in_plots[preds_in_plots.label==2].plot_id.value_counts()[row.id] \n                                              if row.id in preds_in_plots[preds_in_plots.label==2].plot_id.unique() else 0, axis=1)\nplot_dw_data['n_udw_pred'] = plot_dw_data.apply(lambda row: preds_in_plots[preds_in_plots.label==1].plot_id.value_counts()[row.id] \n                                              if row.id in preds_in_plots[preds_in_plots.label==1].plot_id.unique() else 0, axis=1)\n\n\nCompare the numbers of annotations (n_*_ann), predictions (n_*_pred) and field measured (n_*_plot) deadwood instances.\n\n\nCode\nplot_dw_data.pivot_table(index='conservation', values=['n_ddw_plot', 'n_udw_plot', \n                                                       'n_ddw_ann', 'n_udw_ann',\n                                                       'n_ddw_pred', 'n_udw_pred'], \n                         aggfunc='sum', margins=True)\n\n\n\n\n\n\n  \n    \n      \n      n_ddw_ann\n      n_ddw_plot\n      n_ddw_pred\n      n_udw_ann\n      n_udw_plot\n      n_udw_pred\n    \n    \n      conservation\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      44\n      43\n      57\n      13\n      12\n      15\n    \n    \n      1\n      11\n      45\n      22\n      4\n      8\n      3\n    \n    \n      All\n      55\n      88\n      79\n      17\n      20\n      18\n    \n  \n\n\n\n\nGet plot-wise canopy cover percentage based on LiDAR derived canopy height model as the percentage of plot area with height more than 2 meters.\n\n\nCode\npcts = []\n\nwith rio.open('../../data/raw/hiidenportti/full_mosaics/CHM_Hiidenportti_epsg.tif') as src:\n    crs = src.crs\n    for row in plot_dw_data.itertuples():\n        plot_im, plot_tfm = rio_mask.mask(src, [row.geometry], crop=True)\n        pcts.append(plot_im[plot_im > 2].shape[0] / plot_im[plot_im >= 0].shape[0])\nplot_dw_data['canopy_cover_pct'] = pcts\n\n\nPlot the relationship between annotated deadwood and field data.\n\n\n\n\n\nSame for field data and predictions\n\n\n\n\n\nNext the relationship between canopy cover and types of detected deadwood.\n\n\nCode\ng = sns.lmplot(data=plot_dw_data, x='n_udw_plot', y='canopy_cover_pct', col='conservation', hue='conservation', ci=1,\n                legend=False)\ng.axes[0,0].set_title('Managed forests')\ng.axes[0,1].set_title('Protected forests')\ng.set_ylabels('Canopy cover percentage for field plots')\ng.set_xlabels('Number of field-measured standing deadwood within plots')\nplt.show()\n\n\n\n\n\n\n\nCode\ng = sns.lmplot(data=plot_dw_data, x='n_udw_pred', y='canopy_cover_pct', col='conservation', hue='conservation', ci=1,\n                legend=False)\ng.axes[0,0].set_title('Managed forests')\ng.axes[0,1].set_title('Protected forests')\ng.set_ylabels('Canopy cover percentage for field plots')\ng.set_xlabels('Number of predicted standing deadwood within plots')\nplt.show()\n\n\n\n\n\n\n\nCode\ng = sns.lmplot(data=plot_dw_data, x='n_ddw_plot', y='canopy_cover_pct', col='conservation', hue='conservation', ci=1,\n                legend=False)\ng.axes[0,0].set_title('Managed forests')\ng.axes[0,1].set_title('Protected forests')\ng.set_ylabels('Canopy cover percentage for field plots')\ng.set_xlabels('Number of field-measured fallen deadwood within plots')\nplt.show()\n\n\n\n\n\n\n\nCode\ng = sns.lmplot(data=plot_dw_data, x='n_ddw_pred', y='canopy_cover_pct', col='conservation', hue='conservation', ci=1,\n                legend=False)\ng.axes[0,0].set_title('Managed forests')\ng.axes[0,1].set_title('Protected forests')\ng.set_ylabels('Canopy cover percentage for field plots')\ng.set_xlabels('Number of predicted fallen deadwood within plots')\nplt.show()\n\n\n\n\n\nNote that for the test set there are too few plots to draw meaningful conclusions.\nAdd information about conservation area to tree data.\n\n\nCode\ntree_data = tree_data[tree_data.plot_id.isin(plot_dw_data.id.unique())]\ntree_data['conservation'] = tree_data.apply(lambda row: plot_dw_data[plot_dw_data.id == row.plot_id].conservation.unique()[0], axis=1)\npreds_in_plots['conservation'] = preds_in_plots.apply(lambda row: plot_dw_data[plot_dw_data.id == row.plot_id].conservation.unique()[0], axis=1)\nanns_in_plots['tree_length'] = anns_in_plots.apply(lambda row: get_len(row.geometry), axis=1)\npreds_in_plots['tree_length'] = preds_in_plots.apply(lambda row: get_len(row.geometry), axis=1)\n\n\nCompare the distributions of the downed trunk lengths. Both graphs only take the parts within the plots into account. Lengths are binned into 1m bins.\n\n\n\n\n\n\n\n\n\n\nAs expected, annotated trunks are clearly on average shorter than field measured.\nCompare the measured DBH for downed trees and estimated diameter of annotated downed deadwood. For annotated deadwood, the diameter is estimated for the whole tree, not only for the part within the field plot. DBHs are binned into 50mm bins.\n\n\n\n\n\n\n\n\n\n\nSee the statistics for groundwood length and diameter. First for field data.\n\n\nCode\npd.pivot_table(data=tree_data[(tree_data.tree_class == 4)&(tree_data.DBH>0)],\n               index=['conservation'], values=['l', 'DBH'],\n               aggfunc=['mean', 'min', 'max'], margins=True)\n\n\n\n\n\n\n  \n    \n      \n      mean\n      min\n      max\n    \n    \n      \n      DBH\n      l\n      DBH\n      l\n      DBH\n      l\n    \n    \n      conservation\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      140.100016\n      7.040370\n      15.558197\n      0.518478\n      371.498790\n      14.855550\n    \n    \n      1\n      206.043641\n      8.075359\n      86.112222\n      0.214113\n      580.560964\n      18.000666\n    \n    \n      All\n      173.821188\n      7.569625\n      15.558197\n      0.214113\n      580.560964\n      18.000666\n    \n  \n\n\n\n\nThen for predictions.\n\n\nCode\npd.pivot_table(data=preds_in_plots[(preds_in_plots.label==2)&(preds_in_plots.tree_length >= 0.1)], \n               index=['conservation'], values=['tree_length', 'diam'],\n               aggfunc=['mean', 'min', 'max'], margins=True)\n\n\n\n\n\n\n  \n    \n      \n      mean\n      min\n      max\n    \n    \n      \n      diam\n      tree_length\n      diam\n      tree_length\n      diam\n      tree_length\n    \n    \n      conservation\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      166.508194\n      1.889520\n      10.889255\n      0.281128\n      301.422460\n      5.605036\n    \n    \n      1\n      172.456022\n      1.524233\n      6.250000\n      0.285395\n      350.669633\n      4.077773\n    \n    \n      All\n      168.207574\n      1.785152\n      6.250000\n      0.281128\n      350.669633\n      5.605036\n    \n  \n\n\n\n\nThen compare the statistics for volume estimations, first for field data.\n\n\nCode\nanns_in_plots['v_ddw'] = anns_in_plots.geometry.apply(cut_cone_volume)\n\nplot_dw_data['v_ddw_ann'] = plot_dw_data.apply(lambda row: anns_in_plots[(anns_in_plots.plot_id == row.id) &\n                                                                        (anns_in_plots.layer == 'groundwood')\n                                                                       ].v_ddw.sum()\n                                              , axis=1)\npreds_in_plots['v_ddw'] = preds_in_plots.geometry.apply(cut_cone_volume)\nplot_dw_data['v_ddw_pred'] = plot_dw_data.apply(lambda row: preds_in_plots[(preds_in_plots.plot_id == row.id) &\n                                                                        (preds_in_plots.layer == 'groundwood')\n                                                                       ].v_ddw.sum()\n                                              , axis=1)\nplot_dw_data['v_dw_plot'] = (plot_dw_data['v_dw']/10000)*np.pi*9**2\nplot_dw_data['v_ddw_plot'] = (plot_dw_data['v_ddw']/10000)*np.pi*9**2\nplot_dw_data['v_udw_plot'] = plot_dw_data.v_dw_plot - plot_dw_data.v_ddw_plot\npd.pivot_table(data=plot_dw_data, index=['conservation'], values=['v_ddw'],\n               aggfunc=['min', 'max', 'mean', 'median','std', 'count'], margins=True)\n\n\n\n\n\n\n  \n    \n      \n      min\n      max\n      mean\n      median\n      std\n      count\n    \n    \n      \n      v_ddw\n      v_ddw\n      v_ddw\n      v_ddw\n      v_ddw\n      v_ddw\n    \n    \n      conservation\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      1.476682\n      88.948335\n      29.528714\n      22.513292\n      28.686318\n      9\n    \n    \n      1\n      65.155283\n      139.171786\n      82.494768\n      71.816212\n      28.298432\n      6\n    \n    \n      All\n      1.476682\n      139.171786\n      50.715136\n      48.100161\n      38.439837\n      15\n    \n  \n\n\n\n\nThen for predictions.\n\n\nCode\nplot_dw_data['v_ddw_pred_ha'] = (10000 * plot_dw_data.v_ddw_pred) / (np.pi * 9**2)\n\npd.pivot_table(data=plot_dw_data, index=['conservation'], values=['v_ddw_pred_ha'],\n               aggfunc=['min', 'max', 'mean', 'median','std', 'count'], margins=True)\n\n\n\n\n\n\n  \n    \n      \n      min\n      max\n      mean\n      median\n      std\n      count\n    \n    \n      \n      v_ddw_pred_ha\n      v_ddw_pred_ha\n      v_ddw_pred_ha\n      v_ddw_pred_ha\n      v_ddw_pred_ha\n      v_ddw_pred_ha\n    \n    \n      conservation\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      4.239953\n      24.857178\n      13.098547\n      11.497471\n      8.527684\n      9\n    \n    \n      1\n      0.515183\n      30.041972\n      7.896543\n      3.257221\n      11.360618\n      6\n    \n    \n      All\n      0.515183\n      30.041972\n      11.017745\n      6.405077\n      9.726651\n      15\n    \n  \n\n\n\n\nBased on these plots, our predictions clearly underestimate the total volume of fallen deadwood, especially in conserved forests."
  },
  {
    "objectID": "6_result_comparison_with_field_data.html#predictions-vs-annotations-all-available-data",
    "href": "6_result_comparison_with_field_data.html#predictions-vs-annotations-all-available-data",
    "title": "Result comparison with field data",
    "section": "2.1 Predictions vs annotations, all available data",
    "text": "2.1 Predictions vs annotations, all available data\nCompare the numbers, first annotations.\n\n\nCode\npd.crosstab(evo_anns.conservation, evo_anns.label, margins=True)\n\n\n\n\n\n\n  \n    \n      label\n      groundwood\n      uprightwood\n      All\n    \n    \n      conservation\n      \n      \n      \n    \n  \n  \n    \n      0\n      2369\n      386\n      2755\n    \n    \n      1\n      1546\n      1033\n      2579\n    \n    \n      All\n      3915\n      1419\n      5334\n    \n  \n\n\n\n\nThen predictions.\n\n\nCode\npd.crosstab(evo_preds.conservation, evo_preds.layer, margins=True)\n\n\n\n\n\n\n  \n    \n      layer\n      groundwood\n      uprightwood\n      All\n    \n    \n      conservation\n      \n      \n      \n    \n  \n  \n    \n      0\n      2437\n      383\n      2820\n    \n    \n      1\n      1693\n      807\n      2500\n    \n    \n      All\n      4130\n      1190\n      5320\n    \n  \n\n\n\n\nAdd lenght and diameter information to groundwood.\n\n\nCode\nevo_anns['tree_length'] = evo_anns.geometry.apply(get_len)\nevo_preds['tree_length'] = evo_preds.geometry.apply(get_len)\nevo_anns['diam'] = evo_anns.geometry.apply(lambda row: np.mean(get_three_point_diams(row))) * 1000\nevo_preds['diam'] = evo_preds.geometry.apply(lambda row: np.mean(get_three_point_diams(row))) * 1000\n\n\n\n\n\n\n\n\n\n\n\n\nThe clearest difference between these distributions are that on average the predictions are shorter than annotations.\n\n\n\n\n\n\n\n\n\n\nThen statistics for groundwood diameter and length. First annotations.\n\n\nCode\nevo_anns[evo_anns.label=='groundwood'].pivot_table(index='conservation', values=['diam', 'tree_length'], \n                                       margins=True, aggfunc=['mean', 'min', 'max'])\n\n\n\n\n\n\n  \n    \n      \n      mean\n      min\n      max\n    \n    \n      \n      diam\n      tree_length\n      diam\n      tree_length\n      diam\n      tree_length\n    \n    \n      conservation\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      200.474161\n      3.113435\n      85.977992\n      0.605847\n      760.781151\n      24.479043\n    \n    \n      1\n      254.250328\n      3.914179\n      101.096059\n      0.575351\n      709.170305\n      22.625422\n    \n    \n      All\n      221.709909\n      3.429642\n      85.977992\n      0.575351\n      760.781151\n      24.479043\n    \n  \n\n\n\n\nThen predictions. On average, predictions are both shorter and thinner than annotations.\n\n\nCode\nevo_preds[evo_preds.layer=='groundwood'].pivot_table(index='conservation', values=['diam', 'tree_length'], \n                                           margins=True, aggfunc=['mean', 'min', 'max'])\n\n\n\n\n\n\n  \n    \n      \n      mean\n      min\n      max\n    \n    \n      \n      diam\n      tree_length\n      diam\n      tree_length\n      diam\n      tree_length\n    \n    \n      conservation\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      215.721066\n      2.285198\n      66.289676\n      0.281653\n      946.777021\n      14.455624\n    \n    \n      1\n      237.056243\n      2.832194\n      90.435975\n      0.251623\n      710.128841\n      18.956571\n    \n    \n      All\n      224.466939\n      2.509427\n      66.289676\n      0.251623\n      946.777021\n      18.956571\n    \n  \n\n\n\n\nCheck area covered by standing deadwood canopies. First annotations.\n\n\nCode\nevo_anns['area_m2'] = evo_anns.geometry.area\nevo_preds['area_m2'] = evo_preds.geometry.area\nevo_anns[evo_anns.label=='uprightwood'].pivot_table(index='conservation', values=['area_m2'], margins=True,\n                                                    aggfunc=['mean', 'sum'])\n\n\n\n\n\n\n  \n    \n      \n      mean\n      sum\n    \n    \n      \n      area_m2\n      area_m2\n    \n    \n      conservation\n      \n      \n    \n  \n  \n    \n      0\n      3.142462\n      1212.990283\n    \n    \n      1\n      5.811285\n      6003.057735\n    \n    \n      All\n      5.085305\n      7216.048018\n    \n  \n\n\n\n\nThen predictions. On average predicted canopies are larger than annotated.\n\n\nCode\nevo_preds[evo_preds.layer=='uprightwood'].pivot_table(index='conservation', values=['area_m2'], margins=True,\n                                                      aggfunc=['mean', 'sum'])\n\n\n\n\n\n\n  \n    \n      \n      mean\n      sum\n    \n    \n      \n      area_m2\n      area_m2\n    \n    \n      conservation\n      \n      \n    \n  \n  \n    \n      0\n      3.750448\n      1436.421535\n    \n    \n      1\n      5.776561\n      4661.684400\n    \n    \n      All\n      5.124459\n      6098.105935\n    \n  \n\n\n\n\n\n\nCode\nevo_anns['v_ddw'] = evo_anns.geometry.apply(cut_cone_volume)\nevo_preds['v_ddw'] = evo_preds.geometry.apply(cut_cone_volume)\n\nevo_vplot_area = evo_grid.area.sum()\nevo_cons_area = cons_evo.area.sum()\nevo_man_area = evo_vplot_area - evo_cons_area\nevo_man_ha = evo_man_area / 10000\nevo_cons_ha = evo_cons_area / 10000\n\nevo_ann_est_v_man = evo_anns[(evo_anns.label=='groundwood')&(evo_anns.conservation==0)].v_ddw.sum()/evo_man_ha\nevo_ann_est_v_cons = evo_anns[(evo_anns.label=='groundwood')&(evo_anns.conservation==1)].v_ddw.sum()/evo_cons_ha\nevo_ann_est_v = evo_anns[(evo_anns.label=='groundwood')].v_ddw.sum()/(evo_vplot_area/10000)\n\nevo_pred_est_v_man = evo_preds[(evo_preds.layer=='groundwood')&(evo_preds.conservation==0)].v_ddw.sum()/evo_man_ha\nevo_pred_est_v_cons = evo_preds[(evo_preds.layer=='groundwood')&(evo_preds.conservation==1)].v_ddw.sum()/evo_cons_ha\nevo_pred_est_v = evo_preds[(evo_preds.layer=='groundwood')].v_ddw.sum()/(evo_vplot_area/10000)\n\n\n\n\nCode\nprint(f'Estimated groundwood volume in managed forests, based on annotations: {evo_ann_est_v_man:.2f} ha/m³')\nprint(f'Estimated groundwood volume in conserved forests, based on annotations: {evo_ann_est_v_cons:.2f} ha/m³')\nprint(f'Estimated groundwood volume in both types, based on annotations: {evo_ann_est_v:.2f} ha/m³')\n\n\nEstimated groundwood volume in managed forests, based on annotations: 7.08 ha/m³\nEstimated groundwood volume in conserved forests, based on annotations: 13.97 ha/m³\nEstimated groundwood volume in both types, based on annotations: 9.90 ha/m³\n\n\n\n\nCode\nprint(f'Estimated groundwood volume in managed forests, based on predictions: {evo_pred_est_v_man:.2f} ha/m³')\nprint(f'Estimated groundwood volume in conserved forests, based on predictions: {evo_pred_est_v_cons:.2f} ha/m³')\nprint(f'Estimated groundwood volume in both types, based on predictions: {evo_pred_est_v:.2f} ha/m³')\n\n\nEstimated groundwood volume in managed forests, based on predictions: 7.03 ha/m³\nEstimated groundwood volume in conserved forests, based on predictions: 10.17 ha/m³\nEstimated groundwood volume in both types, based on predictions: 8.31 ha/m³"
  },
  {
    "objectID": "6_result_comparison_with_field_data.html#predictions-vs-field-data-plot-wise",
    "href": "6_result_comparison_with_field_data.html#predictions-vs-field-data-plot-wise",
    "title": "Result comparison with field data",
    "section": "2.2 Predictions vs field data, plot-wise",
    "text": "2.2 Predictions vs field data, plot-wise\nAdd canopy density based on LiDAR derived canopy height model. The density is the percentage of field plot area with height above 2 meters.\n\n\nCode\npcts = []\n\nwith rio.open('../../data/raw/sudenpesankangas/full_mosaics/sudenpesankangas_chm.tif') as src:\n    for row in evo_plots.itertuples():\n        plot_im, plot_tfm = rio_mask.mask(src, [row.geometry], crop=True)\n        pcts.append(plot_im[plot_im > 2].shape[0] / plot_im[plot_im >= 0].shape[0])\n\nevo_plots['canopy_cover_pct'] = pcts\n\npd.pivot_table(data=evo_plots, index=['conservation'], values=['canopy_cover_pct'],\n               aggfunc=['min', 'max', 'mean', 'std', 'count'], margins=True)\n\n\n\n\n\n\n  \n    \n      \n      min\n      max\n      mean\n      std\n      count\n    \n    \n      \n      canopy_cover_pct\n      canopy_cover_pct\n      canopy_cover_pct\n      canopy_cover_pct\n      canopy_cover_pct\n    \n    \n      conservation\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      0.237288\n      0.989960\n      0.782452\n      0.200439\n      42\n    \n    \n      1\n      0.678000\n      0.991992\n      0.869878\n      0.085901\n      29\n    \n    \n      All\n      0.237288\n      0.991992\n      0.818162\n      0.168393\n      71\n    \n  \n\n\n\n\nCount the number of deadwood instances similarly as for Hiidenportti data and plot the relationship between them.\n\n\nCode\nevo_plots['n_dw_ann'] = evo_plots.apply(lambda row: evo_anns.plot_id.value_counts()[int(row.id)]\n                                        if row.id in evo_anns.plot_id.unique() else 0, axis=1)\nevo_plots['n_ddw_ann'] = evo_plots.apply(lambda row: evo_anns[evo_anns.label=='groundwood'].plot_id.value_counts()[int(row.id)]\n                                        if row.id in evo_anns[evo_anns.label=='groundwood'].plot_id.unique() else 0, axis=1)\nevo_plots['n_udw_ann'] = evo_plots.apply(lambda row: evo_anns[evo_anns.label!='groundwood'].plot_id.value_counts()[int(row.id)]\n                                        if row.id in evo_anns[evo_anns.label!='groundwood'].plot_id.unique() else 0, axis=1)\n\nevo_plots['n_dw_plot'] = np.round((evo_plots['n_dw']/10000)*np.pi*9**2).astype(int)\nevo_plots['n_ddw_plot'] = np.round((evo_plots['n_ddw']/10000)*np.pi*9**2).astype(int)\nevo_plots['n_udw_plot'] = evo_plots.n_dw_plot - evo_plots.n_ddw_plot\nevo_plots['conservation'] = evo_plots.apply(lambda row: 1 if any(cons_evo.geometry.contains(row.geometry))\n                                                        else 0, axis=1)\n\nevo_plots['n_dw_pred'] = evo_plots.apply(lambda row: evo_preds.plot_id.value_counts()[int(row.id)]\n                                        if row.id in evo_preds.plot_id.unique() else 0, axis=1)\nevo_plots['n_ddw_pred'] = evo_plots.apply(lambda row: evo_preds[evo_preds.layer=='groundwood'].plot_id.value_counts()[int(row.id)]\n                                        if row.id in evo_preds[evo_preds.layer=='groundwood'].plot_id.unique() else 0, axis=1)\nevo_plots['n_udw_pred'] = evo_plots.apply(lambda row: evo_preds[evo_preds.layer!='groundwood'].plot_id.value_counts()[int(row.id)]\n                                        if row.id in evo_preds[evo_preds.layer!='groundwood'].plot_id.unique() else 0, axis=1)\n\n\n\nCompare the number of annotations, predictions and field measurements within plots.\n\n\nCode\nevo_plots.pivot_table(index='conservation', values=['n_ddw_plot', 'n_udw_plot', 'n_ddw_ann', 'n_udw_ann', \n                                                    'n_ddw_pred', 'n_udw_pred'], \n                         aggfunc='sum', margins=True)\n\n\n\n\n\n\n  \n    \n      \n      n_ddw_ann\n      n_ddw_plot\n      n_ddw_pred\n      n_udw_ann\n      n_udw_plot\n      n_udw_pred\n    \n    \n      conservation\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      65\n      14\n      59\n      14\n      68\n      11\n    \n    \n      1\n      22\n      29\n      30\n      29\n      92\n      21\n    \n    \n      All\n      87\n      43\n      89\n      43\n      160\n      32\n    \n  \n\n\n\n\nPlot the relationship between annotated and field data.\n\n\n\n\n\nSame for field data and predictions.\n\n\n\n\n\nRelationship between canopy cover and deadwood.\n\n\nCode\ng = sns.lmplot(data=evo_plots, x='n_udw_plot', y='canopy_cover_pct', col='conservation', hue='conservation', ci=1,\n                legend=False)\ng.axes[0,0].set_title('Managed forests')\ng.axes[0,1].set_title('Protected forests')\ng.set_ylabels('Canopy cover percentage for field plots')\ng.set_xlabels('Number of field-measured standing deadwood within plots')\nplt.show()\n\n\n\n\n\n\n\nCode\ng = sns.lmplot(data=evo_plots, x='n_udw_pred', y='canopy_cover_pct', col='conservation', hue='conservation', ci=1,\n                legend=False)\ng.axes[0,0].set_title('Managed forests')\ng.axes[0,1].set_title('Protected forests')\ng.set_ylabels('Canopy cover percentage for field plots')\ng.set_xlabels('Number of predicted standing deadwood within plots')\nplt.show()\n\n\n\n\n\n\n\nCode\ng = sns.lmplot(data=evo_plots, x='n_ddw_plot', y='canopy_cover_pct', col='conservation', hue='conservation', ci=1,\n                legend=False)\ng.axes[0,0].set_title('Managed forests')\ng.axes[0,1].set_title('Protected forests')\ng.set_ylabels('Canopy cover percentage for field plots')\ng.set_xlabels('Number of field-measured fallen deadwood within plots')\nplt.show()\n\n\n\n\n\n\n\nCode\ng = sns.lmplot(data=evo_plots, x='n_ddw_ann', y='canopy_cover_pct', col='conservation', hue='conservation', ci=1,\n                legend=False)\ng.axes[0,0].set_title('Managed forests')\ng.axes[0,1].set_title('Protected forests')\ng.set_ylabels('Canopy cover percentage for field plots')\ng.set_xlabels('Number of predicted fallen deadwood within plots')\nplt.show()\n\n\n\n\n\nAs Evo data doesn’t have field-measured deadwood lengths, we can’t plot that relationship. We can, however, plot the DBH distributions, even though Evo dataset only has around 50 downed deadwood with dbh measured.\n\n\nCode\nevo_field_data = evo_field_data[evo_field_data.plotid.isin(evo_plots.id.unique())]\n\nevo_field_data['conservation'] = evo_field_data.apply(lambda row: evo_plots[evo_plots.id == row.plotid].conservation.unique()[0], axis=1)\n\n\n\n\n\n\n\n\n\n\n\n\nest_pituus_m is the estimated lenght of the deadwood, based on DBH (lapimitta_mm) and growth formulae, so that is really inaccurate.\n\n\nCode\npd.pivot_table(data=evo_field_data[(evo_field_data.puuluo == 4)&(evo_field_data.lapimitta_mm>0)],\n               index=['conservation'], values=['est_pituus_m', 'lapimitta_mm'],\n               aggfunc=['mean', 'min', 'max', 'count'], margins=True)\n\n\n\n\n\n\n  \n    \n      \n      mean\n      min\n      max\n      count\n    \n    \n      \n      est_pituus_m\n      lapimitta_mm\n      est_pituus_m\n      lapimitta_mm\n      est_pituus_m\n      lapimitta_mm\n      est_pituus_m\n      lapimitta_mm\n    \n    \n      conservation\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      13.609571\n      169.357143\n      5.934\n      55\n      34.211\n      552\n      14\n      14\n    \n    \n      1\n      11.905690\n      115.896552\n      5.446\n      45\n      28.101\n      445\n      29\n      29\n    \n    \n      All\n      12.460442\n      133.302326\n      5.446\n      45\n      34.211\n      552\n      43\n      43\n    \n  \n\n\n\n\nLength and diameter statistics for predictions.\n\n\nCode\nevo_preds_in_plots['tree_length'] = evo_preds_in_plots.geometry.apply(get_len)\n\npd.pivot_table(data=evo_preds_in_plots[evo_preds_in_plots.label==2], index=['conservation'], \n               values=['tree_length', 'diam'],\n               aggfunc=['mean', 'min', 'max'], margins=True)\n\n\n\n\n\n\n  \n    \n      \n      mean\n      min\n      max\n    \n    \n      \n      diam\n      tree_length\n      diam\n      tree_length\n      diam\n      tree_length\n    \n    \n      conservation\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      215.914297\n      1.962542\n      69.619593\n      0.375545\n      502.915702\n      9.128894\n    \n    \n      1\n      211.666849\n      2.007725\n      74.672309\n      0.242613\n      356.577743\n      6.525554\n    \n    \n      All\n      214.482573\n      1.977772\n      69.619593\n      0.242613\n      502.915702\n      9.128894\n    \n  \n\n\n\n\nEstimated volume statistics for Evo area. First based on field data.\n\n\nCode\nevo_preds_in_plots['v_ddw'] = evo_preds_in_plots.geometry.apply(cut_cone_volume)\nevo_plots['v_ddw_pred'] = evo_plots.apply(lambda row: evo_preds_in_plots[(evo_preds_in_plots.plot_id == row.id) &\n                                                              (evo_preds_in_plots.layer == 'groundwood')\n                                                              ].v_ddw.sum()\n                                              , axis=1)\n\nevo_plots['v_ddw_pred_ha'] = (10000 * evo_plots.v_ddw_pred) / (np.pi * 9**2)\npd.pivot_table(data=evo_plots, index=['conservation'], values=['v_ddw'],\n               aggfunc=['min', 'max', 'mean', 'median','std', 'count'], margins=True)\n\n\n\n\n\n\n  \n    \n      \n      min\n      max\n      mean\n      median\n      std\n      count\n    \n    \n      \n      v_ddw\n      v_ddw\n      v_ddw\n      v_ddw\n      v_ddw\n      v_ddw\n    \n    \n      conservation\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      0.0\n      123.004587\n      5.522673\n      0.0\n      23.497052\n      42\n    \n    \n      1\n      0.0\n      98.258093\n      6.338610\n      0.0\n      18.873961\n      29\n    \n    \n      All\n      0.0\n      123.004587\n      5.855943\n      0.0\n      21.587804\n      71\n    \n  \n\n\n\n\nThen based on predictions.\n\n\nCode\npd.pivot_table(data=evo_plots, index=['conservation'], values=['v_ddw_pred_ha'],\n               aggfunc=['min', 'max', 'mean', 'median','std', 'count'], margins=True)\n\n\n\n\n\n\n  \n    \n      \n      min\n      max\n      mean\n      median\n      std\n      count\n    \n    \n      \n      v_ddw_pred_ha\n      v_ddw_pred_ha\n      v_ddw_pred_ha\n      v_ddw_pred_ha\n      v_ddw_pred_ha\n      v_ddw_pred_ha\n    \n    \n      conservation\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      0.0\n      66.098826\n      5.599320\n      0.000000\n      13.330675\n      42\n    \n    \n      1\n      0.0\n      32.592807\n      4.048691\n      1.193922\n      6.801761\n      29\n    \n    \n      All\n      0.0\n      66.098826\n      4.965964\n      0.000000\n      11.098662\n      71"
  },
  {
    "objectID": "7_deriving_standwise_metrics_for_evo.html",
    "href": "7_deriving_standwise_metrics_for_evo.html",
    "title": "Deriving deadwood statistics for full Evo study area",
    "section": "",
    "text": "As individual deadwood instances are not that useful as a final product, here we show how they can be further processed to more useful data.\n\n1 Data aggregated to Forest Centre stand level\n\n\nCode\nfrom drone_detector.utils import * \nfrom drone_detector.imports import *\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nsys.path.append('..')\nfrom src.tree_functions import *\nfrom tqdm import tqdm\ntqdm.pandas()\nimport seaborn as sns\n\n\nRead files for stand data, borders for valid area in UAV imagery and full predictions.\n\n\nCode\nspk_stands = gpd.read_file('../../data/evo/MV_Hämeenlinna/MV_Hämeenlinna.gpkg', layer='stand')\nspk_borders = gpd.read_file('../../data/evo/sudenpesankangas/spk_valid.geojson')\nspk_results = gpd.read_file('../../results/evo/sudenpesankangas/full/spk_full_310826.gpkg', layer='predictions')\n\n\nClip stand data to only contain relevant areas.\n\n\nCode\nspk_stands = spk_stands.clip(spk_borders)\n\n\nAttach tree length, diameter and volume to predictions.\n\n\nCode\nspk_results.head()\n\n\n\n\n\n\n  \n    \n      \n      label\n      score\n      layer\n      geometry\n    \n  \n  \n    \n      0\n      1\n      0.748620\n      uprightwood\n      POLYGON ((403139.067 6788066.734, 403139.019 6...\n    \n    \n      1\n      1\n      0.959675\n      uprightwood\n      POLYGON ((403127.426 6788097.439, 403127.377 6...\n    \n    \n      2\n      1\n      0.940683\n      uprightwood\n      POLYGON ((403142.900 6788100.835, 403142.851 6...\n    \n    \n      3\n      1\n      0.632036\n      uprightwood\n      POLYGON ((403134.750 6788104.570, 403134.702 6...\n    \n    \n      4\n      1\n      0.966496\n      uprightwood\n      POLYGON ((403129.778 6788124.094, 403129.754 6...\n    \n  \n\n\n\n\n\n\nCode\nspk_results['tree_length'] = spk_results.geometry.progress_apply(get_len)\nspk_results['diam'] = spk_results.geometry.progress_apply(lambda row: np.mean(get_three_point_diams(row))) * 1000\nspk_results['v_ddw'] = spk_results.geometry.progress_apply(cut_cone_volume)\n\n\n100%|███████████████████████████████████████████████████████████████████████████| 74395/74395 [02:27<00:00, 504.67it/s]\n100%|███████████████████████████████████████████████████████████████████████████| 74395/74395 [06:34<00:00, 188.58it/s]\n100%|███████████████████████████████████████████████████████████████████████████| 74395/74395 [09:42<00:00, 127.78it/s]\n\n\nDrop most obvious faulty groundwood detections (those with volume more than 5m³). These are for instance roads, gray rooftops etc.\n\n\nCode\nspk_results.drop(spk_results[(spk_results.v_ddw > 5) & (spk_results.layer == 'groundwood')].index, inplace=True)\n\n\nCheck the total amounts of deadwood present.\n\n\nCode\nspk_results.layer.value_counts()\n\n\ngroundwood     60509\nuprightwood    13831\nName: layer, dtype: int64\n\n\nDerive the following:\n\nn_dw: the number of deadwood instances per hectare\nn_ddw: the number of fallen deadwood instances per hectare\nn_udw: the number of standin gdeadwood instances per hectare\nv_ddw: the estimated volume of fallen deadwood per hectare\n\n\n\nCode\nspk_stands['n_dw'] = spk_stands.progress_apply(lambda row: len(spk_results[spk_results.geometry.within(row.geometry)]) \n                                                           / row.area, axis=1)\nspk_stands['n_ddw'] = spk_stands.progress_apply(lambda row: len(spk_results[(spk_results.geometry.within(row.geometry)) \n                                                                             & (spk_results.layer=='groundwood')]) \n                                                           / row.area, axis=1)\nspk_stands['n_udw'] = spk_stands.progress_apply(lambda row: len(spk_results[(spk_results.geometry.within(row.geometry)) \n                                                                             & (spk_results.layer=='uprightwood')]) \n                                                           / row.area, axis=1)\nspk_stands['v_ddw'] = spk_stands.progress_apply(lambda row: spk_results[(spk_results.geometry.within(row.geometry)) \n                                                                        & (spk_results.layer=='groundwood')].v_ddw.sum() \n                                                           / row.area, axis=1)\n\n\n100%|████████████████████████████████████████████████████████████████████████████████| 526/526 [01:39<00:00,  5.27it/s]\n100%|████████████████████████████████████████████████████████████████████████████████| 526/526 [01:49<00:00,  4.81it/s]\n100%|████████████████████████████████████████████████████████████████████████████████| 526/526 [01:49<00:00,  4.81it/s]\n100%|████████████████████████████████████████████████████████████████████████████████| 526/526 [01:44<00:00,  5.02it/s]\n\n\n\n\n\n\n\nBetter maps are done in QGis (use Jenks breaks).\nAdd information about conserved areas\n\n\nCode\nprotected_area = gpd.read_file('../../../../FEO/CDDA_2021_v01_public.gpkg', layer='ProtectedSite')\nprotected_area = protected_area[protected_area.siteName == 'Sudenpesänkankaan suojelualue']\nprotected_area = protected_area.to_crs(spk_stands.crs)\nspk_stands['conservation'] = spk_stands.geometry.apply(lambda row: 1 if row.buffer(-1).within(protected_area.iloc[0].geometry)\n                                                                   else 0)\n\n\nAdd stand-wise canopy density.\n\n\nCode\nimport rasterio.mask as rio_mask\n\npcts = []\n\nwith rio.open('../../data/evo/ChmEvo_norm.tif') as src:\n    crs = src.crs\n    for row in tqdm(spk_stands.itertuples()):\n        plot_im, plot_tfm = rio_mask.mask(src, [box(*row.geometry.bounds)], crop=True)\n        pcts.append(plot_im[plot_im > 2].shape[0] / plot_im[plot_im >= 0].shape[0])\nspk_stands['canopy_cover_pct'] = pcts\n\n\n526it [00:07, 69.67it/s] \n\n\nSee if any correlation can be seen for number of detected trees and canopy cover.\n\n\nCode\nfig, ax = plt.subplots(1,2, dpi=300, figsize=(10,5))\nspk_stands[spk_stands.n_dw > 0].plot(column='n_ddw', legend=True, ax=ax[0], cmap='YlGnBu', \n                                     edgecolor='black', linewidth=.2).set_title('n_ddw')\nspk_stands[spk_stands.n_udw > 0].plot(column='canopy_cover_pct', legend=True, ax=ax[1], cmap='YlGnBu', \n                                      edgecolor='black', linewidth=.2).set_title('canopy_cover_pct')\nplt.show()\n\n\n\n\n\nNot much, aside for perhaps the areas in the upper-central stands.\nNext see effect of canopy cover and different deadwood types, separated by conservation.\n\n\nCode\nsns.lmplot(data=spk_stands, x='n_dw', y='canopy_cover_pct', hue='conservation', col='conservation')\n\n\n<seaborn.axisgrid.FacetGrid>\n\n\n\n\n\nAggregated n_dw\n\n\nCode\npd.pivot_table(data=spk_stands, index=['conservation'], values=['n_dw'],\n               aggfunc=['min', 'max', 'mean', 'median','std', 'count'], margins=True)\n\n\n\n\n\n\n  \n    \n      \n      min\n      max\n      mean\n      median\n      std\n      count\n    \n    \n      \n      n_dw\n      n_dw\n      n_dw\n      n_dw\n      n_dw\n      n_dw\n    \n    \n      conservation\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      0.000000\n      631.790744\n      52.090442\n      19.745502\n      98.545014\n      381\n    \n    \n      1\n      3.995206\n      553.470919\n      100.909354\n      71.261682\n      103.233080\n      145\n    \n    \n      All\n      0.000000\n      631.790744\n      65.548127\n      29.410884\n      102.121836\n      526\n    \n  \n\n\n\n\nSame for n_udw.\n\n\nCode\nsns.lmplot(data=spk_stands, x='n_udw', y='canopy_cover_pct', hue='conservation', col='conservation')\n\n\n<seaborn.axisgrid.FacetGrid>\n\n\n\n\n\nCanopy cover seems not to affect standing deadwood detections.\n\n\nCode\npd.pivot_table(data=spk_stands, index=['conservation'], values=['n_udw'],\n               aggfunc=['min', 'max', 'mean', 'median','std', 'count'], margins=True)\n\n\n\n\n\n\n  \n    \n      \n      min\n      max\n      mean\n      median\n      std\n      count\n    \n    \n      \n      n_udw\n      n_udw\n      n_udw\n      n_udw\n      n_udw\n      n_udw\n    \n    \n      conservation\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      0.0\n      388.329980\n      6.500767\n      2.411091\n      21.967670\n      381\n    \n    \n      1\n      0.0\n      137.254902\n      27.411903\n      23.857527\n      23.109761\n      145\n    \n    \n      All\n      0.0\n      388.329980\n      12.265244\n      4.077489\n      24.150758\n      526\n    \n  \n\n\n\n\nSeems to be clear difference between conserved and managed forests, as mean values differ a lot.\nThen n_ddw.\n\n\nCode\nsns.lmplot(data=spk_stands, x='n_ddw', y='canopy_cover_pct', hue='conservation', col='conservation')\n\n\n<seaborn.axisgrid.FacetGrid>\n\n\n\n\n\nClear-ish that denser canopy means fewer detections.\n\n\nCode\npd.pivot_table(data=spk_stands, index=['conservation'], values=['n_ddw'],\n               aggfunc=['min', 'max', 'mean', 'median','std', 'count'], margins=True)\n\n\n\n\n\n\n  \n    \n      \n      min\n      max\n      mean\n      median\n      std\n      count\n    \n    \n      \n      n_ddw\n      n_ddw\n      n_ddw\n      n_ddw\n      n_ddw\n      n_ddw\n    \n    \n      conservation\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      0.0\n      566.716642\n      45.589675\n      14.925373\n      90.948590\n      381\n    \n    \n      1\n      0.0\n      511.257036\n      73.497452\n      49.458449\n      86.954054\n      145\n    \n    \n      All\n      0.0\n      566.716642\n      53.282883\n      21.543022\n      90.646476\n      526\n    \n  \n\n\n\n\nConserved forests generally have more fallen deadwood instances than managed.\nThen v_ddw.\n\n\nCode\nsns.lmplot(data=spk_stands, x='v_ddw', y='canopy_cover_pct', hue='conservation', col='conservation')\n\n\n<seaborn.axisgrid.FacetGrid>\n\n\n\n\n\n\n\nCode\npd.pivot_table(data=spk_stands, index=['conservation'], values=['v_ddw'],\n               aggfunc=['min', 'max', 'mean', 'median','std', 'count'], margins=True)\n\n\n\n\n\n\n  \n    \n      \n      min\n      max\n      mean\n      median\n      std\n      count\n    \n    \n      \n      v_ddw\n      v_ddw\n      v_ddw\n      v_ddw\n      v_ddw\n      v_ddw\n    \n    \n      conservation\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      0.0\n      54.578953\n      4.757696\n      1.494159\n      8.503932\n      381\n    \n    \n      1\n      0.0\n      214.149358\n      17.851251\n      8.164946\n      35.245017\n      145\n    \n    \n      All\n      0.0\n      214.149358\n      8.367136\n      2.428219\n      20.672735\n      526\n    \n  \n\n\n\n\nDifference in the estimated volume of downed deadwood per hectare between conserved and managed forests is clear.\nFinally v_ddw and n_ddw:\n\n\nCode\nsns.lmplot(data=spk_stands, x='v_ddw', y='n_ddw', hue='conservation', col='conservation')\n\n\n<seaborn.axisgrid.FacetGrid>\n\n\n\n\n\nFallen deadwood in conserved forests are clearly smaller.\n\n\n2 Data aggregated to 16x16m grid\nSome tasks, such as Zonation analyses, require data to be aggregated to 16x16m grid. First make that kind of grid.\n\n\nCode\nfrom itertools import product\ngridsize = 16\nxmin, ymin, xmax, ymax = spk_results.total_bounds\ncells = []\nfor x, y in tqdm(product(np.arange(xmin, xmax+gridsize, gridsize),np.arange(ymin, ymax+gridsize, gridsize))):\n    x1 = x - gridsize\n    y1 = y + gridsize\n    cells.append(box(x,y,x1,y1))\n\n\n58534it [00:00, 120200.79it/s]\n\n\nThen define some functions that get the information.\n\n\nCode\ndef get_n_dw(row, trees, tree_sindex):\n    nearest_idx = list(tree_sindex.nearest(row.geometry.bounds))\n    tempdf = trees.iloc[nearest_idx]\n    return len(tempdf[tempdf.geometry.intersects(row.geometry)])\n\ndef get_n_udw(row, trees, tree_sindex):\n    nearest_idx = list(tree_sindex.nearest(row.geometry.bounds))\n    tempdf = trees.iloc[nearest_idx]\n    tempdf = tempdf[tempdf.layer == 'uprightwood']\n    return len(tempdf[tempdf.geometry.intersects(row.geometry)])\n\ndef get_n_ddw(row, trees, tree_sindex):\n    nearest_idx = list(tree_sindex.nearest(row.geometry.bounds))\n    tempdf = trees.iloc[nearest_idx]\n    tempdf = tempdf[tempdf.layer == 'groundwood']\n    return len(tempdf[tempdf.geometry.intersects(row.geometry)])\n\ndef get_v_ddw(row, trees, tree_sindex):\n    nearest_idx = list(tree_sindex.nearest(row.geometry.bounds))\n    tempdf = trees.iloc[nearest_idx]\n    return tempdf[tempdf.layer=='groundwood'].clip(row.geometry).v_ddw.sum()\n\n\nAttach the information to grid cells.\n\n\nCode\ngrid = gpd.GeoDataFrame(cells, columns=['geometry'], crs=spk_results.crs)\ntrees_sindex = spk_results.sindex\ngrid['n_dw'] = grid.progress_apply(lambda row: get_n_dw(row, spk_results, trees_sindex), axis=1)\ngrid['n_udw'] = grid.progress_apply(lambda row: get_n_udw(row, spk_results, trees_sindex), axis=1)\ngrid['n_ddw'] = grid.progress_apply(lambda row: get_n_ddw(row, spk_results, trees_sindex), axis=1)\ngrid['v_ddw'] = grid.progress_apply(lambda row: get_v_ddw(row, spk_results, \n                                                          trees_sindex), axis=1)\n\n\n100%|███████████████████████████████████████████████████████████████████████████| 58534/58534 [02:26<00:00, 400.86it/s]\n100%|███████████████████████████████████████████████████████████████████████████| 58534/58534 [04:48<00:00, 202.91it/s]\n100%|███████████████████████████████████████████████████████████████████████████| 58534/58534 [03:20<00:00, 291.94it/s]\n100%|███████████████████████████████████████████████████████████████████████████| 58534/58534 [04:14<00:00, 230.02it/s]\n\n\nPlot n_dw.\n\n\nCode\ngrid[grid.n_dw > 0].plot(column='n_dw', legend=True, figsize=(20,12), cmap='YlGnBu').set_title('n_dw in 16m grid')\n\n\nText(0.5, 1.0, 'n_dw in 16m grid')\n\n\n\n\n\nPlot n_udw\n\n\nCode\ngrid[grid.n_udw > 0].plot(column='n_udw', legend=True, figsize=(20,12), cmap='YlGnBu').set_title('n_udw in 16m grid')\n\n\nText(0.5, 1.0, 'n_udw in 16m grid')\n\n\n\n\n\nPlot n_ddw\n\n\nCode\ngrid[grid.n_ddw > 0].plot(column='n_ddw', legend=True, figsize=(20,12), cmap='YlGnBu').set_title('n_ddw in 16m grid')\n\n\nText(0.5, 1.0, 'n_ddw in 16m grid')\n\n\n\n\n\nPlot v_ddw\n\n\nCode\ngrid[(grid.v_ddw > 0)].plot(column='v_ddw', legend=True, figsize=(20,12), cmap='YlGnBu').set_title('v_ddw in 16m grid')\n\n\nText(0.5, 1.0, 'v_ddw in 16m grid')"
  },
  {
    "objectID": "8_configs.html",
    "href": "8_configs.html",
    "title": "Running models with your own data",
    "section": "",
    "text": "All model configurations and weights here are fine-tuned from models available in Detectron2 Model Zoo.\nThe models are trained on 512x512px RGB image patches with spatial resolution between 3.9cm and 4.3 cm and with hand-annotated deadwood data based on visual inspection. The location of training dataset is in the vicinity of Hiidenportti national, Sotkamo, Finland, and the images were acquired during leaf-on season, on 16. and 17.7.2019. Most likely the models are most suitable to use with imagery from leaf-on season and with similar ground sampling distance."
  },
  {
    "objectID": "8_configs.html#hiidenportti-test-set",
    "href": "8_configs.html#hiidenportti-test-set",
    "title": "Running models with your own data",
    "section": "1.1 Hiidenportti test set",
    "text": "1.1 Hiidenportti test set\nHiidenportti test set contains 241 non-overlapping 512x512 pixel image patches, extracted from 5 scenes that cover 11 circular field plots.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nPatch AP50\nPatch AP\nPatch AP-groundwood\nPatch AP-uprightwood\nScene AP50\nScene AP\nScene AP-groundwood\nScene AP-uprightwood\n\n\n\n\nmask_rcnn_R_50_FPN_3x\n0.654\n0.339\n0.316\n0.363\n0.640\n0.315\n0.235\n0.396\n\n\nmask_rcnn_R_101_FPN_3x\n0.704\n0.366\n0.326\n0.406\n0.683\n0.341\n0.246\n0.436\n\n\nmask_rcnn_X_101_32x8d_FPN_3x\n0.679\n0.355\n0.333\n0.377\n0.661\n0.333\n0.255\n0.412\n\n\ncascade_mask_rcnn_R_50_FPN_3x\n0.652\n0.345\n0.306\n0.384\n0.623\n0.317\n0.223\n0.411"
  },
  {
    "objectID": "8_configs.html#sudenpesänkangas-dataset",
    "href": "8_configs.html#sudenpesänkangas-dataset",
    "title": "Running models with your own data",
    "section": "1.2 Sudenpesänkangas dataset",
    "text": "1.2 Sudenpesänkangas dataset\nSudenpesänkangas dataset contains 798 on-overlapping 512x512 pixel image patches, extracted from 71 scenes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nPatch AP50\nPatch AP\nPatch AP-groundwood\nPatch AP-uprightwood\nScene AP50\nScene AP\nScene AP-groundwood\nScene AP-uprightwood\n\n\n\n\nmask_rcnn_R_50_FPN_3x\n0.486\n0.237\n0.175\n0.299\n0.474\n0.221\n0.152\n0.290\n\n\nmask_rcnn_R_101_FPN_3x\n0.519\n0.252\n0.183\n0.321\n0.511\n0.236\n0.160\n0.311\n\n\nmask_rcnn_X_101_32x8d_FPN_3x\n0.502\n0.245\n0.182\n0.307\n0.494\n0.232\n0.159\n0.305\n\n\ncascade_mask_rcnn_R_50_FPN_3x\n0.497\n0.248\n0.172\n0.323\n0.473\n0.225\n0.148\n0.302"
  },
  {
    "objectID": "8_configs.html#running-models-for-image-patches",
    "href": "8_configs.html#running-models-for-image-patches",
    "title": "Running models with your own data",
    "section": "2.1 Running models for image patches",
    "text": "2.1 Running models for image patches\nFor individual image patches, the models are fairly straightforward to run.\nfrom detectron2.engine import DefaultPredictor\nfrom detectron2.config import get_cfg\nfrom detectron2.data import build_detection_test_loader\nimport cv2\n\ncfg = get_cfg()\ncfg.merge_from_file(<path_to_model_config>)\ncfg.OUTPUT_DIR = '<path_to_output>'\ncfg.MODEL.WEIGHTS = '<path_to_weights>'\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5 # score threshold for detections\npredictor = DefaultPredictor(cfg)\n\nimg = cv2.imread('<path_to_image_patch>')\noutputs = predictor(image)\nMore examples are shown on Patch level results."
  },
  {
    "objectID": "8_configs.html#running-models-for-larger-scenes",
    "href": "8_configs.html#running-models-for-larger-scenes",
    "title": "Running models with your own data",
    "section": "2.2 Running models for larger scenes",
    "text": "2.2 Running models for larger scenes\nRunning on larger scenes requires the following steps:\n\nTiling the scenes into smaller image patches, optionally with overlap\nRunning the model on these smaller patches\nGathering the predictions into a single GIS data file\nOptionally post-processing the results\n\ndrone_detector package has helpers for this:\nfrom drone_detector.engines.detectron2.predict import predict_instance_masks\n\npredict_instance_masks(path_to_model_config='<path_to_model_config>', # model config file\n                       path_to_image='<path_to_image>', # which image to process\n                       outfile='<name_for_predictions>.geojson', # where to save the results\n                       processing_dir='temp', # directory for temporary files, deleted afterwards. Default: temp\n                       tile_size=512, # image patch size in pixels, square patches. Default: 400\n                       tile_overlap=256, # overlap between tiles. Default: 100\n                       smooth_preds=False, # not yet implemented, at some points runs dilation+erosion to smooth polygons. Default: False\n                       coco_set='<path_to_coco>', # the coco set the model was trained on to infer the class names. If empty, defaults to dummy categories. Default: None\n                       postproc_results=True # whether to discard masks in the edge regions of patches Default: False\n                      )\nAlso, after installing the package, predict_instance_masks_detectron2 can be used as CLI command with identical syntax.\nWhen provided models default to dummy classes, 1 is standing deadwood and 2 is fallen deadwood.\nMore examples are shown on Scene level results."
  }
]